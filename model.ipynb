{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.data as Data\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") \n",
    "\n",
    "torch.cuda.manual_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"epoch\":1000,\n",
    "    \"batch_size\":32,\n",
    "    \"LR\":0.0001,\n",
    "    \"hidden_size\": 128,\n",
    "    \"num_layers\":2\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self,\n",
    "                 vocab_size=1,\n",
    "                 embedding_dim=100,\n",
    "                 hidden_size=1,\n",
    "                 num_layers=1,\n",
    "                 dropout=0.5\n",
    "                ):\n",
    "        super(Model,self).__init__()\n",
    "        \n",
    "        # attribute\n",
    "\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        # net\n",
    "        \n",
    "\n",
    "        self.embedding_layer = nn.Embedding(\n",
    "            num_embeddings=vocab_size,\n",
    "            embedding_dim=embedding_dim\n",
    "        )\n",
    "        \n",
    "\n",
    "        self.gru = nn.GRU(\n",
    "            input_size = embedding_dim,\n",
    "            hidden_size = self.hidden_size*2,\n",
    "            num_layers=num_layers,\n",
    "            dropout = dropout\n",
    "        )\n",
    "        \n",
    "        self.hidden_layer = nn.Linear(\n",
    "            in_features=self.hidden_size*2,\n",
    "            out_features=self.hidden_size\n",
    "        )\n",
    "\n",
    "        '''\n",
    "        output layer\n",
    "        \n",
    "        \n",
    "        '''\n",
    "        self.output_layer = nn.Linear(\n",
    "            in_features=self.hidden_size,\n",
    "            out_features=vocab_size\n",
    "        )\n",
    "        \n",
    "    def forward(self,x,hidden):\n",
    "        \n",
    "        \n",
    "        seq_len,batch_size = x.size() # input (seq_len,batch_size)\n",
    "     \n",
    "        \n",
    "        '''\n",
    "        Embedding layer\n",
    "        \n",
    "        in: (seq_len,batch_size)\n",
    "        out: (seq_len,batch_size,embeding_dim)\n",
    "        '''\n",
    "#         print(\"emb in size: {}\\n\\n\".format(x.size()))\n",
    "        embedded = self.embedding_layer(x)\n",
    "#         print(\"emb out size: {}\\n\\n\".format(embedded.size()))\n",
    "        \n",
    "        '''\n",
    "        GRU layer\n",
    "        \n",
    "        in shape: (seq_len, batch, input_size[embedding_size])\n",
    "        \n",
    "        out shape: (seq_len, batch, hidden_dim)\n",
    "        '''\n",
    "\n",
    "        out,hidden = self.gru(embedded,hidden)\n",
    "#         print(\"gru out size: {}\\n\\n\".format(out.size()))\n",
    "        out = nn.functional.relu(out)\n",
    "        \n",
    "        out = out.view(seq_len*batch_size,-1)\n",
    "        out = self.hidden_layer(out)\n",
    "        out = nn.functional.relu(out)\n",
    "        \n",
    "\n",
    "        \n",
    "        '''\n",
    "        output layer\n",
    "        \n",
    "        \n",
    "        in: (seq_len*batch_size,hidden_dim)\n",
    "        out: (seq_len*batch_size,vocab_size)\n",
    "        '''\n",
    "#         output_layer_in = out.view(seq_len*batch_size,-1)\n",
    "#         print(\"out layer in  size: {}\\n\\n\".format(output_layer_in.size()))\n",
    "        out = self.output_layer(out)\n",
    "#         print(\"out layer out  size: {}\\n\\n\".format(out.size()))\n",
    "        \n",
    "        return out,hidden\n",
    "    \n",
    "    def init_hidden(self,batch_size):\n",
    "        '''\n",
    "        h_0 of shape (num_layers * num_directions, batch, hidden_size)\n",
    "        '''\n",
    "#         return torch.zeros(self.num_layers,batch_size,self.hidden_size)\n",
    "        return torch.zeros(self.num_layers,batch_size,self.hidden_size*2)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./data/id2char_dict.pickle\",\"rb\") as f:\n",
    "    id2char_dict = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model(\n",
      "  (embedding_layer): Embedding(3754, 300)\n",
      "  (gru): GRU(300, 256, num_layers=2, dropout=0.5)\n",
      "  (hidden_layer): Linear(in_features=256, out_features=128, bias=True)\n",
      "  (output_layer): Linear(in_features=128, out_features=3754, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = Model(\n",
    "    vocab_size=len(id2char_dict),\n",
    "    embedding_dim=300,\n",
    "    hidden_size=config['hidden_size'],\n",
    "    num_layers=config['num_layers'],\n",
    "    \n",
    ").to(device)\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./data/poetry2id_seqs.pickle\",\"rb\") as f:\n",
    "    poetry2id_seq = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3925, 26)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.array(poetry2id_seq)\n",
    "poetry_num, seq_len = x.shape   #(3925, 26)\n",
    "\n",
    "# new_poetry_num = int(poetry_num/config['batch_size'])\n",
    "\n",
    "# x = x[:config['batch_size']*new_poetry_num]\n",
    "\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "x= torch.from_numpy(np.array(poetry2id_seq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [122, 32, 24]\n",
    "\n",
    "data_loader = Data.DataLoader(\n",
    "    dataset=x,\n",
    "    batch_size=config['batch_size'],\n",
    "    shuffle=True,\n",
    "    num_workers=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  0 | step  50 | train loss: 7.9391\n",
      "Epoch:  0 | step  100 | train loss: 6.6287\n",
      "======== Epoch 0 Toal loss 7.411202244642304 =========\n",
      "Epoch:  1 | step  50 | train loss: 6.2981\n",
      "Epoch:  1 | step  100 | train loss: 6.2227\n",
      "======== Epoch 1 Toal loss 6.296315259080592 =========\n",
      "Epoch:  2 | step  50 | train loss: 6.2223\n",
      "Epoch:  2 | step  100 | train loss: 6.0829\n",
      "======== Epoch 2 Toal loss 6.194153460060678 =========\n",
      "Epoch:  3 | step  50 | train loss: 5.9681\n",
      "Epoch:  3 | step  100 | train loss: 5.7773\n",
      "======== Epoch 3 Toal loss 5.938915140260526 =========\n",
      "Epoch:  4 | step  50 | train loss: 5.6342\n",
      "Epoch:  4 | step  100 | train loss: 5.6361\n",
      "======== Epoch 4 Toal loss 5.687759108659698 =========\n",
      "Epoch:  5 | step  50 | train loss: 5.5790\n",
      "Epoch:  5 | step  100 | train loss: 5.6384\n",
      "======== Epoch 5 Toal loss 5.610454276325257 =========\n",
      "Epoch:  6 | step  50 | train loss: 5.4407\n",
      "Epoch:  6 | step  100 | train loss: 5.5542\n",
      "======== Epoch 6 Toal loss 5.582114657735437 =========\n",
      "Epoch:  7 | step  50 | train loss: 5.6285\n",
      "Epoch:  7 | step  100 | train loss: 5.6558\n",
      "======== Epoch 7 Toal loss 5.565109202532264 =========\n",
      "Epoch:  8 | step  50 | train loss: 5.5479\n",
      "Epoch:  8 | step  100 | train loss: 5.4891\n",
      "======== Epoch 8 Toal loss 5.551357703480294 =========\n",
      "Epoch:  9 | step  50 | train loss: 5.5744\n",
      "Epoch:  9 | step  100 | train loss: 5.5451\n",
      "======== Epoch 9 Toal loss 5.533880629190585 =========\n",
      "Epoch:  10 | step  50 | train loss: 5.5907\n",
      "Epoch:  10 | step  100 | train loss: 5.5279\n",
      "======== Epoch 10 Toal loss 5.514552356750984 =========\n",
      "Epoch:  11 | step  50 | train loss: 5.5112\n",
      "Epoch:  11 | step  100 | train loss: 5.5547\n",
      "======== Epoch 11 Toal loss 5.491896342455856 =========\n",
      "Epoch:  12 | step  50 | train loss: 5.3697\n",
      "Epoch:  12 | step  100 | train loss: 5.5928\n",
      "======== Epoch 12 Toal loss 5.467939113213764 =========\n",
      "Epoch:  13 | step  50 | train loss: 5.5344\n",
      "Epoch:  13 | step  100 | train loss: 5.3120\n",
      "======== Epoch 13 Toal loss 5.442254395988899 =========\n",
      "Epoch:  14 | step  50 | train loss: 5.3438\n",
      "Epoch:  14 | step  100 | train loss: 5.4611\n",
      "======== Epoch 14 Toal loss 5.416835269307702 =========\n",
      "Epoch:  15 | step  50 | train loss: 5.5002\n",
      "Epoch:  15 | step  100 | train loss: 5.3782\n",
      "======== Epoch 15 Toal loss 5.393336908604072 =========\n",
      "Epoch:  16 | step  50 | train loss: 5.3778\n",
      "Epoch:  16 | step  100 | train loss: 5.2779\n",
      "======== Epoch 16 Toal loss 5.370512555285198 =========\n",
      "Epoch:  17 | step  50 | train loss: 5.5089\n",
      "Epoch:  17 | step  100 | train loss: 5.3902\n",
      "======== Epoch 17 Toal loss 5.350128158321225 =========\n",
      "Epoch:  18 | step  50 | train loss: 5.3996\n",
      "Epoch:  18 | step  100 | train loss: 5.3759\n",
      "======== Epoch 18 Toal loss 5.330994311386977 =========\n",
      "Epoch:  19 | step  50 | train loss: 5.4563\n",
      "Epoch:  19 | step  100 | train loss: 5.3304\n",
      "======== Epoch 19 Toal loss 5.312332199841011 =========\n",
      "Epoch:  20 | step  50 | train loss: 5.3654\n",
      "Epoch:  20 | step  100 | train loss: 5.2429\n",
      "======== Epoch 20 Toal loss 5.2958895287862635 =========\n",
      "Epoch:  21 | step  50 | train loss: 5.3408\n",
      "Epoch:  21 | step  100 | train loss: 5.2906\n",
      "======== Epoch 21 Toal loss 5.278329372406006 =========\n",
      "Epoch:  22 | step  50 | train loss: 5.2401\n",
      "Epoch:  22 | step  100 | train loss: 5.3160\n",
      "======== Epoch 22 Toal loss 5.262033175646774 =========\n",
      "Epoch:  23 | step  50 | train loss: 5.1201\n",
      "Epoch:  23 | step  100 | train loss: 5.3290\n",
      "======== Epoch 23 Toal loss 5.246377010655597 =========\n",
      "Epoch:  24 | step  50 | train loss: 5.2806\n",
      "Epoch:  24 | step  100 | train loss: 5.2431\n",
      "======== Epoch 24 Toal loss 5.232369833845433 =========\n",
      "Epoch:  25 | step  50 | train loss: 5.1993\n",
      "Epoch:  25 | step  100 | train loss: 5.1841\n",
      "======== Epoch 25 Toal loss 5.216087465363789 =========\n",
      "Epoch:  26 | step  50 | train loss: 5.1944\n",
      "Epoch:  26 | step  100 | train loss: 5.1761\n",
      "======== Epoch 26 Toal loss 5.201646331849137 =========\n",
      "Epoch:  27 | step  50 | train loss: 5.1251\n",
      "Epoch:  27 | step  100 | train loss: 5.0558\n",
      "======== Epoch 27 Toal loss 5.185301641138588 =========\n",
      "Epoch:  28 | step  50 | train loss: 5.2098\n",
      "Epoch:  28 | step  100 | train loss: 5.1718\n",
      "======== Epoch 28 Toal loss 5.170701147094975 =========\n",
      "Epoch:  29 | step  50 | train loss: 5.0957\n",
      "Epoch:  29 | step  100 | train loss: 5.0884\n",
      "======== Epoch 29 Toal loss 5.155024063296434 =========\n",
      "Epoch:  30 | step  50 | train loss: 5.0320\n",
      "Epoch:  30 | step  100 | train loss: 5.2946\n",
      "======== Epoch 30 Toal loss 5.14057489333114 =========\n",
      "Epoch:  31 | step  50 | train loss: 5.1243\n",
      "Epoch:  31 | step  100 | train loss: 5.1244\n",
      "======== Epoch 31 Toal loss 5.125893639355171 =========\n",
      "Epoch:  32 | step  50 | train loss: 5.0948\n",
      "Epoch:  32 | step  100 | train loss: 4.9855\n",
      "======== Epoch 32 Toal loss 5.1120056485742085 =========\n",
      "Epoch:  33 | step  50 | train loss: 5.1315\n",
      "Epoch:  33 | step  100 | train loss: 5.1098\n",
      "======== Epoch 33 Toal loss 5.0952221203625685 =========\n",
      "Epoch:  34 | step  50 | train loss: 5.0628\n",
      "Epoch:  34 | step  100 | train loss: 5.0465\n",
      "======== Epoch 34 Toal loss 5.080473985129256 =========\n",
      "Epoch:  35 | step  50 | train loss: 5.0351\n",
      "Epoch:  35 | step  100 | train loss: 5.0031\n",
      "======== Epoch 35 Toal loss 5.065756499282712 =========\n",
      "Epoch:  36 | step  50 | train loss: 5.0443\n",
      "Epoch:  36 | step  100 | train loss: 5.0385\n",
      "======== Epoch 36 Toal loss 5.05015044483712 =========\n",
      "Epoch:  37 | step  50 | train loss: 5.0831\n",
      "Epoch:  37 | step  100 | train loss: 5.0673\n",
      "======== Epoch 37 Toal loss 5.036639139904239 =========\n",
      "Epoch:  38 | step  50 | train loss: 5.1070\n",
      "Epoch:  38 | step  100 | train loss: 5.0358\n",
      "======== Epoch 38 Toal loss 5.020133189069546 =========\n",
      "Epoch:  39 | step  50 | train loss: 4.9942\n",
      "Epoch:  39 | step  100 | train loss: 4.9205\n",
      "======== Epoch 39 Toal loss 5.005996037304886 =========\n",
      "Epoch:  40 | step  50 | train loss: 5.0298\n",
      "Epoch:  40 | step  100 | train loss: 4.9965\n",
      "======== Epoch 40 Toal loss 4.992066150758324 =========\n",
      "Epoch:  41 | step  50 | train loss: 5.0159\n",
      "Epoch:  41 | step  100 | train loss: 4.9822\n",
      "======== Epoch 41 Toal loss 4.976577727775264 =========\n",
      "Epoch:  42 | step  50 | train loss: 4.9935\n",
      "Epoch:  42 | step  100 | train loss: 4.8511\n",
      "======== Epoch 42 Toal loss 4.961445777396846 =========\n",
      "Epoch:  43 | step  50 | train loss: 4.8488\n",
      "Epoch:  43 | step  100 | train loss: 5.0555\n",
      "======== Epoch 43 Toal loss 4.947058379165525 =========\n",
      "Epoch:  44 | step  50 | train loss: 4.8451\n",
      "Epoch:  44 | step  100 | train loss: 4.8895\n",
      "======== Epoch 44 Toal loss 4.932639048351505 =========\n",
      "Epoch:  45 | step  50 | train loss: 4.8171\n",
      "Epoch:  45 | step  100 | train loss: 4.8222\n",
      "======== Epoch 45 Toal loss 4.918224865828103 =========\n",
      "Epoch:  46 | step  50 | train loss: 4.8329\n",
      "Epoch:  46 | step  100 | train loss: 4.9533\n",
      "======== Epoch 46 Toal loss 4.904712506426059 =========\n",
      "Epoch:  47 | step  50 | train loss: 4.8894\n",
      "Epoch:  47 | step  100 | train loss: 4.9785\n",
      "======== Epoch 47 Toal loss 4.892258527802258 =========\n",
      "Epoch:  48 | step  50 | train loss: 4.8654\n",
      "Epoch:  48 | step  100 | train loss: 4.8341\n",
      "======== Epoch 48 Toal loss 4.8772047748410605 =========\n",
      "Epoch:  49 | step  50 | train loss: 4.8422\n",
      "Epoch:  49 | step  100 | train loss: 4.9219\n",
      "======== Epoch 49 Toal loss 4.863445944902374 =========\n",
      "Epoch:  50 | step  50 | train loss: 4.7996\n",
      "Epoch:  50 | step  100 | train loss: 4.7259\n",
      "======== Epoch 50 Toal loss 4.8519344019696 =========\n",
      "Epoch:  51 | step  50 | train loss: 4.7892\n",
      "Epoch:  51 | step  100 | train loss: 4.7102\n",
      "======== Epoch 51 Toal loss 4.837850369089018 =========\n",
      "Epoch:  52 | step  50 | train loss: 4.7331\n",
      "Epoch:  52 | step  100 | train loss: 4.8274\n",
      "======== Epoch 52 Toal loss 4.825604314726543 =========\n",
      "Epoch:  53 | step  50 | train loss: 4.6942\n",
      "Epoch:  53 | step  100 | train loss: 4.9489\n",
      "======== Epoch 53 Toal loss 4.813223237913799 =========\n",
      "Epoch:  54 | step  50 | train loss: 4.7489\n",
      "Epoch:  54 | step  100 | train loss: 4.8366\n",
      "======== Epoch 54 Toal loss 4.800593294748446 =========\n",
      "Epoch:  55 | step  50 | train loss: 4.6912\n",
      "Epoch:  55 | step  100 | train loss: 4.8658\n",
      "======== Epoch 55 Toal loss 4.7888409916947525 =========\n",
      "Epoch:  56 | step  50 | train loss: 4.7127\n",
      "Epoch:  56 | step  100 | train loss: 4.7598\n",
      "======== Epoch 56 Toal loss 4.7760771154388175 =========\n",
      "Epoch:  57 | step  50 | train loss: 4.7317\n",
      "Epoch:  57 | step  100 | train loss: 4.7902\n",
      "======== Epoch 57 Toal loss 4.764129661932224 =========\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  58 | step  50 | train loss: 4.7879\n",
      "Epoch:  58 | step  100 | train loss: 4.7491\n",
      "======== Epoch 58 Toal loss 4.753566563614016 =========\n",
      "Epoch:  59 | step  50 | train loss: 4.7452\n",
      "Epoch:  59 | step  100 | train loss: 4.7664\n",
      "======== Epoch 59 Toal loss 4.740584505282767 =========\n",
      "Epoch:  60 | step  50 | train loss: 4.6422\n",
      "Epoch:  60 | step  100 | train loss: 4.7103\n",
      "======== Epoch 60 Toal loss 4.730240407028819 =========\n",
      "Epoch:  61 | step  50 | train loss: 4.9007\n",
      "Epoch:  61 | step  100 | train loss: 4.6777\n",
      "======== Epoch 61 Toal loss 4.718006955898874 =========\n",
      "Epoch:  62 | step  50 | train loss: 4.7318\n",
      "Epoch:  62 | step  100 | train loss: 4.7467\n",
      "======== Epoch 62 Toal loss 4.709270620733742 =========\n",
      "Epoch:  63 | step  50 | train loss: 4.6202\n",
      "Epoch:  63 | step  100 | train loss: 4.6719\n",
      "======== Epoch 63 Toal loss 4.696569322570553 =========\n",
      "Epoch:  64 | step  50 | train loss: 4.6389\n",
      "Epoch:  64 | step  100 | train loss: 4.7294\n",
      "======== Epoch 64 Toal loss 4.686535629799695 =========\n",
      "Epoch:  65 | step  50 | train loss: 4.7907\n",
      "Epoch:  65 | step  100 | train loss: 4.5889\n",
      "======== Epoch 65 Toal loss 4.676445511298451 =========\n",
      "Epoch:  66 | step  50 | train loss: 4.6444\n",
      "Epoch:  66 | step  100 | train loss: 4.6978\n",
      "======== Epoch 66 Toal loss 4.6656274175256245 =========\n",
      "Epoch:  67 | step  50 | train loss: 4.5614\n",
      "Epoch:  67 | step  100 | train loss: 4.5685\n",
      "======== Epoch 67 Toal loss 4.655634178378718 =========\n",
      "Epoch:  68 | step  50 | train loss: 4.4854\n",
      "Epoch:  68 | step  100 | train loss: 4.6304\n",
      "======== Epoch 68 Toal loss 4.647873099257306 =========\n",
      "Epoch:  69 | step  50 | train loss: 4.6296\n",
      "Epoch:  69 | step  100 | train loss: 4.6494\n",
      "======== Epoch 69 Toal loss 4.635885940334661 =========\n",
      "Epoch:  70 | step  50 | train loss: 4.5703\n",
      "Epoch:  70 | step  100 | train loss: 4.5097\n",
      "======== Epoch 70 Toal loss 4.6259074211120605 =========\n",
      "Epoch:  71 | step  50 | train loss: 4.5433\n",
      "Epoch:  71 | step  100 | train loss: 4.6454\n",
      "======== Epoch 71 Toal loss 4.615892790197357 =========\n",
      "Epoch:  72 | step  50 | train loss: 4.5747\n",
      "Epoch:  72 | step  100 | train loss: 4.6416\n",
      "======== Epoch 72 Toal loss 4.608029342279202 =========\n",
      "Epoch:  73 | step  50 | train loss: 4.5911\n",
      "Epoch:  73 | step  100 | train loss: 4.6320\n",
      "======== Epoch 73 Toal loss 4.597584177808064 =========\n",
      "Epoch:  74 | step  50 | train loss: 4.5024\n",
      "Epoch:  74 | step  100 | train loss: 4.6521\n",
      "======== Epoch 74 Toal loss 4.5876205607158385 =========\n",
      "Epoch:  75 | step  50 | train loss: 4.6487\n",
      "Epoch:  75 | step  100 | train loss: 4.5884\n",
      "======== Epoch 75 Toal loss 4.579083454318162 =========\n",
      "Epoch:  76 | step  50 | train loss: 4.3819\n",
      "Epoch:  76 | step  100 | train loss: 4.5983\n",
      "======== Epoch 76 Toal loss 4.571271132647507 =========\n",
      "Epoch:  77 | step  50 | train loss: 4.5600\n",
      "Epoch:  77 | step  100 | train loss: 4.6799\n",
      "======== Epoch 77 Toal loss 4.56012432362006 =========\n",
      "Epoch:  78 | step  50 | train loss: 4.5003\n",
      "Epoch:  78 | step  100 | train loss: 4.6061\n",
      "======== Epoch 78 Toal loss 4.548307321905121 =========\n",
      "Epoch:  79 | step  50 | train loss: 4.5778\n",
      "Epoch:  79 | step  100 | train loss: 4.6083\n",
      "======== Epoch 79 Toal loss 4.540580346332333 =========\n",
      "Epoch:  80 | step  50 | train loss: 4.5295\n",
      "Epoch:  80 | step  100 | train loss: 4.3949\n",
      "======== Epoch 80 Toal loss 4.532247841842776 =========\n",
      "Epoch:  81 | step  50 | train loss: 4.5545\n",
      "Epoch:  81 | step  100 | train loss: 4.3940\n",
      "======== Epoch 81 Toal loss 4.523760935155357 =========\n",
      "Epoch:  82 | step  50 | train loss: 4.5118\n",
      "Epoch:  82 | step  100 | train loss: 4.5784\n",
      "======== Epoch 82 Toal loss 4.513896155163525 =========\n",
      "Epoch:  83 | step  50 | train loss: 4.4567\n",
      "Epoch:  83 | step  100 | train loss: 4.5152\n",
      "======== Epoch 83 Toal loss 4.506612572243543 =========\n",
      "Epoch:  84 | step  50 | train loss: 4.3875\n",
      "Epoch:  84 | step  100 | train loss: 4.5053\n",
      "======== Epoch 84 Toal loss 4.498895780826972 =========\n",
      "Epoch:  85 | step  50 | train loss: 4.5507\n",
      "Epoch:  85 | step  100 | train loss: 4.4121\n",
      "======== Epoch 85 Toal loss 4.489798735796921 =========\n",
      "Epoch:  86 | step  50 | train loss: 4.4103\n",
      "Epoch:  86 | step  100 | train loss: 4.3235\n",
      "======== Epoch 86 Toal loss 4.47955620385767 =========\n",
      "Epoch:  87 | step  50 | train loss: 4.5002\n",
      "Epoch:  87 | step  100 | train loss: 4.4785\n",
      "======== Epoch 87 Toal loss 4.471709561541798 =========\n",
      "Epoch:  88 | step  50 | train loss: 4.6498\n",
      "Epoch:  88 | step  100 | train loss: 4.3941\n",
      "======== Epoch 88 Toal loss 4.460677332994415 =========\n",
      "Epoch:  89 | step  50 | train loss: 4.5412\n",
      "Epoch:  89 | step  100 | train loss: 4.4421\n",
      "======== Epoch 89 Toal loss 4.454760237437923 =========\n",
      "Epoch:  90 | step  50 | train loss: 4.4093\n",
      "Epoch:  90 | step  100 | train loss: 4.3603\n",
      "======== Epoch 90 Toal loss 4.4483365973805995 =========\n",
      "Epoch:  91 | step  50 | train loss: 4.3965\n",
      "Epoch:  91 | step  100 | train loss: 4.6384\n",
      "======== Epoch 91 Toal loss 4.438748068925811 =========\n",
      "Epoch:  92 | step  50 | train loss: 4.3268\n",
      "Epoch:  92 | step  100 | train loss: 4.4535\n",
      "======== Epoch 92 Toal loss 4.430534033271355 =========\n",
      "Epoch:  93 | step  50 | train loss: 4.5150\n",
      "Epoch:  93 | step  100 | train loss: 4.5270\n",
      "======== Epoch 93 Toal loss 4.422052216723682 =========\n",
      "Epoch:  94 | step  50 | train loss: 4.3935\n",
      "Epoch:  94 | step  100 | train loss: 4.4096\n",
      "======== Epoch 94 Toal loss 4.414942446762954 =========\n",
      "Epoch:  95 | step  50 | train loss: 4.3493\n",
      "Epoch:  95 | step  100 | train loss: 4.4941\n",
      "======== Epoch 95 Toal loss 4.406004990988631 =========\n",
      "Epoch:  96 | step  50 | train loss: 4.3952\n",
      "Epoch:  96 | step  100 | train loss: 4.3723\n",
      "======== Epoch 96 Toal loss 4.397860589066172 =========\n",
      "Epoch:  97 | step  50 | train loss: 4.4355\n",
      "Epoch:  97 | step  100 | train loss: 4.4966\n",
      "======== Epoch 97 Toal loss 4.38767088137991 =========\n",
      "Epoch:  98 | step  50 | train loss: 4.3614\n",
      "Epoch:  98 | step  100 | train loss: 4.4155\n",
      "======== Epoch 98 Toal loss 4.3815256366884805 =========\n",
      "Epoch:  99 | step  50 | train loss: 4.4353\n",
      "Epoch:  99 | step  100 | train loss: 4.4020\n",
      "======== Epoch 99 Toal loss 4.373568876002862 =========\n",
      "Epoch:  100 | step  50 | train loss: 4.4421\n",
      "Epoch:  100 | step  100 | train loss: 4.2140\n",
      "======== Epoch 100 Toal loss 4.36396810872768 =========\n",
      "Epoch:  101 | step  50 | train loss: 4.2391\n",
      "Epoch:  101 | step  100 | train loss: 4.3536\n",
      "======== Epoch 101 Toal loss 4.3579733197282 =========\n",
      "Epoch:  102 | step  50 | train loss: 4.3134\n",
      "Epoch:  102 | step  100 | train loss: 4.2424\n",
      "======== Epoch 102 Toal loss 4.35035101960345 =========\n",
      "Epoch:  103 | step  50 | train loss: 4.1737\n",
      "Epoch:  103 | step  100 | train loss: 4.3266\n",
      "======== Epoch 103 Toal loss 4.341230272277584 =========\n",
      "Epoch:  104 | step  50 | train loss: 4.3510\n",
      "Epoch:  104 | step  100 | train loss: 4.3131\n",
      "======== Epoch 104 Toal loss 4.335356421586944 =========\n",
      "Epoch:  105 | step  50 | train loss: 4.2266\n",
      "Epoch:  105 | step  100 | train loss: 4.2983\n",
      "======== Epoch 105 Toal loss 4.327385247238284 =========\n",
      "Epoch:  106 | step  50 | train loss: 4.2399\n",
      "Epoch:  106 | step  100 | train loss: 4.3286\n",
      "======== Epoch 106 Toal loss 4.32079736973212 =========\n",
      "Epoch:  107 | step  50 | train loss: 4.3929\n",
      "Epoch:  107 | step  100 | train loss: 4.3506\n",
      "======== Epoch 107 Toal loss 4.311746922934928 =========\n",
      "Epoch:  108 | step  50 | train loss: 4.3166\n",
      "Epoch:  108 | step  100 | train loss: 4.3408\n",
      "======== Epoch 108 Toal loss 4.304352368765731 =========\n",
      "Epoch:  109 | step  50 | train loss: 4.2603\n",
      "Epoch:  109 | step  100 | train loss: 4.3047\n",
      "======== Epoch 109 Toal loss 4.2958216512106295 =========\n",
      "Epoch:  110 | step  50 | train loss: 4.2788\n",
      "Epoch:  110 | step  100 | train loss: 4.2273\n",
      "======== Epoch 110 Toal loss 4.289644097894188 =========\n",
      "Epoch:  111 | step  50 | train loss: 4.3102\n",
      "Epoch:  111 | step  100 | train loss: 4.1591\n",
      "======== Epoch 111 Toal loss 4.282201557624631 =========\n",
      "Epoch:  112 | step  50 | train loss: 4.4732\n",
      "Epoch:  112 | step  100 | train loss: 4.2974\n",
      "======== Epoch 112 Toal loss 4.272538181242904 =========\n",
      "Epoch:  113 | step  50 | train loss: 4.2714\n",
      "Epoch:  113 | step  100 | train loss: 4.2279\n",
      "======== Epoch 113 Toal loss 4.266835631393805 =========\n",
      "Epoch:  114 | step  50 | train loss: 4.3135\n",
      "Epoch:  114 | step  100 | train loss: 4.2368\n",
      "======== Epoch 114 Toal loss 4.260863063781242 =========\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  115 | step  50 | train loss: 4.2701\n",
      "Epoch:  115 | step  100 | train loss: 4.3604\n",
      "======== Epoch 115 Toal loss 4.25371475917537 =========\n",
      "Epoch:  116 | step  50 | train loss: 4.1797\n",
      "Epoch:  116 | step  100 | train loss: 4.2364\n",
      "======== Epoch 116 Toal loss 4.246181999764791 =========\n",
      "Epoch:  117 | step  50 | train loss: 4.3415\n",
      "Epoch:  117 | step  100 | train loss: 4.2300\n",
      "======== Epoch 117 Toal loss 4.238810682684425 =========\n",
      "Epoch:  118 | step  50 | train loss: 4.1280\n",
      "Epoch:  118 | step  100 | train loss: 4.2008\n",
      "======== Epoch 118 Toal loss 4.232003529866536 =========\n",
      "Epoch:  119 | step  50 | train loss: 4.1675\n",
      "Epoch:  119 | step  100 | train loss: 4.1756\n",
      "======== Epoch 119 Toal loss 4.222343762715657 =========\n",
      "Epoch:  120 | step  50 | train loss: 4.2965\n",
      "Epoch:  120 | step  100 | train loss: 4.1038\n",
      "======== Epoch 120 Toal loss 4.215833260761044 =========\n",
      "Epoch:  121 | step  50 | train loss: 4.1402\n",
      "Epoch:  121 | step  100 | train loss: 4.1942\n",
      "======== Epoch 121 Toal loss 4.207360779366842 =========\n",
      "Epoch:  122 | step  50 | train loss: 4.2540\n",
      "Epoch:  122 | step  100 | train loss: 4.2844\n",
      "======== Epoch 122 Toal loss 4.201227891735915 =========\n",
      "Epoch:  123 | step  50 | train loss: 4.0813\n",
      "Epoch:  123 | step  100 | train loss: 4.2287\n",
      "======== Epoch 123 Toal loss 4.19499964830352 =========\n",
      "Epoch:  124 | step  50 | train loss: 4.1479\n",
      "Epoch:  124 | step  100 | train loss: 4.2619\n",
      "======== Epoch 124 Toal loss 4.187692239032528 =========\n",
      "Epoch:  125 | step  50 | train loss: 4.0594\n",
      "Epoch:  125 | step  100 | train loss: 4.2612\n",
      "======== Epoch 125 Toal loss 4.182232500091801 =========\n",
      "Epoch:  126 | step  50 | train loss: 4.2353\n",
      "Epoch:  126 | step  100 | train loss: 4.1304\n",
      "======== Epoch 126 Toal loss 4.175577186956638 =========\n",
      "Epoch:  127 | step  50 | train loss: 4.2596\n",
      "Epoch:  127 | step  100 | train loss: 4.2295\n",
      "======== Epoch 127 Toal loss 4.164591139894191 =========\n",
      "Epoch:  128 | step  50 | train loss: 4.1390\n",
      "Epoch:  128 | step  100 | train loss: 4.0145\n",
      "======== Epoch 128 Toal loss 4.157868146896362 =========\n",
      "Epoch:  129 | step  50 | train loss: 4.1290\n",
      "Epoch:  129 | step  100 | train loss: 4.1915\n",
      "======== Epoch 129 Toal loss 4.150615665001598 =========\n",
      "Epoch:  130 | step  50 | train loss: 4.0924\n",
      "Epoch:  130 | step  100 | train loss: 4.0783\n",
      "======== Epoch 130 Toal loss 4.143292271993993 =========\n",
      "Epoch:  131 | step  50 | train loss: 4.2986\n",
      "Epoch:  131 | step  100 | train loss: 4.2068\n",
      "======== Epoch 131 Toal loss 4.13963240724269 =========\n",
      "Epoch:  132 | step  50 | train loss: 4.1125\n",
      "Epoch:  132 | step  100 | train loss: 4.2502\n",
      "======== Epoch 132 Toal loss 4.129985344119188 =========\n",
      "Epoch:  133 | step  50 | train loss: 4.0297\n",
      "Epoch:  133 | step  100 | train loss: 4.1935\n",
      "======== Epoch 133 Toal loss 4.12340146351636 =========\n",
      "Epoch:  134 | step  50 | train loss: 4.0710\n",
      "Epoch:  134 | step  100 | train loss: 4.1477\n",
      "======== Epoch 134 Toal loss 4.118038181366959 =========\n",
      "Epoch:  135 | step  50 | train loss: 3.9697\n",
      "Epoch:  135 | step  100 | train loss: 4.0567\n",
      "======== Epoch 135 Toal loss 4.11147021859642 =========\n",
      "Epoch:  136 | step  50 | train loss: 4.1795\n",
      "Epoch:  136 | step  100 | train loss: 4.2026\n",
      "======== Epoch 136 Toal loss 4.104158884141503 =========\n",
      "Epoch:  137 | step  50 | train loss: 4.0279\n",
      "Epoch:  137 | step  100 | train loss: 4.0690\n",
      "======== Epoch 137 Toal loss 4.095218714659776 =========\n",
      "Epoch:  138 | step  50 | train loss: 3.9545\n",
      "Epoch:  138 | step  100 | train loss: 4.1296\n",
      "======== Epoch 138 Toal loss 4.087417290462711 =========\n",
      "Epoch:  139 | step  50 | train loss: 4.0653\n",
      "Epoch:  139 | step  100 | train loss: 3.8956\n",
      "======== Epoch 139 Toal loss 4.084032589827126 =========\n",
      "Epoch:  140 | step  50 | train loss: 4.1942\n",
      "Epoch:  140 | step  100 | train loss: 4.1255\n",
      "======== Epoch 140 Toal loss 4.077639938369999 =========\n",
      "Epoch:  141 | step  50 | train loss: 4.1580\n",
      "Epoch:  141 | step  100 | train loss: 4.0032\n",
      "======== Epoch 141 Toal loss 4.068945223723 =========\n",
      "Epoch:  142 | step  50 | train loss: 4.2019\n",
      "Epoch:  142 | step  100 | train loss: 4.1554\n",
      "======== Epoch 142 Toal loss 4.064215084401573 =========\n",
      "Epoch:  143 | step  50 | train loss: 4.0148\n",
      "Epoch:  143 | step  100 | train loss: 4.1156\n",
      "======== Epoch 143 Toal loss 4.055627735649667 =========\n",
      "Epoch:  144 | step  50 | train loss: 3.9973\n",
      "Epoch:  144 | step  100 | train loss: 4.0765\n",
      "======== Epoch 144 Toal loss 4.049771180967006 =========\n",
      "Epoch:  145 | step  50 | train loss: 4.0731\n",
      "Epoch:  145 | step  100 | train loss: 3.9826\n",
      "======== Epoch 145 Toal loss 4.044499926450776 =========\n",
      "Epoch:  146 | step  50 | train loss: 3.9883\n",
      "Epoch:  146 | step  100 | train loss: 4.0501\n",
      "======== Epoch 146 Toal loss 4.034809106733741 =========\n",
      "Epoch:  147 | step  50 | train loss: 4.1443\n",
      "Epoch:  147 | step  100 | train loss: 4.1364\n",
      "======== Epoch 147 Toal loss 4.029507051638471 =========\n",
      "Epoch:  148 | step  50 | train loss: 3.9935\n",
      "Epoch:  148 | step  100 | train loss: 4.0653\n",
      "======== Epoch 148 Toal loss 4.023763379430383 =========\n",
      "Epoch:  149 | step  50 | train loss: 4.0467\n",
      "Epoch:  149 | step  100 | train loss: 4.2332\n",
      "======== Epoch 149 Toal loss 4.016592744889298 =========\n",
      "Epoch:  150 | step  50 | train loss: 3.9472\n",
      "Epoch:  150 | step  100 | train loss: 3.9852\n",
      "======== Epoch 150 Toal loss 4.010349884265807 =========\n",
      "Epoch:  151 | step  50 | train loss: 4.0508\n",
      "Epoch:  151 | step  100 | train loss: 4.0218\n",
      "======== Epoch 151 Toal loss 4.0033722253349735 =========\n",
      "Epoch:  152 | step  50 | train loss: 4.0618\n",
      "Epoch:  152 | step  100 | train loss: 3.9179\n",
      "======== Epoch 152 Toal loss 3.9940482542766786 =========\n",
      "Epoch:  153 | step  50 | train loss: 3.9887\n",
      "Epoch:  153 | step  100 | train loss: 3.9240\n",
      "======== Epoch 153 Toal loss 3.9853230220515554 =========\n",
      "Epoch:  154 | step  50 | train loss: 3.9904\n",
      "Epoch:  154 | step  100 | train loss: 4.0467\n",
      "======== Epoch 154 Toal loss 3.9801166484026407 =========\n",
      "Epoch:  155 | step  50 | train loss: 4.1013\n",
      "Epoch:  155 | step  100 | train loss: 3.8961\n",
      "======== Epoch 155 Toal loss 3.9772909598621897 =========\n",
      "Epoch:  156 | step  50 | train loss: 3.9065\n",
      "Epoch:  156 | step  100 | train loss: 4.0993\n",
      "======== Epoch 156 Toal loss 3.968285149675075 =========\n",
      "Epoch:  157 | step  50 | train loss: 4.0678\n",
      "Epoch:  157 | step  100 | train loss: 4.1430\n",
      "======== Epoch 157 Toal loss 3.9624507873038937 =========\n",
      "Epoch:  158 | step  50 | train loss: 3.9370\n",
      "Epoch:  158 | step  100 | train loss: 4.0607\n",
      "======== Epoch 158 Toal loss 3.9577196090202023 =========\n",
      "Epoch:  159 | step  50 | train loss: 3.9536\n",
      "Epoch:  159 | step  100 | train loss: 3.9840\n",
      "======== Epoch 159 Toal loss 3.9511833849961198 =========\n",
      "Epoch:  160 | step  50 | train loss: 3.8739\n",
      "Epoch:  160 | step  100 | train loss: 4.0252\n",
      "======== Epoch 160 Toal loss 3.9438353205114844 =========\n",
      "Epoch:  161 | step  50 | train loss: 4.0290\n",
      "Epoch:  161 | step  100 | train loss: 3.8935\n",
      "======== Epoch 161 Toal loss 3.936405664537011 =========\n",
      "Epoch:  162 | step  50 | train loss: 3.9767\n",
      "Epoch:  162 | step  100 | train loss: 3.9799\n",
      "======== Epoch 162 Toal loss 3.9354531377311646 =========\n",
      "Epoch:  163 | step  50 | train loss: 3.8969\n",
      "Epoch:  163 | step  100 | train loss: 3.9306\n",
      "======== Epoch 163 Toal loss 3.9241275477215525 =========\n",
      "Epoch:  164 | step  50 | train loss: 3.9227\n",
      "Epoch:  164 | step  100 | train loss: 3.8779\n",
      "======== Epoch 164 Toal loss 3.9200166454160117 =========\n",
      "Epoch:  165 | step  50 | train loss: 3.8556\n",
      "Epoch:  165 | step  100 | train loss: 3.7363\n",
      "======== Epoch 165 Toal loss 3.9145141365082283 =========\n",
      "Epoch:  166 | step  50 | train loss: 3.9646\n",
      "Epoch:  166 | step  100 | train loss: 3.8487\n",
      "======== Epoch 166 Toal loss 3.9078447120945627 =========\n",
      "Epoch:  167 | step  50 | train loss: 3.8860\n",
      "Epoch:  167 | step  100 | train loss: 3.9093\n",
      "======== Epoch 167 Toal loss 3.9034284653702405 =========\n",
      "Epoch:  168 | step  50 | train loss: 3.8796\n",
      "Epoch:  168 | step  100 | train loss: 3.8621\n",
      "======== Epoch 168 Toal loss 3.892513300345196 =========\n",
      "Epoch:  169 | step  50 | train loss: 3.9473\n",
      "Epoch:  169 | step  100 | train loss: 4.0457\n",
      "======== Epoch 169 Toal loss 3.8884266547071253 =========\n",
      "Epoch:  170 | step  50 | train loss: 3.9232\n",
      "Epoch:  170 | step  100 | train loss: 3.8582\n",
      "======== Epoch 170 Toal loss 3.8800582207315335 =========\n",
      "Epoch:  171 | step  50 | train loss: 3.8433\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  171 | step  100 | train loss: 3.8290\n",
      "======== Epoch 171 Toal loss 3.878551045084387 =========\n",
      "Epoch:  172 | step  50 | train loss: 3.8806\n",
      "Epoch:  172 | step  100 | train loss: 3.9203\n",
      "======== Epoch 172 Toal loss 3.871245393908121 =========\n",
      "Epoch:  173 | step  50 | train loss: 3.8698\n",
      "Epoch:  173 | step  100 | train loss: 3.8540\n",
      "======== Epoch 173 Toal loss 3.863753714212557 =========\n",
      "Epoch:  174 | step  50 | train loss: 3.8109\n",
      "Epoch:  174 | step  100 | train loss: 3.9469\n",
      "======== Epoch 174 Toal loss 3.854089281423305 =========\n",
      "Epoch:  175 | step  50 | train loss: 3.7254\n",
      "Epoch:  175 | step  100 | train loss: 3.9772\n",
      "======== Epoch 175 Toal loss 3.850003849200117 =========\n",
      "Epoch:  176 | step  50 | train loss: 3.7451\n",
      "Epoch:  176 | step  100 | train loss: 3.9763\n",
      "======== Epoch 176 Toal loss 3.842551062746746 =========\n",
      "Epoch:  177 | step  50 | train loss: 3.7132\n",
      "Epoch:  177 | step  100 | train loss: 3.9437\n",
      "======== Epoch 177 Toal loss 3.8358239991878107 =========\n",
      "Epoch:  178 | step  50 | train loss: 3.7941\n",
      "Epoch:  178 | step  100 | train loss: 3.8526\n",
      "======== Epoch 178 Toal loss 3.8314713559499602 =========\n",
      "Epoch:  179 | step  50 | train loss: 3.8505\n",
      "Epoch:  179 | step  100 | train loss: 3.7271\n",
      "======== Epoch 179 Toal loss 3.825759172439575 =========\n",
      "Epoch:  180 | step  50 | train loss: 3.6612\n",
      "Epoch:  180 | step  100 | train loss: 3.7908\n",
      "======== Epoch 180 Toal loss 3.8197066027943682 =========\n",
      "Epoch:  181 | step  50 | train loss: 3.9487\n",
      "Epoch:  181 | step  100 | train loss: 3.7682\n",
      "======== Epoch 181 Toal loss 3.8141757569661956 =========\n",
      "Epoch:  182 | step  50 | train loss: 3.7457\n",
      "Epoch:  182 | step  100 | train loss: 3.8813\n",
      "======== Epoch 182 Toal loss 3.804729672951427 =========\n",
      "Epoch:  183 | step  50 | train loss: 3.7157\n",
      "Epoch:  183 | step  100 | train loss: 3.7863\n",
      "======== Epoch 183 Toal loss 3.7993970595724216 =========\n",
      "Epoch:  184 | step  50 | train loss: 3.8265\n",
      "Epoch:  184 | step  100 | train loss: 3.8149\n",
      "======== Epoch 184 Toal loss 3.7943656638385805 =========\n",
      "Epoch:  185 | step  50 | train loss: 3.7459\n",
      "Epoch:  185 | step  100 | train loss: 3.6615\n",
      "======== Epoch 185 Toal loss 3.7860997498520024 =========\n",
      "Epoch:  186 | step  50 | train loss: 3.6411\n",
      "Epoch:  186 | step  100 | train loss: 3.8016\n",
      "======== Epoch 186 Toal loss 3.7819135169672773 =========\n",
      "Epoch:  187 | step  50 | train loss: 3.7254\n",
      "Epoch:  187 | step  100 | train loss: 3.6224\n",
      "======== Epoch 187 Toal loss 3.7772845117057243 =========\n",
      "Epoch:  188 | step  50 | train loss: 3.9106\n",
      "Epoch:  188 | step  100 | train loss: 3.8160\n",
      "======== Epoch 188 Toal loss 3.7689499447985395 =========\n",
      "Epoch:  189 | step  50 | train loss: 3.8553\n",
      "Epoch:  189 | step  100 | train loss: 3.7094\n",
      "======== Epoch 189 Toal loss 3.76378976814146 =========\n",
      "Epoch:  190 | step  50 | train loss: 3.7457\n",
      "Epoch:  190 | step  100 | train loss: 3.7861\n",
      "======== Epoch 190 Toal loss 3.7594465414683023 =========\n",
      "Epoch:  191 | step  50 | train loss: 3.7928\n",
      "Epoch:  191 | step  100 | train loss: 3.8532\n",
      "======== Epoch 191 Toal loss 3.7499877224123574 =========\n",
      "Epoch:  192 | step  50 | train loss: 3.9116\n",
      "Epoch:  192 | step  100 | train loss: 3.8647\n",
      "======== Epoch 192 Toal loss 3.745349990643137 =========\n",
      "Epoch:  193 | step  50 | train loss: 3.8355\n",
      "Epoch:  193 | step  100 | train loss: 3.8148\n",
      "======== Epoch 193 Toal loss 3.7402220043709606 =========\n",
      "Epoch:  194 | step  50 | train loss: 3.6843\n",
      "Epoch:  194 | step  100 | train loss: 3.7578\n",
      "======== Epoch 194 Toal loss 3.7350252876436807 =========\n",
      "Epoch:  195 | step  50 | train loss: 3.6443\n",
      "Epoch:  195 | step  100 | train loss: 3.6428\n",
      "======== Epoch 195 Toal loss 3.729183720379341 =========\n",
      "Epoch:  196 | step  50 | train loss: 3.6394\n",
      "Epoch:  196 | step  100 | train loss: 3.7145\n",
      "======== Epoch 196 Toal loss 3.7244969674242223 =========\n",
      "Epoch:  197 | step  50 | train loss: 3.7956\n",
      "Epoch:  197 | step  100 | train loss: 3.7978\n",
      "======== Epoch 197 Toal loss 3.7158432006835938 =========\n",
      "Epoch:  198 | step  50 | train loss: 3.7786\n",
      "Epoch:  198 | step  100 | train loss: 3.7369\n",
      "======== Epoch 198 Toal loss 3.7138334619320506 =========\n",
      "Epoch:  199 | step  50 | train loss: 3.5956\n",
      "Epoch:  199 | step  100 | train loss: 3.5621\n",
      "======== Epoch 199 Toal loss 3.7022164197472054 =========\n",
      "Epoch:  200 | step  50 | train loss: 3.7711\n",
      "Epoch:  200 | step  100 | train loss: 3.6569\n",
      "======== Epoch 200 Toal loss 3.6996793979551734 =========\n",
      "Epoch:  201 | step  50 | train loss: 3.6267\n",
      "Epoch:  201 | step  100 | train loss: 3.8436\n",
      "======== Epoch 201 Toal loss 3.6909261195640255 =========\n",
      "Epoch:  202 | step  50 | train loss: 3.9432\n",
      "Epoch:  202 | step  100 | train loss: 3.7534\n",
      "======== Epoch 202 Toal loss 3.6885588324166894 =========\n",
      "Epoch:  203 | step  50 | train loss: 3.6983\n",
      "Epoch:  203 | step  100 | train loss: 3.9020\n",
      "======== Epoch 203 Toal loss 3.681168897365167 =========\n",
      "Epoch:  204 | step  50 | train loss: 3.6569\n",
      "Epoch:  204 | step  100 | train loss: 3.6088\n",
      "======== Epoch 204 Toal loss 3.673878334402069 =========\n",
      "Epoch:  205 | step  50 | train loss: 3.5494\n",
      "Epoch:  205 | step  100 | train loss: 3.7672\n",
      "======== Epoch 205 Toal loss 3.671843732275614 =========\n",
      "Epoch:  206 | step  50 | train loss: 3.7196\n",
      "Epoch:  206 | step  100 | train loss: 3.6207\n",
      "======== Epoch 206 Toal loss 3.665877297641785 =========\n",
      "Epoch:  207 | step  50 | train loss: 3.7922\n",
      "Epoch:  207 | step  100 | train loss: 3.6892\n",
      "======== Epoch 207 Toal loss 3.658766998508112 =========\n",
      "Epoch:  208 | step  50 | train loss: 3.5388\n",
      "Epoch:  208 | step  100 | train loss: 3.4898\n",
      "======== Epoch 208 Toal loss 3.6502986845931384 =========\n",
      "Epoch:  209 | step  50 | train loss: 3.6993\n",
      "Epoch:  209 | step  100 | train loss: 3.7505\n",
      "======== Epoch 209 Toal loss 3.6478722425011116 =========\n",
      "Epoch:  210 | step  50 | train loss: 3.6157\n",
      "Epoch:  210 | step  100 | train loss: 3.6583\n",
      "======== Epoch 210 Toal loss 3.640968855803575 =========\n",
      "Epoch:  211 | step  50 | train loss: 3.5203\n",
      "Epoch:  211 | step  100 | train loss: 3.6132\n",
      "======== Epoch 211 Toal loss 3.636600694036096 =========\n",
      "Epoch:  212 | step  50 | train loss: 3.4734\n",
      "Epoch:  212 | step  100 | train loss: 3.6507\n",
      "======== Epoch 212 Toal loss 3.6295554289003698 =========\n",
      "Epoch:  213 | step  50 | train loss: 3.6244\n",
      "Epoch:  213 | step  100 | train loss: 3.6737\n",
      "======== Epoch 213 Toal loss 3.622381504958238 =========\n",
      "Epoch:  214 | step  50 | train loss: 3.5460\n",
      "Epoch:  214 | step  100 | train loss: 3.6016\n",
      "======== Epoch 214 Toal loss 3.616085705718374 =========\n",
      "Epoch:  215 | step  50 | train loss: 3.6091\n",
      "Epoch:  215 | step  100 | train loss: 3.5314\n",
      "======== Epoch 215 Toal loss 3.614077350957607 =========\n",
      "Epoch:  216 | step  50 | train loss: 3.6813\n",
      "Epoch:  216 | step  100 | train loss: 3.5741\n",
      "======== Epoch 216 Toal loss 3.607633751582324 =========\n",
      "Epoch:  217 | step  50 | train loss: 3.7347\n",
      "Epoch:  217 | step  100 | train loss: 3.4385\n",
      "======== Epoch 217 Toal loss 3.600750742889032 =========\n",
      "Epoch:  218 | step  50 | train loss: 3.6369\n",
      "Epoch:  218 | step  100 | train loss: 3.7382\n",
      "======== Epoch 218 Toal loss 3.595800729301887 =========\n",
      "Epoch:  219 | step  50 | train loss: 3.6870\n",
      "Epoch:  219 | step  100 | train loss: 3.6796\n",
      "======== Epoch 219 Toal loss 3.58847315718488 =========\n",
      "Epoch:  220 | step  50 | train loss: 3.5928\n",
      "Epoch:  220 | step  100 | train loss: 3.5787\n",
      "======== Epoch 220 Toal loss 3.5816843373988703 =========\n",
      "Epoch:  221 | step  50 | train loss: 3.6475\n",
      "Epoch:  221 | step  100 | train loss: 3.5780\n",
      "======== Epoch 221 Toal loss 3.579391460108563 =========\n",
      "Epoch:  222 | step  50 | train loss: 3.7348\n",
      "Epoch:  222 | step  100 | train loss: 3.7030\n",
      "======== Epoch 222 Toal loss 3.5711544723045536 =========\n",
      "Epoch:  223 | step  50 | train loss: 3.6830\n",
      "Epoch:  223 | step  100 | train loss: 3.5700\n",
      "======== Epoch 223 Toal loss 3.566055763058546 =========\n",
      "Epoch:  224 | step  50 | train loss: 3.6527\n",
      "Epoch:  224 | step  100 | train loss: 3.6524\n",
      "======== Epoch 224 Toal loss 3.562052333258032 =========\n",
      "Epoch:  225 | step  50 | train loss: 3.5611\n",
      "Epoch:  225 | step  100 | train loss: 3.6825\n",
      "======== Epoch 225 Toal loss 3.556009263527103 =========\n",
      "Epoch:  226 | step  50 | train loss: 3.7875\n",
      "Epoch:  226 | step  100 | train loss: 3.5927\n",
      "======== Epoch 226 Toal loss 3.5543270033549486 =========\n",
      "Epoch:  227 | step  50 | train loss: 3.5551\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  227 | step  100 | train loss: 3.5530\n",
      "======== Epoch 227 Toal loss 3.542559236045775 =========\n",
      "Epoch:  228 | step  50 | train loss: 3.5171\n",
      "Epoch:  228 | step  100 | train loss: 3.4592\n",
      "======== Epoch 228 Toal loss 3.537996673971657 =========\n",
      "Epoch:  229 | step  50 | train loss: 3.5927\n",
      "Epoch:  229 | step  100 | train loss: 3.5818\n",
      "======== Epoch 229 Toal loss 3.532080636761053 =========\n",
      "Epoch:  230 | step  50 | train loss: 3.4928\n",
      "Epoch:  230 | step  100 | train loss: 3.6520\n",
      "======== Epoch 230 Toal loss 3.528750489397747 =========\n",
      "Epoch:  231 | step  50 | train loss: 3.4024\n",
      "Epoch:  231 | step  100 | train loss: 3.5513\n",
      "======== Epoch 231 Toal loss 3.5208140679491247 =========\n",
      "Epoch:  232 | step  50 | train loss: 3.5713\n",
      "Epoch:  232 | step  100 | train loss: 3.4457\n",
      "======== Epoch 232 Toal loss 3.5170798921972755 =========\n",
      "Epoch:  233 | step  50 | train loss: 3.3891\n",
      "Epoch:  233 | step  100 | train loss: 3.4612\n",
      "======== Epoch 233 Toal loss 3.5126015035117546 =========\n",
      "Epoch:  234 | step  50 | train loss: 3.5096\n",
      "Epoch:  234 | step  100 | train loss: 3.4909\n",
      "======== Epoch 234 Toal loss 3.507097269461407 =========\n",
      "Epoch:  235 | step  50 | train loss: 3.5568\n",
      "Epoch:  235 | step  100 | train loss: 3.4916\n",
      "======== Epoch 235 Toal loss 3.501741734946646 =========\n",
      "Epoch:  236 | step  50 | train loss: 3.5075\n",
      "Epoch:  236 | step  100 | train loss: 3.5586\n",
      "======== Epoch 236 Toal loss 3.498430013656616 =========\n",
      "Epoch:  237 | step  50 | train loss: 3.4854\n",
      "Epoch:  237 | step  100 | train loss: 3.5852\n",
      "======== Epoch 237 Toal loss 3.4889708864010447 =========\n",
      "Epoch:  238 | step  50 | train loss: 3.4351\n",
      "Epoch:  238 | step  100 | train loss: 3.4347\n",
      "======== Epoch 238 Toal loss 3.4840217218166445 =========\n",
      "Epoch:  239 | step  50 | train loss: 3.4780\n",
      "Epoch:  239 | step  100 | train loss: 3.4161\n",
      "======== Epoch 239 Toal loss 3.4766331533106363 =========\n",
      "Epoch:  240 | step  50 | train loss: 3.4676\n",
      "Epoch:  240 | step  100 | train loss: 3.5964\n",
      "======== Epoch 240 Toal loss 3.471509660162577 =========\n",
      "Epoch:  241 | step  50 | train loss: 3.5529\n",
      "Epoch:  241 | step  100 | train loss: 3.5698\n",
      "======== Epoch 241 Toal loss 3.4673653656874244 =========\n",
      "Epoch:  242 | step  50 | train loss: 3.4013\n",
      "Epoch:  242 | step  100 | train loss: 3.3811\n",
      "======== Epoch 242 Toal loss 3.462197340600859 =========\n",
      "Epoch:  243 | step  50 | train loss: 3.3543\n",
      "Epoch:  243 | step  100 | train loss: 3.4100\n",
      "======== Epoch 243 Toal loss 3.451342875395364 =========\n",
      "Epoch:  244 | step  50 | train loss: 3.3770\n",
      "Epoch:  244 | step  100 | train loss: 3.5430\n",
      "======== Epoch 244 Toal loss 3.4520559446598456 =========\n",
      "Epoch:  245 | step  50 | train loss: 3.4331\n",
      "Epoch:  245 | step  100 | train loss: 3.1989\n",
      "======== Epoch 245 Toal loss 3.4452757040659585 =========\n",
      "Epoch:  246 | step  50 | train loss: 3.4029\n",
      "Epoch:  246 | step  100 | train loss: 3.4597\n",
      "======== Epoch 246 Toal loss 3.440237305028652 =========\n",
      "Epoch:  247 | step  50 | train loss: 3.2967\n",
      "Epoch:  247 | step  100 | train loss: 3.3353\n",
      "======== Epoch 247 Toal loss 3.438129834043301 =========\n",
      "Epoch:  248 | step  50 | train loss: 3.4529\n",
      "Epoch:  248 | step  100 | train loss: 3.5126\n",
      "======== Epoch 248 Toal loss 3.4268484600191194 =========\n",
      "Epoch:  249 | step  50 | train loss: 3.3030\n",
      "Epoch:  249 | step  100 | train loss: 3.3964\n",
      "======== Epoch 249 Toal loss 3.425757063113577 =========\n",
      "Epoch:  250 | step  50 | train loss: 3.5221\n",
      "Epoch:  250 | step  100 | train loss: 3.4419\n",
      "======== Epoch 250 Toal loss 3.420533829588231 =========\n",
      "Epoch:  251 | step  50 | train loss: 3.4613\n",
      "Epoch:  251 | step  100 | train loss: 3.3411\n",
      "======== Epoch 251 Toal loss 3.4173138238550202 =========\n",
      "Epoch:  252 | step  50 | train loss: 3.3083\n",
      "Epoch:  252 | step  100 | train loss: 3.3685\n",
      "======== Epoch 252 Toal loss 3.4069565815654226 =========\n",
      "Epoch:  253 | step  50 | train loss: 3.3862\n",
      "Epoch:  253 | step  100 | train loss: 3.3683\n",
      "======== Epoch 253 Toal loss 3.4045227388056314 =========\n",
      "Epoch:  254 | step  50 | train loss: 3.1897\n",
      "Epoch:  254 | step  100 | train loss: 3.4577\n",
      "======== Epoch 254 Toal loss 3.398509077909516 =========\n",
      "Epoch:  255 | step  50 | train loss: 3.3890\n",
      "Epoch:  255 | step  100 | train loss: 3.3928\n",
      "======== Epoch 255 Toal loss 3.3942116024048348 =========\n",
      "Epoch:  256 | step  50 | train loss: 3.4229\n",
      "Epoch:  256 | step  100 | train loss: 3.4873\n",
      "======== Epoch 256 Toal loss 3.3881815933599704 =========\n",
      "Epoch:  257 | step  50 | train loss: 3.1452\n",
      "Epoch:  257 | step  100 | train loss: 3.3235\n",
      "======== Epoch 257 Toal loss 3.381421914914759 =========\n",
      "Epoch:  258 | step  50 | train loss: 3.5292\n",
      "Epoch:  258 | step  100 | train loss: 3.4608\n",
      "======== Epoch 258 Toal loss 3.3775227631979843 =========\n",
      "Epoch:  259 | step  50 | train loss: 3.3310\n",
      "Epoch:  259 | step  100 | train loss: 3.5726\n",
      "======== Epoch 259 Toal loss 3.371263364466225 =========\n",
      "Epoch:  260 | step  50 | train loss: 3.2776\n",
      "Epoch:  260 | step  100 | train loss: 3.2879\n",
      "======== Epoch 260 Toal loss 3.3696602243718092 =========\n",
      "Epoch:  261 | step  50 | train loss: 3.3819\n",
      "Epoch:  261 | step  100 | train loss: 3.4372\n",
      "======== Epoch 261 Toal loss 3.366021745573214 =========\n",
      "Epoch:  262 | step  50 | train loss: 3.3623\n",
      "Epoch:  262 | step  100 | train loss: 3.4955\n",
      "======== Epoch 262 Toal loss 3.356084395230301 =========\n",
      "Epoch:  263 | step  50 | train loss: 3.3043\n",
      "Epoch:  263 | step  100 | train loss: 3.3779\n",
      "======== Epoch 263 Toal loss 3.3521968795032038 =========\n",
      "Epoch:  264 | step  50 | train loss: 3.3714\n",
      "Epoch:  264 | step  100 | train loss: 3.3111\n",
      "======== Epoch 264 Toal loss 3.3486666815067694 =========\n",
      "Epoch:  265 | step  50 | train loss: 3.3709\n",
      "Epoch:  265 | step  100 | train loss: 3.4042\n",
      "======== Epoch 265 Toal loss 3.340695693240902 =========\n",
      "Epoch:  266 | step  50 | train loss: 3.2743\n",
      "Epoch:  266 | step  100 | train loss: 3.4072\n",
      "======== Epoch 266 Toal loss 3.3359660966609552 =========\n",
      "Epoch:  267 | step  50 | train loss: 3.2898\n",
      "Epoch:  267 | step  100 | train loss: 3.2772\n",
      "======== Epoch 267 Toal loss 3.327140548364903 =========\n",
      "Epoch:  268 | step  50 | train loss: 3.2983\n",
      "Epoch:  268 | step  100 | train loss: 3.4036\n",
      "======== Epoch 268 Toal loss 3.3216083243610415 =========\n",
      "Epoch:  269 | step  50 | train loss: 3.3063\n",
      "Epoch:  269 | step  100 | train loss: 3.3789\n",
      "======== Epoch 269 Toal loss 3.3232245096346227 =========\n",
      "Epoch:  270 | step  50 | train loss: 3.2368\n",
      "Epoch:  270 | step  100 | train loss: 3.3059\n",
      "======== Epoch 270 Toal loss 3.314441361078402 =========\n",
      "Epoch:  271 | step  50 | train loss: 3.2689\n",
      "Epoch:  271 | step  100 | train loss: 3.3117\n",
      "======== Epoch 271 Toal loss 3.3055586349673387 =========\n",
      "Epoch:  272 | step  50 | train loss: 3.3319\n",
      "Epoch:  272 | step  100 | train loss: 3.3290\n",
      "======== Epoch 272 Toal loss 3.305544663250931 =========\n",
      "Epoch:  273 | step  50 | train loss: 3.2689\n",
      "Epoch:  273 | step  100 | train loss: 3.3608\n",
      "======== Epoch 273 Toal loss 3.2970186908070636 =========\n",
      "Epoch:  274 | step  50 | train loss: 3.4171\n",
      "Epoch:  274 | step  100 | train loss: 3.2130\n",
      "======== Epoch 274 Toal loss 3.2919334027825333 =========\n",
      "Epoch:  275 | step  50 | train loss: 3.2286\n",
      "Epoch:  275 | step  100 | train loss: 3.2738\n",
      "======== Epoch 275 Toal loss 3.29309401667215 =========\n",
      "Epoch:  276 | step  50 | train loss: 3.3950\n",
      "Epoch:  276 | step  100 | train loss: 3.2612\n",
      "======== Epoch 276 Toal loss 3.283683943554638 =========\n",
      "Epoch:  277 | step  50 | train loss: 3.3209\n",
      "Epoch:  277 | step  100 | train loss: 3.4691\n",
      "======== Epoch 277 Toal loss 3.2816928906169363 =========\n",
      "Epoch:  278 | step  50 | train loss: 3.1985\n",
      "Epoch:  278 | step  100 | train loss: 3.4359\n",
      "======== Epoch 278 Toal loss 3.274482639824472 =========\n",
      "Epoch:  279 | step  50 | train loss: 3.2392\n",
      "Epoch:  279 | step  100 | train loss: 3.3504\n",
      "======== Epoch 279 Toal loss 3.265707043128285 =========\n",
      "Epoch:  280 | step  50 | train loss: 3.2632\n",
      "Epoch:  280 | step  100 | train loss: 3.2182\n",
      "======== Epoch 280 Toal loss 3.2639126622579933 =========\n",
      "Epoch:  281 | step  50 | train loss: 3.3174\n",
      "Epoch:  281 | step  100 | train loss: 3.3617\n",
      "======== Epoch 281 Toal loss 3.2587404929525485 =========\n",
      "Epoch:  282 | step  50 | train loss: 3.3382\n",
      "Epoch:  282 | step  100 | train loss: 3.2951\n",
      "======== Epoch 282 Toal loss 3.252138440201922 =========\n",
      "Epoch:  283 | step  50 | train loss: 3.3747\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  283 | step  100 | train loss: 3.3963\n",
      "======== Epoch 283 Toal loss 3.2477489293106205 =========\n",
      "Epoch:  284 | step  50 | train loss: 3.2619\n",
      "Epoch:  284 | step  100 | train loss: 3.2042\n",
      "======== Epoch 284 Toal loss 3.24456922600909 =========\n",
      "Epoch:  285 | step  50 | train loss: 3.2034\n",
      "Epoch:  285 | step  100 | train loss: 3.2922\n",
      "======== Epoch 285 Toal loss 3.2374287756477913 =========\n",
      "Epoch:  286 | step  50 | train loss: 3.1986\n",
      "Epoch:  286 | step  100 | train loss: 3.3400\n",
      "======== Epoch 286 Toal loss 3.2350135101535455 =========\n",
      "Epoch:  287 | step  50 | train loss: 3.1810\n",
      "Epoch:  287 | step  100 | train loss: 3.3092\n",
      "======== Epoch 287 Toal loss 3.229684223004473 =========\n",
      "Epoch:  288 | step  50 | train loss: 3.3982\n",
      "Epoch:  288 | step  100 | train loss: 3.3215\n",
      "======== Epoch 288 Toal loss 3.221318361235828 =========\n",
      "Epoch:  289 | step  50 | train loss: 3.2280\n",
      "Epoch:  289 | step  100 | train loss: 3.4899\n",
      "======== Epoch 289 Toal loss 3.2205754314980855 =========\n",
      "Epoch:  290 | step  50 | train loss: 3.1369\n",
      "Epoch:  290 | step  100 | train loss: 3.0878\n",
      "======== Epoch 290 Toal loss 3.215438104257351 =========\n",
      "Epoch:  291 | step  50 | train loss: 3.2210\n",
      "Epoch:  291 | step  100 | train loss: 3.1748\n",
      "======== Epoch 291 Toal loss 3.209745100843228 =========\n",
      "Epoch:  292 | step  50 | train loss: 3.3532\n",
      "Epoch:  292 | step  100 | train loss: 3.2910\n",
      "======== Epoch 292 Toal loss 3.2062668955422997 =========\n",
      "Epoch:  293 | step  50 | train loss: 3.2045\n",
      "Epoch:  293 | step  100 | train loss: 3.0803\n",
      "======== Epoch 293 Toal loss 3.2014236915402297 =========\n",
      "Epoch:  294 | step  50 | train loss: 3.1793\n",
      "Epoch:  294 | step  100 | train loss: 3.2728\n",
      "======== Epoch 294 Toal loss 3.1942776354347786 =========\n",
      "Epoch:  295 | step  50 | train loss: 3.2377\n",
      "Epoch:  295 | step  100 | train loss: 3.1164\n",
      "======== Epoch 295 Toal loss 3.1908693856340116 =========\n",
      "Epoch:  296 | step  50 | train loss: 3.1556\n",
      "Epoch:  296 | step  100 | train loss: 3.0702\n",
      "======== Epoch 296 Toal loss 3.1788603658598613 =========\n",
      "Epoch:  297 | step  50 | train loss: 3.0498\n",
      "Epoch:  297 | step  100 | train loss: 3.3006\n",
      "======== Epoch 297 Toal loss 3.1836522090725783 =========\n",
      "Epoch:  298 | step  50 | train loss: 3.2890\n",
      "Epoch:  298 | step  100 | train loss: 3.2305\n",
      "======== Epoch 298 Toal loss 3.178428870875661 =========\n",
      "Epoch:  299 | step  50 | train loss: 3.1030\n",
      "Epoch:  299 | step  100 | train loss: 3.0950\n",
      "======== Epoch 299 Toal loss 3.167099130832083 =========\n",
      "Epoch:  300 | step  50 | train loss: 3.2644\n",
      "Epoch:  300 | step  100 | train loss: 3.2700\n",
      "======== Epoch 300 Toal loss 3.1664755111787377 =========\n",
      "Epoch:  301 | step  50 | train loss: 3.2421\n",
      "Epoch:  301 | step  100 | train loss: 3.1067\n",
      "======== Epoch 301 Toal loss 3.161953509338503 =========\n",
      "Epoch:  302 | step  50 | train loss: 3.2223\n",
      "Epoch:  302 | step  100 | train loss: 3.2060\n",
      "======== Epoch 302 Toal loss 3.158767742839286 =========\n",
      "Epoch:  303 | step  50 | train loss: 3.1303\n",
      "Epoch:  303 | step  100 | train loss: 3.1568\n",
      "======== Epoch 303 Toal loss 3.1510747835888124 =========\n",
      "Epoch:  304 | step  50 | train loss: 3.2007\n",
      "Epoch:  304 | step  100 | train loss: 3.1795\n",
      "======== Epoch 304 Toal loss 3.1458272449369353 =========\n",
      "Epoch:  305 | step  50 | train loss: 3.2589\n",
      "Epoch:  305 | step  100 | train loss: 3.1807\n",
      "======== Epoch 305 Toal loss 3.1438444067792193 =========\n",
      "Epoch:  306 | step  50 | train loss: 3.1681\n",
      "Epoch:  306 | step  100 | train loss: 3.1875\n",
      "======== Epoch 306 Toal loss 3.1366102055805487 =========\n",
      "Epoch:  307 | step  50 | train loss: 3.1283\n",
      "Epoch:  307 | step  100 | train loss: 3.0089\n",
      "======== Epoch 307 Toal loss 3.1339979346205546 =========\n",
      "Epoch:  308 | step  50 | train loss: 3.2370\n",
      "Epoch:  308 | step  100 | train loss: 3.1562\n",
      "======== Epoch 308 Toal loss 3.1278825911079964 =========\n",
      "Epoch:  309 | step  50 | train loss: 3.1380\n",
      "Epoch:  309 | step  100 | train loss: 3.1672\n",
      "======== Epoch 309 Toal loss 3.1238832396220384 =========\n",
      "Epoch:  310 | step  50 | train loss: 3.2507\n",
      "Epoch:  310 | step  100 | train loss: 3.0993\n",
      "======== Epoch 310 Toal loss 3.1206331388737127 =========\n",
      "Epoch:  311 | step  50 | train loss: 2.9859\n",
      "Epoch:  311 | step  100 | train loss: 3.2091\n",
      "======== Epoch 311 Toal loss 3.114025835099259 =========\n",
      "Epoch:  312 | step  50 | train loss: 3.1861\n",
      "Epoch:  312 | step  100 | train loss: 3.1142\n",
      "======== Epoch 312 Toal loss 3.109111148167432 =========\n",
      "Epoch:  313 | step  50 | train loss: 3.1300\n",
      "Epoch:  313 | step  100 | train loss: 3.1151\n",
      "======== Epoch 313 Toal loss 3.106847352128688 =========\n",
      "Epoch:  314 | step  50 | train loss: 3.0033\n",
      "Epoch:  314 | step  100 | train loss: 3.0377\n",
      "======== Epoch 314 Toal loss 3.1003532874874953 =========\n",
      "Epoch:  315 | step  50 | train loss: 3.1315\n",
      "Epoch:  315 | step  100 | train loss: 3.0705\n",
      "======== Epoch 315 Toal loss 3.096490991793997 =========\n",
      "Epoch:  316 | step  50 | train loss: 3.0638\n",
      "Epoch:  316 | step  100 | train loss: 3.0247\n",
      "======== Epoch 316 Toal loss 3.0945979211388566 =========\n",
      "Epoch:  317 | step  50 | train loss: 2.9101\n",
      "Epoch:  317 | step  100 | train loss: 3.1405\n",
      "======== Epoch 317 Toal loss 3.0870848128466104 =========\n",
      "Epoch:  318 | step  50 | train loss: 2.9820\n",
      "Epoch:  318 | step  100 | train loss: 3.1243\n",
      "======== Epoch 318 Toal loss 3.079389006141725 =========\n",
      "Epoch:  319 | step  50 | train loss: 2.9989\n",
      "Epoch:  319 | step  100 | train loss: 3.0619\n",
      "======== Epoch 319 Toal loss 3.077695015000134 =========\n",
      "Epoch:  320 | step  50 | train loss: 2.9960\n",
      "Epoch:  320 | step  100 | train loss: 3.0969\n",
      "======== Epoch 320 Toal loss 3.0719414222531203 =========\n",
      "Epoch:  321 | step  50 | train loss: 3.0184\n",
      "Epoch:  321 | step  100 | train loss: 3.1077\n",
      "======== Epoch 321 Toal loss 3.066934643722162 =========\n",
      "Epoch:  322 | step  50 | train loss: 3.1008\n",
      "Epoch:  322 | step  100 | train loss: 2.9822\n",
      "======== Epoch 322 Toal loss 3.0633479618444674 =========\n",
      "Epoch:  323 | step  50 | train loss: 3.0856\n",
      "Epoch:  323 | step  100 | train loss: 3.1764\n",
      "======== Epoch 323 Toal loss 3.057630193911917 =========\n",
      "Epoch:  324 | step  50 | train loss: 3.0373\n",
      "Epoch:  324 | step  100 | train loss: 3.1699\n",
      "======== Epoch 324 Toal loss 3.054067045692506 =========\n",
      "Epoch:  325 | step  50 | train loss: 3.0549\n",
      "Epoch:  325 | step  100 | train loss: 3.0911\n",
      "======== Epoch 325 Toal loss 3.047876677862028 =========\n",
      "Epoch:  326 | step  50 | train loss: 2.9557\n",
      "Epoch:  326 | step  100 | train loss: 3.0055\n",
      "======== Epoch 326 Toal loss 3.049149962944713 =========\n",
      "Epoch:  327 | step  50 | train loss: 3.0573\n",
      "Epoch:  327 | step  100 | train loss: 3.0636\n",
      "======== Epoch 327 Toal loss 3.042509416254555 =========\n",
      "Epoch:  328 | step  50 | train loss: 3.1177\n",
      "Epoch:  328 | step  100 | train loss: 2.8467\n",
      "======== Epoch 328 Toal loss 3.0357146301889806 =========\n",
      "Epoch:  329 | step  50 | train loss: 3.0946\n",
      "Epoch:  329 | step  100 | train loss: 3.0539\n",
      "======== Epoch 329 Toal loss 3.0294140315637357 =========\n",
      "Epoch:  330 | step  50 | train loss: 2.9803\n",
      "Epoch:  330 | step  100 | train loss: 2.8877\n",
      "======== Epoch 330 Toal loss 3.025890466643543 =========\n",
      "Epoch:  331 | step  50 | train loss: 2.9839\n",
      "Epoch:  331 | step  100 | train loss: 2.8713\n",
      "======== Epoch 331 Toal loss 3.0210346322718675 =========\n",
      "Epoch:  332 | step  50 | train loss: 2.8841\n",
      "Epoch:  332 | step  100 | train loss: 2.9300\n",
      "======== Epoch 332 Toal loss 3.016474341958519 =========\n",
      "Epoch:  333 | step  50 | train loss: 3.1022\n",
      "Epoch:  333 | step  100 | train loss: 3.0544\n",
      "======== Epoch 333 Toal loss 3.0150879107839694 =========\n",
      "Epoch:  334 | step  50 | train loss: 3.1145\n",
      "Epoch:  334 | step  100 | train loss: 3.2331\n",
      "======== Epoch 334 Toal loss 3.0108464985359005 =========\n",
      "Epoch:  335 | step  50 | train loss: 3.1862\n",
      "Epoch:  335 | step  100 | train loss: 3.0105\n",
      "======== Epoch 335 Toal loss 3.0012322073060322 =========\n",
      "Epoch:  336 | step  50 | train loss: 3.0802\n",
      "Epoch:  336 | step  100 | train loss: 3.1911\n",
      "======== Epoch 336 Toal loss 3.001117324441429 =========\n",
      "Epoch:  337 | step  50 | train loss: 2.9734\n",
      "Epoch:  337 | step  100 | train loss: 3.1839\n",
      "======== Epoch 337 Toal loss 2.9932070553787358 =========\n",
      "Epoch:  338 | step  50 | train loss: 2.9826\n",
      "Epoch:  338 | step  100 | train loss: 3.2136\n",
      "======== Epoch 338 Toal loss 2.9896187278313366 =========\n",
      "Epoch:  339 | step  50 | train loss: 3.0710\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  339 | step  100 | train loss: 2.8676\n",
      "======== Epoch 339 Toal loss 2.9849842544493637 =========\n",
      "Epoch:  340 | step  50 | train loss: 2.9730\n",
      "Epoch:  340 | step  100 | train loss: 3.1834\n",
      "======== Epoch 340 Toal loss 2.980473051226236 =========\n",
      "Epoch:  341 | step  50 | train loss: 3.0619\n",
      "Epoch:  341 | step  100 | train loss: 2.9608\n",
      "======== Epoch 341 Toal loss 2.9775431737667177 =========\n",
      "Epoch:  342 | step  50 | train loss: 2.9445\n",
      "Epoch:  342 | step  100 | train loss: 3.0687\n",
      "======== Epoch 342 Toal loss 2.9722379998462958 =========\n",
      "Epoch:  343 | step  50 | train loss: 2.9842\n",
      "Epoch:  343 | step  100 | train loss: 3.0179\n",
      "======== Epoch 343 Toal loss 2.9688986588299757 =========\n",
      "Epoch:  344 | step  50 | train loss: 2.9950\n",
      "Epoch:  344 | step  100 | train loss: 2.8700\n",
      "======== Epoch 344 Toal loss 2.964227934193805 =========\n",
      "Epoch:  345 | step  50 | train loss: 3.0564\n",
      "Epoch:  345 | step  100 | train loss: 2.7302\n",
      "======== Epoch 345 Toal loss 2.9581771711023843 =========\n",
      "Epoch:  346 | step  50 | train loss: 3.0025\n",
      "Epoch:  346 | step  100 | train loss: 3.0405\n",
      "======== Epoch 346 Toal loss 2.9553133607879887 =========\n",
      "Epoch:  347 | step  50 | train loss: 3.0125\n",
      "Epoch:  347 | step  100 | train loss: 2.9233\n",
      "======== Epoch 347 Toal loss 2.94821493412421 =========\n",
      "Epoch:  348 | step  50 | train loss: 2.8029\n",
      "Epoch:  348 | step  100 | train loss: 3.1204\n",
      "======== Epoch 348 Toal loss 2.9407650280774122 =========\n",
      "Epoch:  349 | step  50 | train loss: 2.8263\n",
      "Epoch:  349 | step  100 | train loss: 2.9413\n",
      "======== Epoch 349 Toal loss 2.936412258846004 =========\n",
      "Epoch:  350 | step  50 | train loss: 2.8138\n",
      "Epoch:  350 | step  100 | train loss: 3.1022\n",
      "======== Epoch 350 Toal loss 2.93673039258011 =========\n",
      "Epoch:  351 | step  50 | train loss: 2.9354\n",
      "Epoch:  351 | step  100 | train loss: 2.9098\n",
      "======== Epoch 351 Toal loss 2.9309203430889097 =========\n",
      "Epoch:  352 | step  50 | train loss: 2.9624\n",
      "Epoch:  352 | step  100 | train loss: 3.0773\n",
      "======== Epoch 352 Toal loss 2.928270962180161 =========\n",
      "Epoch:  353 | step  50 | train loss: 3.0810\n",
      "Epoch:  353 | step  100 | train loss: 3.0281\n",
      "======== Epoch 353 Toal loss 2.923669574706535 =========\n",
      "Epoch:  354 | step  50 | train loss: 2.8874\n",
      "Epoch:  354 | step  100 | train loss: 2.9187\n",
      "======== Epoch 354 Toal loss 2.9179361700042477 =========\n",
      "Epoch:  355 | step  50 | train loss: 3.0325\n",
      "Epoch:  355 | step  100 | train loss: 2.9895\n",
      "======== Epoch 355 Toal loss 2.9136878242337607 =========\n",
      "Epoch:  356 | step  50 | train loss: 2.8554\n",
      "Epoch:  356 | step  100 | train loss: 2.8862\n",
      "======== Epoch 356 Toal loss 2.909885164198837 =========\n",
      "Epoch:  357 | step  50 | train loss: 2.9475\n",
      "Epoch:  357 | step  100 | train loss: 2.9144\n",
      "======== Epoch 357 Toal loss 2.9094900677843794 =========\n",
      "Epoch:  358 | step  50 | train loss: 2.8980\n",
      "Epoch:  358 | step  100 | train loss: 2.9263\n",
      "======== Epoch 358 Toal loss 2.9003864071233485 =========\n",
      "Epoch:  359 | step  50 | train loss: 2.9162\n",
      "Epoch:  359 | step  100 | train loss: 2.9843\n",
      "======== Epoch 359 Toal loss 2.8976034749814166 =========\n",
      "Epoch:  360 | step  50 | train loss: 3.0657\n",
      "Epoch:  360 | step  100 | train loss: 2.8956\n",
      "======== Epoch 360 Toal loss 2.8886098260801982 =========\n",
      "Epoch:  361 | step  50 | train loss: 2.7873\n",
      "Epoch:  361 | step  100 | train loss: 2.9361\n",
      "======== Epoch 361 Toal loss 2.8879483599003737 =========\n",
      "Epoch:  362 | step  50 | train loss: 2.7150\n",
      "Epoch:  362 | step  100 | train loss: 2.9081\n",
      "======== Epoch 362 Toal loss 2.887615347296242 =========\n",
      "Epoch:  363 | step  50 | train loss: 2.7987\n",
      "Epoch:  363 | step  100 | train loss: 2.8295\n",
      "======== Epoch 363 Toal loss 2.878898773736101 =========\n",
      "Epoch:  364 | step  50 | train loss: 2.8824\n",
      "Epoch:  364 | step  100 | train loss: 2.9654\n",
      "======== Epoch 364 Toal loss 2.879141712576393 =========\n",
      "Epoch:  365 | step  50 | train loss: 2.7721\n",
      "Epoch:  365 | step  100 | train loss: 3.0194\n",
      "======== Epoch 365 Toal loss 2.8730060957311614 =========\n",
      "Epoch:  366 | step  50 | train loss: 3.0647\n",
      "Epoch:  366 | step  100 | train loss: 2.9121\n",
      "======== Epoch 366 Toal loss 2.869255255877487 =========\n",
      "Epoch:  367 | step  50 | train loss: 2.8163\n",
      "Epoch:  367 | step  100 | train loss: 2.8491\n",
      "======== Epoch 367 Toal loss 2.862156301979127 =========\n",
      "Epoch:  368 | step  50 | train loss: 2.7602\n",
      "Epoch:  368 | step  100 | train loss: 2.8670\n",
      "======== Epoch 368 Toal loss 2.859558781957239 =========\n",
      "Epoch:  369 | step  50 | train loss: 2.8392\n",
      "Epoch:  369 | step  100 | train loss: 2.8318\n",
      "======== Epoch 369 Toal loss 2.8558851335106827 =========\n",
      "Epoch:  370 | step  50 | train loss: 2.7822\n",
      "Epoch:  370 | step  100 | train loss: 2.7824\n",
      "======== Epoch 370 Toal loss 2.850937781295156 =========\n",
      "Epoch:  371 | step  50 | train loss: 2.8470\n",
      "Epoch:  371 | step  100 | train loss: 2.8200\n",
      "======== Epoch 371 Toal loss 2.847864170384601 =========\n",
      "Epoch:  372 | step  50 | train loss: 2.9160\n",
      "Epoch:  372 | step  100 | train loss: 3.0194\n",
      "======== Epoch 372 Toal loss 2.844295243906781 =========\n",
      "Epoch:  373 | step  50 | train loss: 2.8603\n",
      "Epoch:  373 | step  100 | train loss: 2.8868\n",
      "======== Epoch 373 Toal loss 2.8375560597675604 =========\n",
      "Epoch:  374 | step  50 | train loss: 2.8281\n",
      "Epoch:  374 | step  100 | train loss: 3.0084\n",
      "======== Epoch 374 Toal loss 2.8355735046107595 =========\n",
      "Epoch:  375 | step  50 | train loss: 2.8454\n",
      "Epoch:  375 | step  100 | train loss: 2.8781\n",
      "======== Epoch 375 Toal loss 2.8288622057534814 =========\n",
      "Epoch:  376 | step  50 | train loss: 2.8646\n",
      "Epoch:  376 | step  100 | train loss: 2.9397\n",
      "======== Epoch 376 Toal loss 2.824549944420171 =========\n",
      "Epoch:  377 | step  50 | train loss: 2.8696\n",
      "Epoch:  377 | step  100 | train loss: 2.8074\n",
      "======== Epoch 377 Toal loss 2.8184657465151655 =========\n",
      "Epoch:  378 | step  50 | train loss: 2.8264\n",
      "Epoch:  378 | step  100 | train loss: 2.8860\n",
      "======== Epoch 378 Toal loss 2.8154383112744585 =========\n",
      "Epoch:  379 | step  50 | train loss: 2.6551\n",
      "Epoch:  379 | step  100 | train loss: 2.8574\n",
      "======== Epoch 379 Toal loss 2.8132200861364844 =========\n",
      "Epoch:  380 | step  50 | train loss: 2.5907\n",
      "Epoch:  380 | step  100 | train loss: 2.8168\n",
      "======== Epoch 380 Toal loss 2.807150933800674 =========\n",
      "Epoch:  381 | step  50 | train loss: 2.6333\n",
      "Epoch:  381 | step  100 | train loss: 2.9693\n",
      "======== Epoch 381 Toal loss 2.8071540041667657 =========\n",
      "Epoch:  382 | step  50 | train loss: 2.8201\n",
      "Epoch:  382 | step  100 | train loss: 2.8007\n",
      "======== Epoch 382 Toal loss 2.7980848521721073 =========\n",
      "Epoch:  383 | step  50 | train loss: 2.7919\n",
      "Epoch:  383 | step  100 | train loss: 2.6135\n",
      "======== Epoch 383 Toal loss 2.796390279521787 =========\n",
      "Epoch:  384 | step  50 | train loss: 2.8802\n",
      "Epoch:  384 | step  100 | train loss: 2.7516\n",
      "======== Epoch 384 Toal loss 2.7881755848241045 =========\n",
      "Epoch:  385 | step  50 | train loss: 2.8057\n",
      "Epoch:  385 | step  100 | train loss: 2.7402\n",
      "======== Epoch 385 Toal loss 2.789028755048426 =========\n",
      "Epoch:  386 | step  50 | train loss: 2.6364\n",
      "Epoch:  386 | step  100 | train loss: 2.7869\n",
      "======== Epoch 386 Toal loss 2.783562869560428 =========\n",
      "Epoch:  387 | step  50 | train loss: 2.7174\n",
      "Epoch:  387 | step  100 | train loss: 2.8340\n",
      "======== Epoch 387 Toal loss 2.783698388231479 =========\n",
      "Epoch:  388 | step  50 | train loss: 2.9165\n",
      "Epoch:  388 | step  100 | train loss: 2.7492\n",
      "======== Epoch 388 Toal loss 2.7789280589033916 =========\n",
      "Epoch:  389 | step  50 | train loss: 2.7629\n",
      "Epoch:  389 | step  100 | train loss: 2.8807\n",
      "======== Epoch 389 Toal loss 2.767379902242645 =========\n",
      "Epoch:  390 | step  50 | train loss: 2.8490\n",
      "Epoch:  390 | step  100 | train loss: 2.6438\n",
      "======== Epoch 390 Toal loss 2.7657718425843774 =========\n",
      "Epoch:  391 | step  50 | train loss: 2.7706\n",
      "Epoch:  391 | step  100 | train loss: 2.8982\n",
      "======== Epoch 391 Toal loss 2.766776829231076 =========\n",
      "Epoch:  392 | step  50 | train loss: 2.7220\n",
      "Epoch:  392 | step  100 | train loss: 2.7202\n",
      "======== Epoch 392 Toal loss 2.759696157967172 =========\n",
      "Epoch:  393 | step  50 | train loss: 2.7393\n",
      "Epoch:  393 | step  100 | train loss: 2.7235\n",
      "======== Epoch 393 Toal loss 2.7551797560560023 =========\n",
      "Epoch:  394 | step  50 | train loss: 2.7800\n",
      "Epoch:  394 | step  100 | train loss: 2.7450\n",
      "======== Epoch 394 Toal loss 2.7510139263742337 =========\n",
      "Epoch:  395 | step  50 | train loss: 2.9201\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  395 | step  100 | train loss: 2.7963\n",
      "======== Epoch 395 Toal loss 2.750917392048409 =========\n",
      "Epoch:  396 | step  50 | train loss: 2.7090\n",
      "Epoch:  396 | step  100 | train loss: 2.7565\n",
      "======== Epoch 396 Toal loss 2.7392953062445167 =========\n",
      "Epoch:  397 | step  50 | train loss: 2.7207\n",
      "Epoch:  397 | step  100 | train loss: 2.8944\n",
      "======== Epoch 397 Toal loss 2.7426111388012644 =========\n",
      "Epoch:  398 | step  50 | train loss: 2.7915\n",
      "Epoch:  398 | step  100 | train loss: 2.6213\n",
      "======== Epoch 398 Toal loss 2.7342126253174572 =========\n",
      "Epoch:  399 | step  50 | train loss: 2.6088\n",
      "Epoch:  399 | step  100 | train loss: 2.8482\n",
      "======== Epoch 399 Toal loss 2.730392717733616 =========\n",
      "Epoch:  400 | step  50 | train loss: 2.6828\n",
      "Epoch:  400 | step  100 | train loss: 2.6199\n",
      "======== Epoch 400 Toal loss 2.7286360399509832 =========\n",
      "Epoch:  401 | step  50 | train loss: 2.7299\n",
      "Epoch:  401 | step  100 | train loss: 2.7560\n",
      "======== Epoch 401 Toal loss 2.726004418318834 =========\n",
      "Epoch:  402 | step  50 | train loss: 2.5568\n",
      "Epoch:  402 | step  100 | train loss: 2.7072\n",
      "======== Epoch 402 Toal loss 2.713418466288869 =========\n",
      "Epoch:  403 | step  50 | train loss: 2.8683\n",
      "Epoch:  403 | step  100 | train loss: 2.7659\n",
      "======== Epoch 403 Toal loss 2.713802355091746 =========\n",
      "Epoch:  404 | step  50 | train loss: 2.7792\n",
      "Epoch:  404 | step  100 | train loss: 2.8088\n",
      "======== Epoch 404 Toal loss 2.70984171270355 =========\n",
      "Epoch:  405 | step  50 | train loss: 2.7653\n",
      "Epoch:  405 | step  100 | train loss: 2.7524\n",
      "======== Epoch 405 Toal loss 2.704085821058692 =========\n",
      "Epoch:  406 | step  50 | train loss: 2.7423\n",
      "Epoch:  406 | step  100 | train loss: 2.7787\n",
      "======== Epoch 406 Toal loss 2.7040199128592888 =========\n",
      "Epoch:  407 | step  50 | train loss: 2.6149\n",
      "Epoch:  407 | step  100 | train loss: 2.6179\n",
      "======== Epoch 407 Toal loss 2.6966355331544953 =========\n",
      "Epoch:  408 | step  50 | train loss: 2.7157\n",
      "Epoch:  408 | step  100 | train loss: 2.5515\n",
      "======== Epoch 408 Toal loss 2.6962740847734903 =========\n",
      "Epoch:  409 | step  50 | train loss: 2.6211\n",
      "Epoch:  409 | step  100 | train loss: 2.7070\n",
      "======== Epoch 409 Toal loss 2.691554778959693 =========\n",
      "Epoch:  410 | step  50 | train loss: 2.7200\n",
      "Epoch:  410 | step  100 | train loss: 2.6183\n",
      "======== Epoch 410 Toal loss 2.686410037482657 =========\n",
      "Epoch:  411 | step  50 | train loss: 2.7242\n",
      "Epoch:  411 | step  100 | train loss: 2.6390\n",
      "======== Epoch 411 Toal loss 2.680694481221641 =========\n",
      "Epoch:  412 | step  50 | train loss: 2.6706\n",
      "Epoch:  412 | step  100 | train loss: 2.6687\n",
      "======== Epoch 412 Toal loss 2.6824665844925053 =========\n",
      "Epoch:  413 | step  50 | train loss: 2.6021\n",
      "Epoch:  413 | step  100 | train loss: 2.7276\n",
      "======== Epoch 413 Toal loss 2.6732465949484974 =========\n",
      "Epoch:  414 | step  50 | train loss: 2.6861\n",
      "Epoch:  414 | step  100 | train loss: 2.5347\n",
      "======== Epoch 414 Toal loss 2.671901094235056 =========\n",
      "Epoch:  415 | step  50 | train loss: 2.7702\n",
      "Epoch:  415 | step  100 | train loss: 2.7520\n",
      "======== Epoch 415 Toal loss 2.668058011589981 =========\n",
      "Epoch:  416 | step  50 | train loss: 2.6959\n",
      "Epoch:  416 | step  100 | train loss: 2.7293\n",
      "======== Epoch 416 Toal loss 2.6614954064532026 =========\n",
      "Epoch:  417 | step  50 | train loss: 2.6106\n",
      "Epoch:  417 | step  100 | train loss: 2.6971\n",
      "======== Epoch 417 Toal loss 2.6614868195076298 =========\n",
      "Epoch:  418 | step  50 | train loss: 2.6798\n",
      "Epoch:  418 | step  100 | train loss: 2.6406\n",
      "======== Epoch 418 Toal loss 2.6588895727948443 =========\n",
      "Epoch:  419 | step  50 | train loss: 2.6815\n",
      "Epoch:  419 | step  100 | train loss: 2.5771\n",
      "======== Epoch 419 Toal loss 2.6514318260720104 =========\n",
      "Epoch:  420 | step  50 | train loss: 2.5851\n",
      "Epoch:  420 | step  100 | train loss: 2.7318\n",
      "======== Epoch 420 Toal loss 2.645595684284117 =========\n",
      "Epoch:  421 | step  50 | train loss: 2.6437\n",
      "Epoch:  421 | step  100 | train loss: 2.7437\n",
      "======== Epoch 421 Toal loss 2.644570782901795 =========\n",
      "Epoch:  422 | step  50 | train loss: 2.8365\n",
      "Epoch:  422 | step  100 | train loss: 2.7344\n",
      "======== Epoch 422 Toal loss 2.6424743043697947 =========\n",
      "Epoch:  423 | step  50 | train loss: 2.6787\n",
      "Epoch:  423 | step  100 | train loss: 2.7759\n",
      "======== Epoch 423 Toal loss 2.632171968134438 =========\n",
      "Epoch:  424 | step  50 | train loss: 2.6981\n",
      "Epoch:  424 | step  100 | train loss: 2.6589\n",
      "======== Epoch 424 Toal loss 2.634256793231499 =========\n",
      "Epoch:  425 | step  50 | train loss: 2.7631\n",
      "Epoch:  425 | step  100 | train loss: 2.5503\n",
      "======== Epoch 425 Toal loss 2.6303068234668516 =========\n",
      "Epoch:  426 | step  50 | train loss: 2.7203\n",
      "Epoch:  426 | step  100 | train loss: 2.8951\n",
      "======== Epoch 426 Toal loss 2.623460961551201 =========\n",
      "Epoch:  427 | step  50 | train loss: 2.7802\n",
      "Epoch:  427 | step  100 | train loss: 2.5410\n",
      "======== Epoch 427 Toal loss 2.623250610460111 =========\n",
      "Epoch:  428 | step  50 | train loss: 2.8488\n",
      "Epoch:  428 | step  100 | train loss: 2.5980\n",
      "======== Epoch 428 Toal loss 2.6148077142917043 =========\n",
      "Epoch:  429 | step  50 | train loss: 2.5697\n",
      "Epoch:  429 | step  100 | train loss: 2.5959\n",
      "======== Epoch 429 Toal loss 2.6101622465180188 =========\n",
      "Epoch:  430 | step  50 | train loss: 2.5886\n",
      "Epoch:  430 | step  100 | train loss: 2.6952\n",
      "======== Epoch 430 Toal loss 2.611364928687491 =========\n",
      "Epoch:  431 | step  50 | train loss: 2.7492\n",
      "Epoch:  431 | step  100 | train loss: 2.7589\n",
      "======== Epoch 431 Toal loss 2.6030545816188906 =========\n",
      "Epoch:  432 | step  50 | train loss: 2.6744\n",
      "Epoch:  432 | step  100 | train loss: 2.7371\n",
      "======== Epoch 432 Toal loss 2.6019455960126425 =========\n",
      "Epoch:  433 | step  50 | train loss: 2.5693\n",
      "Epoch:  433 | step  100 | train loss: 2.5968\n",
      "======== Epoch 433 Toal loss 2.6007668002834166 =========\n",
      "Epoch:  434 | step  50 | train loss: 2.6069\n",
      "Epoch:  434 | step  100 | train loss: 2.6637\n",
      "======== Epoch 434 Toal loss 2.598113565910153 =========\n",
      "Epoch:  435 | step  50 | train loss: 2.6383\n",
      "Epoch:  435 | step  100 | train loss: 2.5365\n",
      "======== Epoch 435 Toal loss 2.58868840264111 =========\n",
      "Epoch:  436 | step  50 | train loss: 2.6615\n",
      "Epoch:  436 | step  100 | train loss: 2.4332\n",
      "======== Epoch 436 Toal loss 2.5946830055577967 =========\n",
      "Epoch:  437 | step  50 | train loss: 2.4841\n",
      "Epoch:  437 | step  100 | train loss: 2.5496\n",
      "======== Epoch 437 Toal loss 2.5874287830135687 =========\n",
      "Epoch:  438 | step  50 | train loss: 2.5017\n",
      "Epoch:  438 | step  100 | train loss: 2.6222\n",
      "======== Epoch 438 Toal loss 2.5825974669882923 =========\n",
      "Epoch:  439 | step  50 | train loss: 2.5144\n",
      "Epoch:  439 | step  100 | train loss: 2.6455\n",
      "======== Epoch 439 Toal loss 2.5781206115474546 =========\n",
      "Epoch:  440 | step  50 | train loss: 2.7379\n",
      "Epoch:  440 | step  100 | train loss: 2.5135\n",
      "======== Epoch 440 Toal loss 2.575495159722925 =========\n",
      "Epoch:  441 | step  50 | train loss: 2.5496\n",
      "Epoch:  441 | step  100 | train loss: 2.5977\n",
      "======== Epoch 441 Toal loss 2.5724924638019346 =========\n",
      "Epoch:  442 | step  50 | train loss: 2.6712\n",
      "Epoch:  442 | step  100 | train loss: 2.5561\n",
      "======== Epoch 442 Toal loss 2.5645532104057995 =========\n",
      "Epoch:  443 | step  50 | train loss: 2.4798\n",
      "Epoch:  443 | step  100 | train loss: 2.4693\n",
      "======== Epoch 443 Toal loss 2.562799917003973 =========\n",
      "Epoch:  444 | step  50 | train loss: 2.3808\n",
      "Epoch:  444 | step  100 | train loss: 2.5902\n",
      "======== Epoch 444 Toal loss 2.5581699231775796 =========\n",
      "Epoch:  445 | step  50 | train loss: 2.5756\n",
      "Epoch:  445 | step  100 | train loss: 2.6389\n",
      "======== Epoch 445 Toal loss 2.55848707997702 =========\n",
      "Epoch:  446 | step  50 | train loss: 2.5999\n",
      "Epoch:  446 | step  100 | train loss: 2.5976\n",
      "======== Epoch 446 Toal loss 2.55316978353795 =========\n",
      "Epoch:  447 | step  50 | train loss: 2.6054\n",
      "Epoch:  447 | step  100 | train loss: 2.4405\n",
      "======== Epoch 447 Toal loss 2.5481767247362836 =========\n",
      "Epoch:  448 | step  50 | train loss: 2.4344\n",
      "Epoch:  448 | step  100 | train loss: 2.6405\n",
      "======== Epoch 448 Toal loss 2.544815325155491 =========\n",
      "Epoch:  449 | step  50 | train loss: 2.4288\n",
      "Epoch:  449 | step  100 | train loss: 2.5077\n",
      "======== Epoch 449 Toal loss 2.5363411166803624 =========\n",
      "Epoch:  450 | step  50 | train loss: 2.6080\n",
      "Epoch:  450 | step  100 | train loss: 2.4613\n",
      "======== Epoch 450 Toal loss 2.5367172520335126 =========\n",
      "Epoch:  451 | step  50 | train loss: 2.6235\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  451 | step  100 | train loss: 2.5197\n",
      "======== Epoch 451 Toal loss 2.5289417883244956 =========\n",
      "Epoch:  452 | step  50 | train loss: 2.5739\n",
      "Epoch:  452 | step  100 | train loss: 2.6529\n",
      "======== Epoch 452 Toal loss 2.5300885274158262 =========\n",
      "Epoch:  453 | step  50 | train loss: 2.5467\n",
      "Epoch:  453 | step  100 | train loss: 2.5788\n",
      "======== Epoch 453 Toal loss 2.5245090538893287 =========\n",
      "Epoch:  454 | step  50 | train loss: 2.6202\n",
      "Epoch:  454 | step  100 | train loss: 2.5169\n",
      "======== Epoch 454 Toal loss 2.5256551920883052 =========\n",
      "Epoch:  455 | step  50 | train loss: 2.5319\n",
      "Epoch:  455 | step  100 | train loss: 2.4405\n",
      "======== Epoch 455 Toal loss 2.521222323906131 =========\n",
      "Epoch:  456 | step  50 | train loss: 2.4204\n",
      "Epoch:  456 | step  100 | train loss: 2.5186\n",
      "======== Epoch 456 Toal loss 2.515246747955074 =========\n",
      "Epoch:  457 | step  50 | train loss: 2.4146\n",
      "Epoch:  457 | step  100 | train loss: 2.4831\n",
      "======== Epoch 457 Toal loss 2.5118063504133765 =========\n",
      "Epoch:  458 | step  50 | train loss: 2.5149\n",
      "Epoch:  458 | step  100 | train loss: 2.4718\n",
      "======== Epoch 458 Toal loss 2.5087007584610608 =========\n",
      "Epoch:  459 | step  50 | train loss: 2.4663\n",
      "Epoch:  459 | step  100 | train loss: 2.6767\n",
      "======== Epoch 459 Toal loss 2.5106208925324727 =========\n",
      "Epoch:  460 | step  50 | train loss: 2.5904\n",
      "Epoch:  460 | step  100 | train loss: 2.5133\n",
      "======== Epoch 460 Toal loss 2.5023496616177443 =========\n",
      "Epoch:  461 | step  50 | train loss: 2.5315\n",
      "Epoch:  461 | step  100 | train loss: 2.5013\n",
      "======== Epoch 461 Toal loss 2.495900446806497 =========\n",
      "Epoch:  462 | step  50 | train loss: 2.4337\n",
      "Epoch:  462 | step  100 | train loss: 2.6547\n",
      "======== Epoch 462 Toal loss 2.49034928694004 =========\n",
      "Epoch:  463 | step  50 | train loss: 2.6089\n",
      "Epoch:  463 | step  100 | train loss: 2.3194\n",
      "======== Epoch 463 Toal loss 2.493709822011188 =========\n",
      "Epoch:  464 | step  50 | train loss: 2.4229\n",
      "Epoch:  464 | step  100 | train loss: 2.3792\n",
      "======== Epoch 464 Toal loss 2.4857750598008073 =========\n",
      "Epoch:  465 | step  50 | train loss: 2.5175\n",
      "Epoch:  465 | step  100 | train loss: 2.4185\n",
      "======== Epoch 465 Toal loss 2.4841961976958484 =========\n",
      "Epoch:  466 | step  50 | train loss: 2.5038\n",
      "Epoch:  466 | step  100 | train loss: 2.5790\n",
      "======== Epoch 466 Toal loss 2.483555159917692 =========\n",
      "Epoch:  467 | step  50 | train loss: 2.3727\n",
      "Epoch:  467 | step  100 | train loss: 2.4897\n",
      "======== Epoch 467 Toal loss 2.4792050055372035 =========\n",
      "Epoch:  468 | step  50 | train loss: 2.5318\n",
      "Epoch:  468 | step  100 | train loss: 2.3552\n",
      "======== Epoch 468 Toal loss 2.4771158230013963 =========\n",
      "Epoch:  469 | step  50 | train loss: 2.6124\n",
      "Epoch:  469 | step  100 | train loss: 2.4049\n",
      "======== Epoch 469 Toal loss 2.4677117898212217 =========\n",
      "Epoch:  470 | step  50 | train loss: 2.4626\n",
      "Epoch:  470 | step  100 | train loss: 2.5685\n",
      "======== Epoch 470 Toal loss 2.4730303927165704 =========\n",
      "Epoch:  471 | step  50 | train loss: 2.5152\n",
      "Epoch:  471 | step  100 | train loss: 2.4588\n",
      "======== Epoch 471 Toal loss 2.464306682105956 =========\n",
      "Epoch:  472 | step  50 | train loss: 2.4247\n",
      "Epoch:  472 | step  100 | train loss: 2.4926\n",
      "======== Epoch 472 Toal loss 2.45761448387208 =========\n",
      "Epoch:  473 | step  50 | train loss: 2.4969\n",
      "Epoch:  473 | step  100 | train loss: 2.3839\n",
      "======== Epoch 473 Toal loss 2.45620088654805 =========\n",
      "Epoch:  474 | step  50 | train loss: 2.6023\n",
      "Epoch:  474 | step  100 | train loss: 2.5474\n",
      "======== Epoch 474 Toal loss 2.4541845593026013 =========\n",
      "Epoch:  475 | step  50 | train loss: 2.4038\n",
      "Epoch:  475 | step  100 | train loss: 2.2182\n",
      "======== Epoch 475 Toal loss 2.4482511737482335 =========\n",
      "Epoch:  476 | step  50 | train loss: 2.4021\n",
      "Epoch:  476 | step  100 | train loss: 2.5225\n",
      "======== Epoch 476 Toal loss 2.445102711034015 =========\n",
      "Epoch:  477 | step  50 | train loss: 2.5120\n",
      "Epoch:  477 | step  100 | train loss: 2.4407\n",
      "======== Epoch 477 Toal loss 2.439851245259851 =========\n",
      "Epoch:  478 | step  50 | train loss: 2.4604\n",
      "Epoch:  478 | step  100 | train loss: 2.4697\n",
      "======== Epoch 478 Toal loss 2.4380086127335465 =========\n",
      "Epoch:  479 | step  50 | train loss: 2.4902\n",
      "Epoch:  479 | step  100 | train loss: 2.6565\n",
      "======== Epoch 479 Toal loss 2.4337922092375717 =========\n",
      "Epoch:  480 | step  50 | train loss: 2.2658\n",
      "Epoch:  480 | step  100 | train loss: 2.3294\n",
      "======== Epoch 480 Toal loss 2.432339102272096 =========\n",
      "Epoch:  481 | step  50 | train loss: 2.4284\n",
      "Epoch:  481 | step  100 | train loss: 2.5930\n",
      "======== Epoch 481 Toal loss 2.42958231282428 =========\n",
      "Epoch:  482 | step  50 | train loss: 2.4856\n",
      "Epoch:  482 | step  100 | train loss: 2.5799\n",
      "======== Epoch 482 Toal loss 2.4280493356348054 =========\n",
      "Epoch:  483 | step  50 | train loss: 2.3751\n",
      "Epoch:  483 | step  100 | train loss: 2.5886\n",
      "======== Epoch 483 Toal loss 2.4200788338979087 =========\n",
      "Epoch:  484 | step  50 | train loss: 2.2469\n",
      "Epoch:  484 | step  100 | train loss: 2.4792\n",
      "======== Epoch 484 Toal loss 2.418185282528885 =========\n",
      "Epoch:  485 | step  50 | train loss: 2.4300\n",
      "Epoch:  485 | step  100 | train loss: 2.5109\n",
      "======== Epoch 485 Toal loss 2.414384378650324 =========\n",
      "Epoch:  486 | step  50 | train loss: 2.5991\n",
      "Epoch:  486 | step  100 | train loss: 2.3902\n",
      "======== Epoch 486 Toal loss 2.4059979334110166 =========\n",
      "Epoch:  487 | step  50 | train loss: 2.3626\n",
      "Epoch:  487 | step  100 | train loss: 2.4442\n",
      "======== Epoch 487 Toal loss 2.4065875464338595 =========\n",
      "Epoch:  488 | step  50 | train loss: 2.5168\n",
      "Epoch:  488 | step  100 | train loss: 2.3677\n",
      "======== Epoch 488 Toal loss 2.406301385988065 =========\n",
      "Epoch:  489 | step  50 | train loss: 2.4427\n",
      "Epoch:  489 | step  100 | train loss: 2.4376\n",
      "======== Epoch 489 Toal loss 2.3977595654929558 =========\n",
      "Epoch:  490 | step  50 | train loss: 2.3042\n",
      "Epoch:  490 | step  100 | train loss: 2.2860\n",
      "======== Epoch 490 Toal loss 2.3995202828228956 =========\n",
      "Epoch:  491 | step  50 | train loss: 2.1907\n",
      "Epoch:  491 | step  100 | train loss: 2.4826\n",
      "======== Epoch 491 Toal loss 2.3975475726088855 =========\n",
      "Epoch:  492 | step  50 | train loss: 2.5576\n",
      "Epoch:  492 | step  100 | train loss: 2.5430\n",
      "======== Epoch 492 Toal loss 2.3869386203889924 =========\n",
      "Epoch:  493 | step  50 | train loss: 2.3960\n",
      "Epoch:  493 | step  100 | train loss: 2.4423\n",
      "======== Epoch 493 Toal loss 2.3891710905524772 =========\n",
      "Epoch:  494 | step  50 | train loss: 2.3464\n",
      "Epoch:  494 | step  100 | train loss: 2.3251\n",
      "======== Epoch 494 Toal loss 2.3831153400545197 =========\n",
      "Epoch:  495 | step  50 | train loss: 2.2387\n",
      "Epoch:  495 | step  100 | train loss: 2.4741\n",
      "======== Epoch 495 Toal loss 2.3789573064664515 =========\n",
      "Epoch:  496 | step  50 | train loss: 2.1933\n",
      "Epoch:  496 | step  100 | train loss: 2.3709\n",
      "======== Epoch 496 Toal loss 2.3797004242253497 =========\n",
      "Epoch:  497 | step  50 | train loss: 2.3869\n",
      "Epoch:  497 | step  100 | train loss: 2.4152\n",
      "======== Epoch 497 Toal loss 2.3740767133914358 =========\n",
      "Epoch:  498 | step  50 | train loss: 2.3780\n",
      "Epoch:  498 | step  100 | train loss: 2.4871\n",
      "======== Epoch 498 Toal loss 2.371496495192613 =========\n",
      "Epoch:  499 | step  50 | train loss: 2.4184\n",
      "Epoch:  499 | step  100 | train loss: 2.3639\n",
      "======== Epoch 499 Toal loss 2.365041833582932 =========\n",
      "Epoch:  500 | step  50 | train loss: 2.2130\n",
      "Epoch:  500 | step  100 | train loss: 2.3559\n",
      "======== Epoch 500 Toal loss 2.3656548852843 =========\n",
      "Epoch:  501 | step  50 | train loss: 2.4952\n",
      "Epoch:  501 | step  100 | train loss: 2.3551\n",
      "======== Epoch 501 Toal loss 2.3563450429497697 =========\n",
      "Epoch:  502 | step  50 | train loss: 2.3443\n",
      "Epoch:  502 | step  100 | train loss: 2.4915\n",
      "======== Epoch 502 Toal loss 2.357606781207449 =========\n",
      "Epoch:  503 | step  50 | train loss: 2.3353\n",
      "Epoch:  503 | step  100 | train loss: 2.3350\n",
      "======== Epoch 503 Toal loss 2.3556120996552754 =========\n",
      "Epoch:  504 | step  50 | train loss: 2.4558\n",
      "Epoch:  504 | step  100 | train loss: 2.5091\n",
      "======== Epoch 504 Toal loss 2.3517479063049564 =========\n",
      "Epoch:  505 | step  50 | train loss: 2.2839\n",
      "Epoch:  505 | step  100 | train loss: 2.2609\n",
      "======== Epoch 505 Toal loss 2.3492259068217702 =========\n",
      "Epoch:  506 | step  50 | train loss: 2.4194\n",
      "Epoch:  506 | step  100 | train loss: 2.4177\n",
      "======== Epoch 506 Toal loss 2.3453950339216525 =========\n",
      "Epoch:  507 | step  50 | train loss: 2.4011\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  507 | step  100 | train loss: 2.4156\n",
      "======== Epoch 507 Toal loss 2.337840566790201 =========\n",
      "Epoch:  508 | step  50 | train loss: 2.3811\n",
      "Epoch:  508 | step  100 | train loss: 2.3574\n",
      "======== Epoch 508 Toal loss 2.337728981080094 =========\n",
      "Epoch:  509 | step  50 | train loss: 2.3017\n",
      "Epoch:  509 | step  100 | train loss: 2.3491\n",
      "======== Epoch 509 Toal loss 2.335998426607954 =========\n",
      "Epoch:  510 | step  50 | train loss: 2.5378\n",
      "Epoch:  510 | step  100 | train loss: 2.2896\n",
      "======== Epoch 510 Toal loss 2.3351797340362053 =========\n",
      "Epoch:  511 | step  50 | train loss: 2.3694\n",
      "Epoch:  511 | step  100 | train loss: 2.4839\n",
      "======== Epoch 511 Toal loss 2.324184487505657 =========\n",
      "Epoch:  512 | step  50 | train loss: 2.3815\n",
      "Epoch:  512 | step  100 | train loss: 2.2227\n",
      "======== Epoch 512 Toal loss 2.3253959252582335 =========\n",
      "Epoch:  513 | step  50 | train loss: 2.3094\n",
      "Epoch:  513 | step  100 | train loss: 2.5009\n",
      "======== Epoch 513 Toal loss 2.316755765821876 =========\n",
      "Epoch:  514 | step  50 | train loss: 2.4532\n",
      "Epoch:  514 | step  100 | train loss: 2.3300\n",
      "======== Epoch 514 Toal loss 2.320907226422938 =========\n",
      "Epoch:  515 | step  50 | train loss: 2.1939\n",
      "Epoch:  515 | step  100 | train loss: 2.3420\n",
      "======== Epoch 515 Toal loss 2.311170802852972 =========\n",
      "Epoch:  516 | step  50 | train loss: 2.3232\n",
      "Epoch:  516 | step  100 | train loss: 2.4462\n",
      "======== Epoch 516 Toal loss 2.3136990070343018 =========\n",
      "Epoch:  517 | step  50 | train loss: 2.0819\n",
      "Epoch:  517 | step  100 | train loss: 2.3864\n",
      "======== Epoch 517 Toal loss 2.311141709971234 =========\n",
      "Epoch:  518 | step  50 | train loss: 2.2586\n",
      "Epoch:  518 | step  100 | train loss: 2.2565\n",
      "======== Epoch 518 Toal loss 2.3068207608975047 =========\n",
      "Epoch:  519 | step  50 | train loss: 2.2146\n",
      "Epoch:  519 | step  100 | train loss: 2.3392\n",
      "======== Epoch 519 Toal loss 2.3020973825842383 =========\n",
      "Epoch:  520 | step  50 | train loss: 2.4895\n",
      "Epoch:  520 | step  100 | train loss: 2.4453\n",
      "======== Epoch 520 Toal loss 2.2972560839924387 =========\n",
      "Epoch:  521 | step  50 | train loss: 2.3389\n",
      "Epoch:  521 | step  100 | train loss: 2.2662\n",
      "======== Epoch 521 Toal loss 2.299310358559213 =========\n",
      "Epoch:  522 | step  50 | train loss: 2.3002\n",
      "Epoch:  522 | step  100 | train loss: 2.2944\n",
      "======== Epoch 522 Toal loss 2.2930387229454228 =========\n",
      "Epoch:  523 | step  50 | train loss: 2.1422\n",
      "Epoch:  523 | step  100 | train loss: 2.3283\n",
      "======== Epoch 523 Toal loss 2.287136371542768 =========\n",
      "Epoch:  524 | step  50 | train loss: 2.2718\n",
      "Epoch:  524 | step  100 | train loss: 2.1541\n",
      "======== Epoch 524 Toal loss 2.28855696926272 =========\n",
      "Epoch:  525 | step  50 | train loss: 2.2389\n",
      "Epoch:  525 | step  100 | train loss: 2.2565\n",
      "======== Epoch 525 Toal loss 2.2877308857150194 =========\n",
      "Epoch:  526 | step  50 | train loss: 2.1829\n",
      "Epoch:  526 | step  100 | train loss: 2.3326\n",
      "======== Epoch 526 Toal loss 2.279951622815636 =========\n",
      "Epoch:  527 | step  50 | train loss: 2.2189\n",
      "Epoch:  527 | step  100 | train loss: 2.3767\n",
      "======== Epoch 527 Toal loss 2.278045900468904 =========\n",
      "Epoch:  528 | step  50 | train loss: 2.2721\n",
      "Epoch:  528 | step  100 | train loss: 2.2921\n",
      "======== Epoch 528 Toal loss 2.276180825582365 =========\n",
      "Epoch:  529 | step  50 | train loss: 2.2219\n",
      "Epoch:  529 | step  100 | train loss: 2.2926\n",
      "======== Epoch 529 Toal loss 2.2742090438439595 =========\n",
      "Epoch:  530 | step  50 | train loss: 2.1878\n",
      "Epoch:  530 | step  100 | train loss: 2.2577\n",
      "======== Epoch 530 Toal loss 2.2636277326723424 =========\n",
      "Epoch:  531 | step  50 | train loss: 2.0945\n",
      "Epoch:  531 | step  100 | train loss: 2.4967\n",
      "======== Epoch 531 Toal loss 2.254986900624221 =========\n",
      "Epoch:  532 | step  50 | train loss: 2.2518\n",
      "Epoch:  532 | step  100 | train loss: 2.2515\n",
      "======== Epoch 532 Toal loss 2.2609198093414307 =========\n",
      "Epoch:  533 | step  50 | train loss: 2.2515\n",
      "Epoch:  533 | step  100 | train loss: 2.4385\n",
      "======== Epoch 533 Toal loss 2.260273160004034 =========\n",
      "Epoch:  534 | step  50 | train loss: 2.1937\n",
      "Epoch:  534 | step  100 | train loss: 2.2476\n",
      "======== Epoch 534 Toal loss 2.2567772167484934 =========\n",
      "Epoch:  535 | step  50 | train loss: 2.1762\n",
      "Epoch:  535 | step  100 | train loss: 2.2852\n",
      "======== Epoch 535 Toal loss 2.249449986752456 =========\n",
      "Epoch:  536 | step  50 | train loss: 2.1862\n",
      "Epoch:  536 | step  100 | train loss: 2.1938\n",
      "======== Epoch 536 Toal loss 2.248286642679354 =========\n",
      "Epoch:  537 | step  50 | train loss: 2.3260\n",
      "Epoch:  537 | step  100 | train loss: 2.3057\n",
      "======== Epoch 537 Toal loss 2.2452628767587304 =========\n",
      "Epoch:  538 | step  50 | train loss: 2.1482\n",
      "Epoch:  538 | step  100 | train loss: 2.2168\n",
      "======== Epoch 538 Toal loss 2.244081744333593 =========\n",
      "Epoch:  539 | step  50 | train loss: 2.1860\n",
      "Epoch:  539 | step  100 | train loss: 2.2453\n",
      "======== Epoch 539 Toal loss 2.240214301318657 =========\n",
      "Epoch:  540 | step  50 | train loss: 2.2574\n",
      "Epoch:  540 | step  100 | train loss: 2.1908\n",
      "======== Epoch 540 Toal loss 2.2344359285463162 =========\n",
      "Epoch:  541 | step  50 | train loss: 2.1018\n",
      "Epoch:  541 | step  100 | train loss: 2.1756\n",
      "======== Epoch 541 Toal loss 2.231508143549043 =========\n",
      "Epoch:  542 | step  50 | train loss: 2.0061\n",
      "Epoch:  542 | step  100 | train loss: 2.3431\n",
      "======== Epoch 542 Toal loss 2.233878534983813 =========\n",
      "Epoch:  543 | step  50 | train loss: 2.2620\n",
      "Epoch:  543 | step  100 | train loss: 2.1469\n",
      "======== Epoch 543 Toal loss 2.2294766689703716 =========\n",
      "Epoch:  544 | step  50 | train loss: 2.2586\n",
      "Epoch:  544 | step  100 | train loss: 2.1149\n",
      "======== Epoch 544 Toal loss 2.222306664397077 =========\n",
      "Epoch:  545 | step  50 | train loss: 2.2216\n",
      "Epoch:  545 | step  100 | train loss: 2.1601\n",
      "======== Epoch 545 Toal loss 2.2209726145597006 =========\n",
      "Epoch:  546 | step  50 | train loss: 2.2627\n",
      "Epoch:  546 | step  100 | train loss: 2.1663\n",
      "======== Epoch 546 Toal loss 2.223211627665574 =========\n",
      "Epoch:  547 | step  50 | train loss: 2.1749\n",
      "Epoch:  547 | step  100 | train loss: 2.1697\n",
      "======== Epoch 547 Toal loss 2.2133994354465143 =========\n",
      "Epoch:  548 | step  50 | train loss: 2.1062\n",
      "Epoch:  548 | step  100 | train loss: 2.0582\n",
      "======== Epoch 548 Toal loss 2.212406434663912 =========\n",
      "Epoch:  549 | step  50 | train loss: 2.1396\n",
      "Epoch:  549 | step  100 | train loss: 2.1753\n",
      "======== Epoch 549 Toal loss 2.2137032601891495 =========\n",
      "Epoch:  550 | step  50 | train loss: 2.3439\n",
      "Epoch:  550 | step  100 | train loss: 2.3500\n",
      "======== Epoch 550 Toal loss 2.2077021346828802 =========\n",
      "Epoch:  551 | step  50 | train loss: 2.2178\n",
      "Epoch:  551 | step  100 | train loss: 2.3396\n",
      "======== Epoch 551 Toal loss 2.207289689924659 =========\n",
      "Epoch:  552 | step  50 | train loss: 2.2055\n",
      "Epoch:  552 | step  100 | train loss: 2.1521\n",
      "======== Epoch 552 Toal loss 2.1959112213879095 =========\n",
      "Epoch:  553 | step  50 | train loss: 2.2614\n",
      "Epoch:  553 | step  100 | train loss: 2.2437\n",
      "======== Epoch 553 Toal loss 2.1983593140191178 =========\n",
      "Epoch:  554 | step  50 | train loss: 2.3785\n",
      "Epoch:  554 | step  100 | train loss: 2.1558\n",
      "======== Epoch 554 Toal loss 2.200331528981527 =========\n",
      "Epoch:  555 | step  50 | train loss: 2.1437\n",
      "Epoch:  555 | step  100 | train loss: 2.3487\n",
      "======== Epoch 555 Toal loss 2.194156355974151 =========\n",
      "Epoch:  556 | step  50 | train loss: 2.1807\n",
      "Epoch:  556 | step  100 | train loss: 2.1290\n",
      "======== Epoch 556 Toal loss 2.183452457916446 =========\n",
      "Epoch:  557 | step  50 | train loss: 2.1815\n",
      "Epoch:  557 | step  100 | train loss: 2.1179\n",
      "======== Epoch 557 Toal loss 2.1884808511268803 =========\n",
      "Epoch:  558 | step  50 | train loss: 2.2887\n",
      "Epoch:  558 | step  100 | train loss: 2.2131\n",
      "======== Epoch 558 Toal loss 2.1883079036464537 =========\n",
      "Epoch:  559 | step  50 | train loss: 2.1738\n",
      "Epoch:  559 | step  100 | train loss: 2.1681\n",
      "======== Epoch 559 Toal loss 2.1769422583463713 =========\n",
      "Epoch:  560 | step  50 | train loss: 2.1064\n",
      "Epoch:  560 | step  100 | train loss: 2.2807\n",
      "======== Epoch 560 Toal loss 2.180664273781505 =========\n",
      "Epoch:  561 | step  50 | train loss: 2.1846\n",
      "Epoch:  561 | step  100 | train loss: 2.1776\n",
      "======== Epoch 561 Toal loss 2.175585381383818 =========\n",
      "Epoch:  562 | step  50 | train loss: 2.1201\n",
      "Epoch:  562 | step  100 | train loss: 2.1905\n",
      "======== Epoch 562 Toal loss 2.170732228736567 =========\n",
      "Epoch:  563 | step  50 | train loss: 1.9171\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  563 | step  100 | train loss: 2.1751\n",
      "======== Epoch 563 Toal loss 2.166734771030705 =========\n",
      "Epoch:  564 | step  50 | train loss: 2.1410\n",
      "Epoch:  564 | step  100 | train loss: 2.0253\n",
      "======== Epoch 564 Toal loss 2.1695601048508313 =========\n",
      "Epoch:  565 | step  50 | train loss: 2.1334\n",
      "Epoch:  565 | step  100 | train loss: 2.2091\n",
      "======== Epoch 565 Toal loss 2.161707721105436 =========\n",
      "Epoch:  566 | step  50 | train loss: 2.1237\n",
      "Epoch:  566 | step  100 | train loss: 2.1501\n",
      "======== Epoch 566 Toal loss 2.1568125496065713 =========\n",
      "Epoch:  567 | step  50 | train loss: 2.1494\n",
      "Epoch:  567 | step  100 | train loss: 2.0308\n",
      "======== Epoch 567 Toal loss 2.1597351291315343 =========\n",
      "Epoch:  568 | step  50 | train loss: 2.2396\n",
      "Epoch:  568 | step  100 | train loss: 2.1551\n",
      "======== Epoch 568 Toal loss 2.151872106683933 =========\n",
      "Epoch:  569 | step  50 | train loss: 2.0391\n",
      "Epoch:  569 | step  100 | train loss: 2.2304\n",
      "======== Epoch 569 Toal loss 2.1546009023015094 =========\n",
      "Epoch:  570 | step  50 | train loss: 2.1128\n",
      "Epoch:  570 | step  100 | train loss: 2.0228\n",
      "======== Epoch 570 Toal loss 2.150384797313349 =========\n",
      "Epoch:  571 | step  50 | train loss: 2.1355\n",
      "Epoch:  571 | step  100 | train loss: 2.0593\n",
      "======== Epoch 571 Toal loss 2.1450158619299167 =========\n",
      "Epoch:  572 | step  50 | train loss: 2.2139\n",
      "Epoch:  572 | step  100 | train loss: 2.2032\n",
      "======== Epoch 572 Toal loss 2.1443700974549706 =========\n",
      "Epoch:  573 | step  50 | train loss: 2.0964\n",
      "Epoch:  573 | step  100 | train loss: 2.3172\n",
      "======== Epoch 573 Toal loss 2.134367692761305 =========\n",
      "Epoch:  574 | step  50 | train loss: 2.0981\n",
      "Epoch:  574 | step  100 | train loss: 2.1806\n",
      "======== Epoch 574 Toal loss 2.1377060675039523 =========\n",
      "Epoch:  575 | step  50 | train loss: 2.0041\n",
      "Epoch:  575 | step  100 | train loss: 2.0505\n",
      "======== Epoch 575 Toal loss 2.130483021581076 =========\n",
      "Epoch:  576 | step  50 | train loss: 2.0158\n",
      "Epoch:  576 | step  100 | train loss: 2.3146\n",
      "======== Epoch 576 Toal loss 2.1294509114288704 =========\n",
      "Epoch:  577 | step  50 | train loss: 2.0257\n",
      "Epoch:  577 | step  100 | train loss: 2.0372\n",
      "======== Epoch 577 Toal loss 2.1230717722962544 =========\n",
      "Epoch:  578 | step  50 | train loss: 2.2162\n",
      "Epoch:  578 | step  100 | train loss: 2.1767\n",
      "======== Epoch 578 Toal loss 2.1235603181327263 =========\n",
      "Epoch:  579 | step  50 | train loss: 1.9745\n",
      "Epoch:  579 | step  100 | train loss: 2.0556\n",
      "======== Epoch 579 Toal loss 2.121500028827326 =========\n",
      "Epoch:  580 | step  50 | train loss: 2.0441\n",
      "Epoch:  580 | step  100 | train loss: 2.1720\n",
      "======== Epoch 580 Toal loss 2.1119681616139605 =========\n",
      "Epoch:  581 | step  50 | train loss: 2.2751\n",
      "Epoch:  581 | step  100 | train loss: 2.1511\n",
      "======== Epoch 581 Toal loss 2.1142589968394456 =========\n",
      "Epoch:  582 | step  50 | train loss: 1.9307\n",
      "Epoch:  582 | step  100 | train loss: 2.0460\n",
      "======== Epoch 582 Toal loss 2.104710201906964 =========\n",
      "Epoch:  583 | step  50 | train loss: 1.9113\n",
      "Epoch:  583 | step  100 | train loss: 2.0918\n",
      "======== Epoch 583 Toal loss 2.113439832276445 =========\n",
      "Epoch:  584 | step  50 | train loss: 2.0353\n",
      "Epoch:  584 | step  100 | train loss: 2.1263\n",
      "======== Epoch 584 Toal loss 2.1074211374530947 =========\n",
      "Epoch:  585 | step  50 | train loss: 2.0974\n",
      "Epoch:  585 | step  100 | train loss: 2.0959\n",
      "======== Epoch 585 Toal loss 2.1002448186641787 =========\n",
      "Epoch:  586 | step  50 | train loss: 2.0995\n",
      "Epoch:  586 | step  100 | train loss: 2.0823\n",
      "======== Epoch 586 Toal loss 2.1017848582771737 =========\n",
      "Epoch:  587 | step  50 | train loss: 1.9762\n",
      "Epoch:  587 | step  100 | train loss: 2.0242\n",
      "======== Epoch 587 Toal loss 2.0969189337598597 =========\n",
      "Epoch:  588 | step  50 | train loss: 2.0212\n",
      "Epoch:  588 | step  100 | train loss: 1.9780\n",
      "======== Epoch 588 Toal loss 2.0927404349412377 =========\n",
      "Epoch:  589 | step  50 | train loss: 2.1481\n",
      "Epoch:  589 | step  100 | train loss: 2.1980\n",
      "======== Epoch 589 Toal loss 2.089145705951908 =========\n",
      "Epoch:  590 | step  50 | train loss: 2.0381\n",
      "Epoch:  590 | step  100 | train loss: 2.0531\n",
      "======== Epoch 590 Toal loss 2.087019854444798 =========\n",
      "Epoch:  591 | step  50 | train loss: 2.0789\n",
      "Epoch:  591 | step  100 | train loss: 2.0607\n",
      "======== Epoch 591 Toal loss 2.0844950675964355 =========\n",
      "Epoch:  592 | step  50 | train loss: 1.9902\n",
      "Epoch:  592 | step  100 | train loss: 2.1741\n",
      "======== Epoch 592 Toal loss 2.081219246717003 =========\n",
      "Epoch:  593 | step  50 | train loss: 1.9641\n",
      "Epoch:  593 | step  100 | train loss: 2.0552\n",
      "======== Epoch 593 Toal loss 2.083090499164612 =========\n",
      "Epoch:  594 | step  50 | train loss: 2.0299\n",
      "Epoch:  594 | step  100 | train loss: 2.1911\n",
      "======== Epoch 594 Toal loss 2.0754801887806837 =========\n",
      "Epoch:  595 | step  50 | train loss: 2.0965\n",
      "Epoch:  595 | step  100 | train loss: 2.0326\n",
      "======== Epoch 595 Toal loss 2.0777427432982902 =========\n",
      "Epoch:  596 | step  50 | train loss: 2.2282\n",
      "Epoch:  596 | step  100 | train loss: 2.1873\n",
      "======== Epoch 596 Toal loss 2.0722479742716966 =========\n",
      "Epoch:  597 | step  50 | train loss: 2.1833\n",
      "Epoch:  597 | step  100 | train loss: 2.1933\n",
      "======== Epoch 597 Toal loss 2.0676842113820517 =========\n",
      "Epoch:  598 | step  50 | train loss: 2.2154\n",
      "Epoch:  598 | step  100 | train loss: 2.0246\n",
      "======== Epoch 598 Toal loss 2.071244908542168 =========\n",
      "Epoch:  599 | step  50 | train loss: 2.0683\n",
      "Epoch:  599 | step  100 | train loss: 1.9150\n",
      "======== Epoch 599 Toal loss 2.0644172022982343 =========\n",
      "Epoch:  600 | step  50 | train loss: 1.9444\n",
      "Epoch:  600 | step  100 | train loss: 2.0295\n",
      "======== Epoch 600 Toal loss 2.0653776792975944 =========\n",
      "Epoch:  601 | step  50 | train loss: 2.0782\n",
      "Epoch:  601 | step  100 | train loss: 2.0288\n",
      "======== Epoch 601 Toal loss 2.0640526050474586 =========\n",
      "Epoch:  602 | step  50 | train loss: 2.0450\n",
      "Epoch:  602 | step  100 | train loss: 2.0403\n",
      "======== Epoch 602 Toal loss 2.0616634318499063 =========\n",
      "Epoch:  603 | step  50 | train loss: 1.9849\n",
      "Epoch:  603 | step  100 | train loss: 1.9982\n",
      "======== Epoch 603 Toal loss 2.054702315873247 =========\n",
      "Epoch:  604 | step  50 | train loss: 2.0333\n",
      "Epoch:  604 | step  100 | train loss: 2.1728\n",
      "======== Epoch 604 Toal loss 2.0481498134814626 =========\n",
      "Epoch:  605 | step  50 | train loss: 2.0383\n",
      "Epoch:  605 | step  100 | train loss: 2.0430\n",
      "======== Epoch 605 Toal loss 2.051320460753712 =========\n",
      "Epoch:  606 | step  50 | train loss: 2.0366\n",
      "Epoch:  606 | step  100 | train loss: 2.0808\n",
      "======== Epoch 606 Toal loss 2.0480124940717124 =========\n",
      "Epoch:  607 | step  50 | train loss: 1.9873\n",
      "Epoch:  607 | step  100 | train loss: 1.9761\n",
      "======== Epoch 607 Toal loss 2.045472783770988 =========\n",
      "Epoch:  608 | step  50 | train loss: 1.9739\n",
      "Epoch:  608 | step  100 | train loss: 2.0760\n",
      "======== Epoch 608 Toal loss 2.039928522536425 =========\n",
      "Epoch:  609 | step  50 | train loss: 2.1362\n",
      "Epoch:  609 | step  100 | train loss: 2.2164\n",
      "======== Epoch 609 Toal loss 2.038930330819231 =========\n",
      "Epoch:  610 | step  50 | train loss: 1.9414\n",
      "Epoch:  610 | step  100 | train loss: 1.9314\n",
      "======== Epoch 610 Toal loss 2.039358535433203 =========\n",
      "Epoch:  611 | step  50 | train loss: 2.1038\n",
      "Epoch:  611 | step  100 | train loss: 1.9975\n",
      "======== Epoch 611 Toal loss 2.0344838330416177 =========\n",
      "Epoch:  612 | step  50 | train loss: 2.0705\n",
      "Epoch:  612 | step  100 | train loss: 1.9720\n",
      "======== Epoch 612 Toal loss 2.032078127550885 =========\n",
      "Epoch:  613 | step  50 | train loss: 2.0529\n",
      "Epoch:  613 | step  100 | train loss: 2.1484\n",
      "======== Epoch 613 Toal loss 2.0294467579058515 =========\n",
      "Epoch:  614 | step  50 | train loss: 2.0231\n",
      "Epoch:  614 | step  100 | train loss: 1.9660\n",
      "======== Epoch 614 Toal loss 2.027779647974464 =========\n",
      "Epoch:  615 | step  50 | train loss: 1.8800\n",
      "Epoch:  615 | step  100 | train loss: 2.1363\n",
      "======== Epoch 615 Toal loss 2.0218147049105264 =========\n",
      "Epoch:  616 | step  50 | train loss: 2.0348\n",
      "Epoch:  616 | step  100 | train loss: 2.1924\n",
      "======== Epoch 616 Toal loss 2.0165497025823207 =========\n",
      "Epoch:  617 | step  50 | train loss: 1.8203\n",
      "Epoch:  617 | step  100 | train loss: 2.0882\n",
      "======== Epoch 617 Toal loss 2.0174192364622905 =========\n",
      "Epoch:  618 | step  50 | train loss: 2.1056\n",
      "Epoch:  618 | step  100 | train loss: 2.0915\n",
      "======== Epoch 618 Toal loss 2.015916607244228 =========\n",
      "Epoch:  619 | step  50 | train loss: 1.9643\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  619 | step  100 | train loss: 1.9828\n",
      "======== Epoch 619 Toal loss 2.0095598707354165 =========\n",
      "Epoch:  620 | step  50 | train loss: 1.9538\n",
      "Epoch:  620 | step  100 | train loss: 2.0548\n",
      "======== Epoch 620 Toal loss 2.0100578399208504 =========\n",
      "Epoch:  621 | step  50 | train loss: 2.0394\n",
      "Epoch:  621 | step  100 | train loss: 2.1728\n",
      "======== Epoch 621 Toal loss 2.006632811654874 =========\n",
      "Epoch:  622 | step  50 | train loss: 1.9623\n",
      "Epoch:  622 | step  100 | train loss: 2.1287\n",
      "======== Epoch 622 Toal loss 2.0055327250705504 =========\n",
      "Epoch:  623 | step  50 | train loss: 1.9699\n",
      "Epoch:  623 | step  100 | train loss: 1.9490\n",
      "======== Epoch 623 Toal loss 2.006241223676418 =========\n",
      "Epoch:  624 | step  50 | train loss: 2.0260\n",
      "Epoch:  624 | step  100 | train loss: 2.1050\n",
      "======== Epoch 624 Toal loss 2.000233499984431 =========\n",
      "Epoch:  625 | step  50 | train loss: 1.9828\n",
      "Epoch:  625 | step  100 | train loss: 2.1080\n",
      "======== Epoch 625 Toal loss 2.0003613892609513 =========\n",
      "Epoch:  626 | step  50 | train loss: 1.8838\n",
      "Epoch:  626 | step  100 | train loss: 1.9488\n",
      "======== Epoch 626 Toal loss 1.9940354165022935 =========\n",
      "Epoch:  627 | step  50 | train loss: 1.8617\n",
      "Epoch:  627 | step  100 | train loss: 1.9812\n",
      "======== Epoch 627 Toal loss 1.9926549700217517 =========\n",
      "Epoch:  628 | step  50 | train loss: 1.9831\n",
      "Epoch:  628 | step  100 | train loss: 2.0740\n",
      "======== Epoch 628 Toal loss 1.9885718522033071 =========\n",
      "Epoch:  629 | step  50 | train loss: 2.0294\n",
      "Epoch:  629 | step  100 | train loss: 1.9474\n",
      "======== Epoch 629 Toal loss 1.9883605396844508 =========\n",
      "Epoch:  630 | step  50 | train loss: 1.9425\n",
      "Epoch:  630 | step  100 | train loss: 1.8882\n",
      "======== Epoch 630 Toal loss 1.9861649429895045 =========\n",
      "Epoch:  631 | step  50 | train loss: 1.9680\n",
      "Epoch:  631 | step  100 | train loss: 1.9344\n",
      "======== Epoch 631 Toal loss 1.9797879825762617 =========\n",
      "Epoch:  632 | step  50 | train loss: 1.9349\n",
      "Epoch:  632 | step  100 | train loss: 2.0291\n",
      "======== Epoch 632 Toal loss 1.9771523940853957 =========\n",
      "Epoch:  633 | step  50 | train loss: 2.1303\n",
      "Epoch:  633 | step  100 | train loss: 2.0085\n",
      "======== Epoch 633 Toal loss 1.9775314941638853 =========\n",
      "Epoch:  634 | step  50 | train loss: 2.0125\n",
      "Epoch:  634 | step  100 | train loss: 2.0421\n",
      "======== Epoch 634 Toal loss 1.9752374625787503 =========\n",
      "Epoch:  635 | step  50 | train loss: 1.9760\n",
      "Epoch:  635 | step  100 | train loss: 1.9277\n",
      "======== Epoch 635 Toal loss 1.967188874880473 =========\n",
      "Epoch:  636 | step  50 | train loss: 1.9225\n",
      "Epoch:  636 | step  100 | train loss: 1.8401\n",
      "======== Epoch 636 Toal loss 1.9629796685242071 =========\n",
      "Epoch:  637 | step  50 | train loss: 2.0142\n",
      "Epoch:  637 | step  100 | train loss: 1.9801\n",
      "======== Epoch 637 Toal loss 1.9650397252261154 =========\n",
      "Epoch:  638 | step  50 | train loss: 1.9678\n",
      "Epoch:  638 | step  100 | train loss: 1.8538\n",
      "======== Epoch 638 Toal loss 1.964427337413881 =========\n",
      "Epoch:  639 | step  50 | train loss: 1.9691\n",
      "Epoch:  639 | step  100 | train loss: 2.0023\n",
      "======== Epoch 639 Toal loss 1.960805461658695 =========\n",
      "Epoch:  640 | step  50 | train loss: 1.8448\n",
      "Epoch:  640 | step  100 | train loss: 2.0764\n",
      "======== Epoch 640 Toal loss 1.9526112554519157 =========\n",
      "Epoch:  641 | step  50 | train loss: 1.8972\n",
      "Epoch:  641 | step  100 | train loss: 1.8220\n",
      "======== Epoch 641 Toal loss 1.951737574445523 =========\n",
      "Epoch:  642 | step  50 | train loss: 1.8886\n",
      "Epoch:  642 | step  100 | train loss: 2.0779\n",
      "======== Epoch 642 Toal loss 1.9485136154221325 =========\n",
      "Epoch:  643 | step  50 | train loss: 2.0314\n",
      "Epoch:  643 | step  100 | train loss: 1.8675\n",
      "======== Epoch 643 Toal loss 1.9505175836687165 =========\n",
      "Epoch:  644 | step  50 | train loss: 1.9237\n",
      "Epoch:  644 | step  100 | train loss: 1.8392\n",
      "======== Epoch 644 Toal loss 1.9445783975647717 =========\n",
      "Epoch:  645 | step  50 | train loss: 1.9206\n",
      "Epoch:  645 | step  100 | train loss: 1.9954\n",
      "======== Epoch 645 Toal loss 1.943382999761318 =========\n",
      "Epoch:  646 | step  50 | train loss: 1.9936\n",
      "Epoch:  646 | step  100 | train loss: 1.9502\n",
      "======== Epoch 646 Toal loss 1.9398189036826776 =========\n",
      "Epoch:  647 | step  50 | train loss: 1.9075\n",
      "Epoch:  647 | step  100 | train loss: 1.9281\n",
      "======== Epoch 647 Toal loss 1.9434524493488838 =========\n",
      "Epoch:  648 | step  50 | train loss: 1.8997\n",
      "Epoch:  648 | step  100 | train loss: 2.0419\n",
      "======== Epoch 648 Toal loss 1.9407227921292065 =========\n",
      "Epoch:  649 | step  50 | train loss: 1.9676\n",
      "Epoch:  649 | step  100 | train loss: 2.0185\n",
      "======== Epoch 649 Toal loss 1.9346735021932338 =========\n",
      "Epoch:  650 | step  50 | train loss: 1.8563\n",
      "Epoch:  650 | step  100 | train loss: 2.0236\n",
      "======== Epoch 650 Toal loss 1.9347932619777153 =========\n",
      "Epoch:  651 | step  50 | train loss: 1.8926\n",
      "Epoch:  651 | step  100 | train loss: 1.8024\n",
      "======== Epoch 651 Toal loss 1.9291725052081472 =========\n",
      "Epoch:  652 | step  50 | train loss: 1.9283\n",
      "Epoch:  652 | step  100 | train loss: 1.9807\n",
      "======== Epoch 652 Toal loss 1.9291546199379899 =========\n",
      "Epoch:  653 | step  50 | train loss: 1.7921\n",
      "Epoch:  653 | step  100 | train loss: 1.7923\n",
      "======== Epoch 653 Toal loss 1.9248795586872876 =========\n",
      "Epoch:  654 | step  50 | train loss: 1.9161\n",
      "Epoch:  654 | step  100 | train loss: 2.0527\n",
      "======== Epoch 654 Toal loss 1.9241864322646847 =========\n",
      "Epoch:  655 | step  50 | train loss: 1.8682\n",
      "Epoch:  655 | step  100 | train loss: 1.8781\n",
      "======== Epoch 655 Toal loss 1.9100367363875475 =========\n",
      "Epoch:  656 | step  50 | train loss: 1.8279\n",
      "Epoch:  656 | step  100 | train loss: 1.8752\n",
      "======== Epoch 656 Toal loss 1.9156099625719272 =========\n",
      "Epoch:  657 | step  50 | train loss: 1.9023\n",
      "Epoch:  657 | step  100 | train loss: 1.9114\n",
      "======== Epoch 657 Toal loss 1.9128846259621102 =========\n",
      "Epoch:  658 | step  50 | train loss: 1.8328\n",
      "Epoch:  658 | step  100 | train loss: 2.0775\n",
      "======== Epoch 658 Toal loss 1.9101093086769911 =========\n",
      "Epoch:  659 | step  50 | train loss: 1.8434\n",
      "Epoch:  659 | step  100 | train loss: 1.9561\n",
      "======== Epoch 659 Toal loss 1.9061599543424157 =========\n",
      "Epoch:  660 | step  50 | train loss: 2.0010\n",
      "Epoch:  660 | step  100 | train loss: 2.1173\n",
      "======== Epoch 660 Toal loss 1.90362392596113 =========\n",
      "Epoch:  661 | step  50 | train loss: 1.9981\n",
      "Epoch:  661 | step  100 | train loss: 1.8475\n",
      "======== Epoch 661 Toal loss 1.9026362469525842 =========\n",
      "Epoch:  662 | step  50 | train loss: 1.8706\n",
      "Epoch:  662 | step  100 | train loss: 1.9214\n",
      "======== Epoch 662 Toal loss 1.9021623212147534 =========\n",
      "Epoch:  663 | step  50 | train loss: 1.8990\n",
      "Epoch:  663 | step  100 | train loss: 1.9154\n",
      "======== Epoch 663 Toal loss 1.8985119428091901 =========\n",
      "Epoch:  664 | step  50 | train loss: 1.9102\n",
      "Epoch:  664 | step  100 | train loss: 1.9332\n",
      "======== Epoch 664 Toal loss 1.8964410089864963 =========\n",
      "Epoch:  665 | step  50 | train loss: 1.7800\n",
      "Epoch:  665 | step  100 | train loss: 2.0176\n",
      "======== Epoch 665 Toal loss 1.8940087081940193 =========\n",
      "Epoch:  666 | step  50 | train loss: 1.8412\n",
      "Epoch:  666 | step  100 | train loss: 1.8436\n",
      "======== Epoch 666 Toal loss 1.8917681560283754 =========\n",
      "Epoch:  667 | step  50 | train loss: 1.8461\n",
      "Epoch:  667 | step  100 | train loss: 1.8941\n",
      "======== Epoch 667 Toal loss 1.8894719447546857 =========\n",
      "Epoch:  668 | step  50 | train loss: 2.0245\n",
      "Epoch:  668 | step  100 | train loss: 1.9658\n",
      "======== Epoch 668 Toal loss 1.8875033273929502 =========\n",
      "Epoch:  669 | step  50 | train loss: 1.8565\n",
      "Epoch:  669 | step  100 | train loss: 1.8959\n",
      "======== Epoch 669 Toal loss 1.888572231540835 =========\n",
      "Epoch:  670 | step  50 | train loss: 1.8075\n",
      "Epoch:  670 | step  100 | train loss: 1.9343\n",
      "======== Epoch 670 Toal loss 1.8829026493599745 =========\n",
      "Epoch:  671 | step  50 | train loss: 1.8391\n",
      "Epoch:  671 | step  100 | train loss: 1.8412\n",
      "======== Epoch 671 Toal loss 1.8769902911612657 =========\n",
      "Epoch:  672 | step  50 | train loss: 1.8792\n",
      "Epoch:  672 | step  100 | train loss: 1.7443\n",
      "======== Epoch 672 Toal loss 1.8731786410013835 =========\n",
      "Epoch:  673 | step  50 | train loss: 1.8816\n",
      "Epoch:  673 | step  100 | train loss: 1.7988\n",
      "======== Epoch 673 Toal loss 1.8720234438655823 =========\n",
      "Epoch:  674 | step  50 | train loss: 1.8437\n",
      "Epoch:  674 | step  100 | train loss: 1.8619\n",
      "======== Epoch 674 Toal loss 1.8682681128261536 =========\n",
      "Epoch:  675 | step  50 | train loss: 1.9259\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  675 | step  100 | train loss: 1.9379\n",
      "======== Epoch 675 Toal loss 1.868881484357322 =========\n",
      "Epoch:  676 | step  50 | train loss: 1.9159\n",
      "Epoch:  676 | step  100 | train loss: 1.9934\n",
      "======== Epoch 676 Toal loss 1.8708878212827977 =========\n",
      "Epoch:  677 | step  50 | train loss: 1.8729\n",
      "Epoch:  677 | step  100 | train loss: 1.9019\n",
      "======== Epoch 677 Toal loss 1.8645138226873506 =========\n",
      "Epoch:  678 | step  50 | train loss: 1.7695\n",
      "Epoch:  678 | step  100 | train loss: 1.7516\n",
      "======== Epoch 678 Toal loss 1.8645237403187325 =========\n",
      "Epoch:  679 | step  50 | train loss: 1.8802\n",
      "Epoch:  679 | step  100 | train loss: 1.8010\n",
      "======== Epoch 679 Toal loss 1.8622290758582634 =========\n",
      "Epoch:  680 | step  50 | train loss: 1.9301\n",
      "Epoch:  680 | step  100 | train loss: 1.8575\n",
      "======== Epoch 680 Toal loss 1.8554004556764432 =========\n",
      "Epoch:  681 | step  50 | train loss: 1.9286\n",
      "Epoch:  681 | step  100 | train loss: 1.8461\n",
      "======== Epoch 681 Toal loss 1.8547080571089334 =========\n",
      "Epoch:  682 | step  50 | train loss: 1.8567\n",
      "Epoch:  682 | step  100 | train loss: 1.6963\n",
      "======== Epoch 682 Toal loss 1.8493860349422548 =========\n",
      "Epoch:  683 | step  50 | train loss: 1.9556\n",
      "Epoch:  683 | step  100 | train loss: 1.8625\n",
      "======== Epoch 683 Toal loss 1.8530168707777814 =========\n",
      "Epoch:  684 | step  50 | train loss: 1.8099\n",
      "Epoch:  684 | step  100 | train loss: 1.9361\n",
      "======== Epoch 684 Toal loss 1.8481553870487988 =========\n",
      "Epoch:  685 | step  50 | train loss: 1.8674\n",
      "Epoch:  685 | step  100 | train loss: 1.9101\n",
      "======== Epoch 685 Toal loss 1.8474567634303396 =========\n",
      "Epoch:  686 | step  50 | train loss: 1.9023\n",
      "Epoch:  686 | step  100 | train loss: 1.8582\n",
      "======== Epoch 686 Toal loss 1.8449823807894699 =========\n",
      "Epoch:  687 | step  50 | train loss: 1.7574\n",
      "Epoch:  687 | step  100 | train loss: 1.8026\n",
      "======== Epoch 687 Toal loss 1.8436765476940125 =========\n",
      "Epoch:  688 | step  50 | train loss: 1.9583\n",
      "Epoch:  688 | step  100 | train loss: 1.7756\n",
      "======== Epoch 688 Toal loss 1.8438888856065951 =========\n",
      "Epoch:  689 | step  50 | train loss: 1.8439\n",
      "Epoch:  689 | step  100 | train loss: 1.8431\n",
      "======== Epoch 689 Toal loss 1.8345663169535196 =========\n",
      "Epoch:  690 | step  50 | train loss: 1.8164\n",
      "Epoch:  690 | step  100 | train loss: 1.8019\n",
      "======== Epoch 690 Toal loss 1.8337523355716612 =========\n",
      "Epoch:  691 | step  50 | train loss: 1.7742\n",
      "Epoch:  691 | step  100 | train loss: 1.7739\n",
      "======== Epoch 691 Toal loss 1.8262567839971402 =========\n",
      "Epoch:  692 | step  50 | train loss: 1.8073\n",
      "Epoch:  692 | step  100 | train loss: 1.9391\n",
      "======== Epoch 692 Toal loss 1.8345775071198378 =========\n",
      "Epoch:  693 | step  50 | train loss: 1.6709\n",
      "Epoch:  693 | step  100 | train loss: 1.7665\n",
      "======== Epoch 693 Toal loss 1.8244918642974481 =========\n",
      "Epoch:  694 | step  50 | train loss: 1.8075\n",
      "Epoch:  694 | step  100 | train loss: 1.8024\n",
      "======== Epoch 694 Toal loss 1.8235056545676254 =========\n",
      "Epoch:  695 | step  50 | train loss: 1.7830\n",
      "Epoch:  695 | step  100 | train loss: 2.0511\n",
      "======== Epoch 695 Toal loss 1.819489867706609 =========\n",
      "Epoch:  696 | step  50 | train loss: 1.9537\n",
      "Epoch:  696 | step  100 | train loss: 1.7630\n",
      "======== Epoch 696 Toal loss 1.822354739274436 =========\n",
      "Epoch:  697 | step  50 | train loss: 1.8686\n",
      "Epoch:  697 | step  100 | train loss: 1.8885\n",
      "======== Epoch 697 Toal loss 1.8178184584873478 =========\n",
      "Epoch:  698 | step  50 | train loss: 1.7594\n",
      "Epoch:  698 | step  100 | train loss: 1.6946\n",
      "======== Epoch 698 Toal loss 1.8129762498343862 =========\n",
      "Epoch:  699 | step  50 | train loss: 1.7759\n",
      "Epoch:  699 | step  100 | train loss: 1.7966\n",
      "======== Epoch 699 Toal loss 1.810970739620488 =========\n",
      "Epoch:  700 | step  50 | train loss: 1.7549\n",
      "Epoch:  700 | step  100 | train loss: 1.7930\n",
      "======== Epoch 700 Toal loss 1.8122394395068409 =========\n",
      "Epoch:  701 | step  50 | train loss: 1.7915\n",
      "Epoch:  701 | step  100 | train loss: 1.7770\n",
      "======== Epoch 701 Toal loss 1.8051377205344719 =========\n",
      "Epoch:  702 | step  50 | train loss: 1.7519\n",
      "Epoch:  702 | step  100 | train loss: 1.8799\n",
      "======== Epoch 702 Toal loss 1.8092870363375035 =========\n",
      "Epoch:  703 | step  50 | train loss: 1.7784\n",
      "Epoch:  703 | step  100 | train loss: 1.8725\n",
      "======== Epoch 703 Toal loss 1.8043324743829123 =========\n",
      "Epoch:  704 | step  50 | train loss: 1.7548\n",
      "Epoch:  704 | step  100 | train loss: 1.9037\n",
      "======== Epoch 704 Toal loss 1.8018970935325311 =========\n",
      "Epoch:  705 | step  50 | train loss: 1.7925\n",
      "Epoch:  705 | step  100 | train loss: 1.7577\n",
      "======== Epoch 705 Toal loss 1.8033964343187285 =========\n",
      "Epoch:  706 | step  50 | train loss: 1.6962\n",
      "Epoch:  706 | step  100 | train loss: 1.9055\n",
      "======== Epoch 706 Toal loss 1.7966424498131606 =========\n",
      "Epoch:  707 | step  50 | train loss: 1.7357\n",
      "Epoch:  707 | step  100 | train loss: 1.8148\n",
      "======== Epoch 707 Toal loss 1.7969931441593945 =========\n",
      "Epoch:  708 | step  50 | train loss: 1.8272\n",
      "Epoch:  708 | step  100 | train loss: 1.8975\n",
      "======== Epoch 708 Toal loss 1.7921700477600098 =========\n",
      "Epoch:  709 | step  50 | train loss: 1.9724\n",
      "Epoch:  709 | step  100 | train loss: 1.7622\n",
      "======== Epoch 709 Toal loss 1.7903241965828873 =========\n",
      "Epoch:  710 | step  50 | train loss: 1.9177\n",
      "Epoch:  710 | step  100 | train loss: 1.7446\n",
      "======== Epoch 710 Toal loss 1.7849172033914706 =========\n",
      "Epoch:  711 | step  50 | train loss: 1.6991\n",
      "Epoch:  711 | step  100 | train loss: 1.7162\n",
      "======== Epoch 711 Toal loss 1.7865487298345177 =========\n",
      "Epoch:  712 | step  50 | train loss: 1.8208\n",
      "Epoch:  712 | step  100 | train loss: 1.8211\n",
      "======== Epoch 712 Toal loss 1.784453722519603 =========\n",
      "Epoch:  713 | step  50 | train loss: 1.7081\n",
      "Epoch:  713 | step  100 | train loss: 1.8432\n",
      "======== Epoch 713 Toal loss 1.7837307763293506 =========\n",
      "Epoch:  714 | step  50 | train loss: 1.8751\n",
      "Epoch:  714 | step  100 | train loss: 1.7664\n",
      "======== Epoch 714 Toal loss 1.7786106375174793 =========\n",
      "Epoch:  715 | step  50 | train loss: 1.7998\n",
      "Epoch:  715 | step  100 | train loss: 1.7799\n",
      "======== Epoch 715 Toal loss 1.7780254236081752 =========\n",
      "Epoch:  716 | step  50 | train loss: 1.6989\n",
      "Epoch:  716 | step  100 | train loss: 1.7257\n",
      "======== Epoch 716 Toal loss 1.767072135840005 =========\n",
      "Epoch:  717 | step  50 | train loss: 1.7012\n",
      "Epoch:  717 | step  100 | train loss: 2.0145\n",
      "======== Epoch 717 Toal loss 1.7740811768586073 =========\n",
      "Epoch:  718 | step  50 | train loss: 1.7951\n",
      "Epoch:  718 | step  100 | train loss: 1.7618\n",
      "======== Epoch 718 Toal loss 1.7707109141155957 =========\n",
      "Epoch:  719 | step  50 | train loss: 1.8281\n",
      "Epoch:  719 | step  100 | train loss: 1.9349\n",
      "======== Epoch 719 Toal loss 1.7691409587860107 =========\n",
      "Epoch:  720 | step  50 | train loss: 1.7507\n",
      "Epoch:  720 | step  100 | train loss: 1.7462\n",
      "======== Epoch 720 Toal loss 1.766173100083824 =========\n",
      "Epoch:  721 | step  50 | train loss: 1.6706\n",
      "Epoch:  721 | step  100 | train loss: 1.8147\n",
      "======== Epoch 721 Toal loss 1.7646655600245407 =========\n",
      "Epoch:  722 | step  50 | train loss: 1.7659\n",
      "Epoch:  722 | step  100 | train loss: 1.7145\n",
      "======== Epoch 722 Toal loss 1.761062314839867 =========\n",
      "Epoch:  723 | step  50 | train loss: 1.8203\n",
      "Epoch:  723 | step  100 | train loss: 1.8091\n",
      "======== Epoch 723 Toal loss 1.7535053404366099 =========\n",
      "Epoch:  724 | step  50 | train loss: 1.7551\n",
      "Epoch:  724 | step  100 | train loss: 1.8090\n",
      "======== Epoch 724 Toal loss 1.7546919235369054 =========\n",
      "Epoch:  725 | step  50 | train loss: 1.7767\n",
      "Epoch:  725 | step  100 | train loss: 1.8389\n",
      "======== Epoch 725 Toal loss 1.7504165705626573 =========\n",
      "Epoch:  726 | step  50 | train loss: 1.7886\n",
      "Epoch:  726 | step  100 | train loss: 1.7902\n",
      "======== Epoch 726 Toal loss 1.7533934736639503 =========\n",
      "Epoch:  727 | step  50 | train loss: 1.7217\n",
      "Epoch:  727 | step  100 | train loss: 1.8222\n",
      "======== Epoch 727 Toal loss 1.744213895099919 =========\n",
      "Epoch:  728 | step  50 | train loss: 1.6395\n",
      "Epoch:  728 | step  100 | train loss: 1.6849\n",
      "======== Epoch 728 Toal loss 1.7416887041029891 =========\n",
      "Epoch:  729 | step  50 | train loss: 1.6226\n",
      "Epoch:  729 | step  100 | train loss: 1.6641\n",
      "======== Epoch 729 Toal loss 1.744207542117049 =========\n",
      "Epoch:  730 | step  50 | train loss: 1.7382\n",
      "Epoch:  730 | step  100 | train loss: 1.9089\n",
      "======== Epoch 730 Toal loss 1.742674689951951 =========\n",
      "Epoch:  731 | step  50 | train loss: 1.7660\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  731 | step  100 | train loss: 1.6485\n",
      "======== Epoch 731 Toal loss 1.7398996420992099 =========\n",
      "Epoch:  732 | step  50 | train loss: 1.7059\n",
      "Epoch:  732 | step  100 | train loss: 1.7542\n",
      "======== Epoch 732 Toal loss 1.7409777418384706 =========\n",
      "Epoch:  733 | step  50 | train loss: 1.8021\n",
      "Epoch:  733 | step  100 | train loss: 1.8328\n",
      "======== Epoch 733 Toal loss 1.7379850750047017 =========\n",
      "Epoch:  734 | step  50 | train loss: 1.6669\n",
      "Epoch:  734 | step  100 | train loss: 1.6992\n",
      "======== Epoch 734 Toal loss 1.7331153144681357 =========\n",
      "Epoch:  735 | step  50 | train loss: 1.8339\n",
      "Epoch:  735 | step  100 | train loss: 1.7344\n",
      "======== Epoch 735 Toal loss 1.7307930574184511 =========\n",
      "Epoch:  736 | step  50 | train loss: 1.5683\n",
      "Epoch:  736 | step  100 | train loss: 1.6287\n",
      "======== Epoch 736 Toal loss 1.7285748254962083 =========\n",
      "Epoch:  737 | step  50 | train loss: 1.7351\n",
      "Epoch:  737 | step  100 | train loss: 1.6834\n",
      "======== Epoch 737 Toal loss 1.7254545339723912 =========\n",
      "Epoch:  738 | step  50 | train loss: 1.7650\n",
      "Epoch:  738 | step  100 | train loss: 1.6815\n",
      "======== Epoch 738 Toal loss 1.7260072541430713 =========\n",
      "Epoch:  739 | step  50 | train loss: 1.8244\n",
      "Epoch:  739 | step  100 | train loss: 1.7871\n",
      "======== Epoch 739 Toal loss 1.7249068264069596 =========\n",
      "Epoch:  740 | step  50 | train loss: 1.7799\n",
      "Epoch:  740 | step  100 | train loss: 1.5861\n",
      "======== Epoch 740 Toal loss 1.7202108516925718 =========\n",
      "Epoch:  741 | step  50 | train loss: 1.6258\n",
      "Epoch:  741 | step  100 | train loss: 1.8468\n",
      "======== Epoch 741 Toal loss 1.7166577945879804 =========\n",
      "Epoch:  742 | step  50 | train loss: 1.6163\n",
      "Epoch:  742 | step  100 | train loss: 1.6917\n",
      "======== Epoch 742 Toal loss 1.7095589414844667 =========\n",
      "Epoch:  743 | step  50 | train loss: 1.7826\n",
      "Epoch:  743 | step  100 | train loss: 1.7359\n",
      "======== Epoch 743 Toal loss 1.7181584137242014 =========\n",
      "Epoch:  744 | step  50 | train loss: 1.6550\n",
      "Epoch:  744 | step  100 | train loss: 1.7366\n",
      "======== Epoch 744 Toal loss 1.7136516493510425 =========\n",
      "Epoch:  745 | step  50 | train loss: 1.7066\n",
      "Epoch:  745 | step  100 | train loss: 1.7833\n",
      "======== Epoch 745 Toal loss 1.708721875175228 =========\n",
      "Epoch:  746 | step  50 | train loss: 1.7875\n",
      "Epoch:  746 | step  100 | train loss: 1.6517\n",
      "======== Epoch 746 Toal loss 1.7082088875576733 =========\n",
      "Epoch:  747 | step  50 | train loss: 1.7430\n",
      "Epoch:  747 | step  100 | train loss: 1.7969\n",
      "======== Epoch 747 Toal loss 1.7061254008998716 =========\n",
      "Epoch:  748 | step  50 | train loss: 1.6783\n",
      "Epoch:  748 | step  100 | train loss: 1.6322\n",
      "======== Epoch 748 Toal loss 1.703302803078318 =========\n",
      "Epoch:  749 | step  50 | train loss: 1.6764\n",
      "Epoch:  749 | step  100 | train loss: 1.7283\n",
      "======== Epoch 749 Toal loss 1.7001338751335455 =========\n",
      "Epoch:  750 | step  50 | train loss: 1.6096\n",
      "Epoch:  750 | step  100 | train loss: 1.6848\n",
      "======== Epoch 750 Toal loss 1.6978052263337422 =========\n",
      "Epoch:  751 | step  50 | train loss: 1.7012\n",
      "Epoch:  751 | step  100 | train loss: 1.6156\n",
      "======== Epoch 751 Toal loss 1.6971127608927286 =========\n",
      "Epoch:  752 | step  50 | train loss: 1.7126\n",
      "Epoch:  752 | step  100 | train loss: 1.7121\n",
      "======== Epoch 752 Toal loss 1.6947051092861145 =========\n",
      "Epoch:  753 | step  50 | train loss: 1.6161\n",
      "Epoch:  753 | step  100 | train loss: 1.7805\n",
      "======== Epoch 753 Toal loss 1.6909260924269514 =========\n",
      "Epoch:  754 | step  50 | train loss: 1.6355\n",
      "Epoch:  754 | step  100 | train loss: 1.6780\n",
      "======== Epoch 754 Toal loss 1.689109238182626 =========\n",
      "Epoch:  755 | step  50 | train loss: 1.7942\n",
      "Epoch:  755 | step  100 | train loss: 1.7790\n",
      "======== Epoch 755 Toal loss 1.687751674070591 =========\n",
      "Epoch:  756 | step  50 | train loss: 1.7423\n",
      "Epoch:  756 | step  100 | train loss: 1.7314\n",
      "======== Epoch 756 Toal loss 1.6880202555074924 =========\n",
      "Epoch:  757 | step  50 | train loss: 1.5320\n",
      "Epoch:  757 | step  100 | train loss: 1.5917\n",
      "======== Epoch 757 Toal loss 1.683710553781773 =========\n",
      "Epoch:  758 | step  50 | train loss: 1.6920\n",
      "Epoch:  758 | step  100 | train loss: 1.6564\n",
      "======== Epoch 758 Toal loss 1.682401642566774 =========\n",
      "Epoch:  759 | step  50 | train loss: 1.6545\n",
      "Epoch:  759 | step  100 | train loss: 1.6422\n",
      "======== Epoch 759 Toal loss 1.677017806991329 =========\n",
      "Epoch:  760 | step  50 | train loss: 1.5636\n",
      "Epoch:  760 | step  100 | train loss: 1.6370\n",
      "======== Epoch 760 Toal loss 1.6781687610517673 =========\n",
      "Epoch:  761 | step  50 | train loss: 1.5880\n",
      "Epoch:  761 | step  100 | train loss: 1.4628\n",
      "======== Epoch 761 Toal loss 1.6773015725903395 =========\n",
      "Epoch:  762 | step  50 | train loss: 1.6390\n",
      "Epoch:  762 | step  100 | train loss: 1.6218\n",
      "======== Epoch 762 Toal loss 1.675025911835151 =========\n",
      "Epoch:  763 | step  50 | train loss: 1.6903\n",
      "Epoch:  763 | step  100 | train loss: 1.6024\n",
      "======== Epoch 763 Toal loss 1.6747423517025584 =========\n",
      "Epoch:  764 | step  50 | train loss: 1.5465\n",
      "Epoch:  764 | step  100 | train loss: 1.7286\n",
      "======== Epoch 764 Toal loss 1.6741595655922 =========\n",
      "Epoch:  765 | step  50 | train loss: 1.6334\n",
      "Epoch:  765 | step  100 | train loss: 1.6624\n",
      "======== Epoch 765 Toal loss 1.6673057340994113 =========\n",
      "Epoch:  766 | step  50 | train loss: 1.5523\n",
      "Epoch:  766 | step  100 | train loss: 1.7523\n",
      "======== Epoch 766 Toal loss 1.6671522187023629 =========\n",
      "Epoch:  767 | step  50 | train loss: 1.6383\n",
      "Epoch:  767 | step  100 | train loss: 1.6796\n",
      "======== Epoch 767 Toal loss 1.6618701994903688 =========\n",
      "Epoch:  768 | step  50 | train loss: 1.8121\n",
      "Epoch:  768 | step  100 | train loss: 1.7243\n",
      "======== Epoch 768 Toal loss 1.6572355448715086 =========\n",
      "Epoch:  769 | step  50 | train loss: 1.6023\n",
      "Epoch:  769 | step  100 | train loss: 1.6304\n",
      "======== Epoch 769 Toal loss 1.6586161415751388 =========\n",
      "Epoch:  770 | step  50 | train loss: 1.6582\n",
      "Epoch:  770 | step  100 | train loss: 1.7247\n",
      "======== Epoch 770 Toal loss 1.6542533073968035 =========\n",
      "Epoch:  771 | step  50 | train loss: 1.6282\n",
      "Epoch:  771 | step  100 | train loss: 1.6621\n",
      "======== Epoch 771 Toal loss 1.6558205480498027 =========\n",
      "Epoch:  772 | step  50 | train loss: 1.6183\n",
      "Epoch:  772 | step  100 | train loss: 1.5984\n",
      "======== Epoch 772 Toal loss 1.6546973639387426 =========\n",
      "Epoch:  773 | step  50 | train loss: 1.6292\n",
      "Epoch:  773 | step  100 | train loss: 1.7094\n",
      "======== Epoch 773 Toal loss 1.6499846650332939 =========\n",
      "Epoch:  774 | step  50 | train loss: 1.6921\n",
      "Epoch:  774 | step  100 | train loss: 1.6198\n",
      "======== Epoch 774 Toal loss 1.6459373807519433 =========\n",
      "Epoch:  775 | step  50 | train loss: 1.6901\n",
      "Epoch:  775 | step  100 | train loss: 1.5541\n",
      "======== Epoch 775 Toal loss 1.6453597477781094 =========\n",
      "Epoch:  776 | step  50 | train loss: 1.7217\n",
      "Epoch:  776 | step  100 | train loss: 1.5759\n",
      "======== Epoch 776 Toal loss 1.642530303660447 =========\n",
      "Epoch:  777 | step  50 | train loss: 1.5450\n",
      "Epoch:  777 | step  100 | train loss: 1.5569\n",
      "======== Epoch 777 Toal loss 1.641700868684102 =========\n",
      "Epoch:  778 | step  50 | train loss: 1.7334\n",
      "Epoch:  778 | step  100 | train loss: 1.6994\n",
      "======== Epoch 778 Toal loss 1.639493541019719 =========\n",
      "Epoch:  779 | step  50 | train loss: 1.5233\n",
      "Epoch:  779 | step  100 | train loss: 1.5027\n",
      "======== Epoch 779 Toal loss 1.644119946937251 =========\n",
      "Epoch:  780 | step  50 | train loss: 1.6387\n",
      "Epoch:  780 | step  100 | train loss: 1.7572\n",
      "======== Epoch 780 Toal loss 1.6388570167184846 =========\n",
      "Epoch:  781 | step  50 | train loss: 1.5607\n",
      "Epoch:  781 | step  100 | train loss: 1.6860\n",
      "======== Epoch 781 Toal loss 1.639111234889767 =========\n",
      "Epoch:  782 | step  50 | train loss: 1.7356\n",
      "Epoch:  782 | step  100 | train loss: 1.6317\n",
      "======== Epoch 782 Toal loss 1.6301414753363384 =========\n",
      "Epoch:  783 | step  50 | train loss: 1.6007\n",
      "Epoch:  783 | step  100 | train loss: 1.7073\n",
      "======== Epoch 783 Toal loss 1.6341676014225657 =========\n",
      "Epoch:  784 | step  50 | train loss: 1.6525\n",
      "Epoch:  784 | step  100 | train loss: 1.6986\n",
      "======== Epoch 784 Toal loss 1.6339653410562656 =========\n",
      "Epoch:  785 | step  50 | train loss: 1.6117\n",
      "Epoch:  785 | step  100 | train loss: 1.6055\n",
      "======== Epoch 785 Toal loss 1.6288285013136825 =========\n",
      "Epoch:  786 | step  50 | train loss: 1.5617\n",
      "Epoch:  786 | step  100 | train loss: 1.6195\n",
      "======== Epoch 786 Toal loss 1.623512028678646 =========\n",
      "Epoch:  787 | step  50 | train loss: 1.6595\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  787 | step  100 | train loss: 1.7317\n",
      "======== Epoch 787 Toal loss 1.6213777171887034 =========\n",
      "Epoch:  788 | step  50 | train loss: 1.6503\n",
      "Epoch:  788 | step  100 | train loss: 1.5329\n",
      "======== Epoch 788 Toal loss 1.617895375422346 =========\n",
      "Epoch:  789 | step  50 | train loss: 1.5888\n",
      "Epoch:  789 | step  100 | train loss: 1.6768\n",
      "======== Epoch 789 Toal loss 1.6162961653577603 =========\n",
      "Epoch:  790 | step  50 | train loss: 1.6010\n",
      "Epoch:  790 | step  100 | train loss: 1.6233\n",
      "======== Epoch 790 Toal loss 1.6169116913787718 =========\n",
      "Epoch:  791 | step  50 | train loss: 1.6361\n",
      "Epoch:  791 | step  100 | train loss: 1.5624\n",
      "======== Epoch 791 Toal loss 1.6164991254728984 =========\n",
      "Epoch:  792 | step  50 | train loss: 1.6481\n",
      "Epoch:  792 | step  100 | train loss: 1.5956\n",
      "======== Epoch 792 Toal loss 1.6160169461878335 =========\n",
      "Epoch:  793 | step  50 | train loss: 1.4975\n",
      "Epoch:  793 | step  100 | train loss: 1.6813\n",
      "======== Epoch 793 Toal loss 1.613447644846226 =========\n",
      "Epoch:  794 | step  50 | train loss: 1.5932\n",
      "Epoch:  794 | step  100 | train loss: 1.5676\n",
      "======== Epoch 794 Toal loss 1.61305424740644 =========\n",
      "Epoch:  795 | step  50 | train loss: 1.5531\n",
      "Epoch:  795 | step  100 | train loss: 1.6135\n",
      "======== Epoch 795 Toal loss 1.6096469357730896 =========\n",
      "Epoch:  796 | step  50 | train loss: 1.6124\n",
      "Epoch:  796 | step  100 | train loss: 1.6573\n",
      "======== Epoch 796 Toal loss 1.604898161035243 =========\n",
      "Epoch:  797 | step  50 | train loss: 1.6283\n",
      "Epoch:  797 | step  100 | train loss: 1.6292\n",
      "======== Epoch 797 Toal loss 1.6000315639061657 =========\n",
      "Epoch:  798 | step  50 | train loss: 1.5856\n",
      "Epoch:  798 | step  100 | train loss: 1.6516\n",
      "======== Epoch 798 Toal loss 1.6002030663373994 =========\n",
      "Epoch:  799 | step  50 | train loss: 1.5642\n",
      "Epoch:  799 | step  100 | train loss: 1.4910\n",
      "======== Epoch 799 Toal loss 1.5996646755109958 =========\n",
      "Epoch:  800 | step  50 | train loss: 1.6947\n",
      "Epoch:  800 | step  100 | train loss: 1.6342\n",
      "======== Epoch 800 Toal loss 1.59555915797629 =========\n",
      "Epoch:  801 | step  50 | train loss: 1.5744\n",
      "Epoch:  801 | step  100 | train loss: 1.5423\n",
      "======== Epoch 801 Toal loss 1.5980890155807743 =========\n",
      "Epoch:  802 | step  50 | train loss: 1.6055\n",
      "Epoch:  802 | step  100 | train loss: 1.6866\n",
      "======== Epoch 802 Toal loss 1.5913795184313766 =========\n",
      "Epoch:  803 | step  50 | train loss: 1.4164\n",
      "Epoch:  803 | step  100 | train loss: 1.5135\n",
      "======== Epoch 803 Toal loss 1.593239516746707 =========\n",
      "Epoch:  804 | step  50 | train loss: 1.5294\n",
      "Epoch:  804 | step  100 | train loss: 1.7066\n",
      "======== Epoch 804 Toal loss 1.5916380921030433 =========\n",
      "Epoch:  805 | step  50 | train loss: 1.6606\n",
      "Epoch:  805 | step  100 | train loss: 1.5904\n",
      "======== Epoch 805 Toal loss 1.5856846192987955 =========\n",
      "Epoch:  806 | step  50 | train loss: 1.6014\n",
      "Epoch:  806 | step  100 | train loss: 1.6099\n",
      "======== Epoch 806 Toal loss 1.591956032000906 =========\n",
      "Epoch:  807 | step  50 | train loss: 1.6100\n",
      "Epoch:  807 | step  100 | train loss: 1.6592\n",
      "======== Epoch 807 Toal loss 1.5873372486936368 =========\n",
      "Epoch:  808 | step  50 | train loss: 1.6946\n",
      "Epoch:  808 | step  100 | train loss: 1.4999\n",
      "======== Epoch 808 Toal loss 1.5830886499668524 =========\n",
      "Epoch:  809 | step  50 | train loss: 1.4955\n",
      "Epoch:  809 | step  100 | train loss: 1.5645\n",
      "======== Epoch 809 Toal loss 1.577847598044853 =========\n",
      "Epoch:  810 | step  50 | train loss: 1.5120\n",
      "Epoch:  810 | step  100 | train loss: 1.5441\n",
      "======== Epoch 810 Toal loss 1.5787705260563671 =========\n",
      "Epoch:  811 | step  50 | train loss: 1.4795\n",
      "Epoch:  811 | step  100 | train loss: 1.5848\n",
      "======== Epoch 811 Toal loss 1.5794658912875787 =========\n",
      "Epoch:  812 | step  50 | train loss: 1.5362\n",
      "Epoch:  812 | step  100 | train loss: 1.4414\n",
      "======== Epoch 812 Toal loss 1.5742590785995731 =========\n",
      "Epoch:  813 | step  50 | train loss: 1.6221\n",
      "Epoch:  813 | step  100 | train loss: 1.5260\n",
      "======== Epoch 813 Toal loss 1.5739872329603366 =========\n",
      "Epoch:  814 | step  50 | train loss: 1.5535\n",
      "Epoch:  814 | step  100 | train loss: 1.6132\n",
      "======== Epoch 814 Toal loss 1.5655339006485978 =========\n",
      "Epoch:  815 | step  50 | train loss: 1.6273\n",
      "Epoch:  815 | step  100 | train loss: 1.6374\n",
      "======== Epoch 815 Toal loss 1.5644765016509266 =========\n",
      "Epoch:  816 | step  50 | train loss: 1.5821\n",
      "Epoch:  816 | step  100 | train loss: 1.5442\n",
      "======== Epoch 816 Toal loss 1.5659664917767533 =========\n",
      "Epoch:  817 | step  50 | train loss: 1.4264\n",
      "Epoch:  817 | step  100 | train loss: 1.6153\n",
      "======== Epoch 817 Toal loss 1.5571050265940225 =========\n",
      "Epoch:  818 | step  50 | train loss: 1.5588\n",
      "Epoch:  818 | step  100 | train loss: 1.5661\n",
      "======== Epoch 818 Toal loss 1.5650127253881314 =========\n",
      "Epoch:  819 | step  50 | train loss: 1.5094\n",
      "Epoch:  819 | step  100 | train loss: 1.5775\n",
      "======== Epoch 819 Toal loss 1.56135566060136 =========\n",
      "Epoch:  820 | step  50 | train loss: 1.5716\n",
      "Epoch:  820 | step  100 | train loss: 1.6822\n",
      "======== Epoch 820 Toal loss 1.5609577729450008 =========\n",
      "Epoch:  821 | step  50 | train loss: 1.5973\n",
      "Epoch:  821 | step  100 | train loss: 1.5173\n",
      "======== Epoch 821 Toal loss 1.5572708922673046 =========\n",
      "Epoch:  822 | step  50 | train loss: 1.5288\n",
      "Epoch:  822 | step  100 | train loss: 1.5022\n",
      "======== Epoch 822 Toal loss 1.5601081227868554 =========\n",
      "Epoch:  823 | step  50 | train loss: 1.5307\n",
      "Epoch:  823 | step  100 | train loss: 1.7096\n",
      "======== Epoch 823 Toal loss 1.5547363864697092 =========\n",
      "Epoch:  824 | step  50 | train loss: 1.5262\n",
      "Epoch:  824 | step  100 | train loss: 1.5677\n",
      "======== Epoch 824 Toal loss 1.5508266522632381 =========\n",
      "Epoch:  825 | step  50 | train loss: 1.6362\n",
      "Epoch:  825 | step  100 | train loss: 1.5125\n",
      "======== Epoch 825 Toal loss 1.5493383921258819 =========\n",
      "Epoch:  826 | step  50 | train loss: 1.5972\n",
      "Epoch:  826 | step  100 | train loss: 1.5071\n",
      "======== Epoch 826 Toal loss 1.548474809018577 =========\n",
      "Epoch:  827 | step  50 | train loss: 1.4421\n",
      "Epoch:  827 | step  100 | train loss: 1.5990\n",
      "======== Epoch 827 Toal loss 1.5504155178380206 =========\n",
      "Epoch:  828 | step  50 | train loss: 1.6125\n",
      "Epoch:  828 | step  100 | train loss: 1.5584\n",
      "======== Epoch 828 Toal loss 1.5437067650197966 =========\n",
      "Epoch:  829 | step  50 | train loss: 1.5665\n",
      "Epoch:  829 | step  100 | train loss: 1.5328\n",
      "======== Epoch 829 Toal loss 1.544565011815327 =========\n",
      "Epoch:  830 | step  50 | train loss: 1.5709\n",
      "Epoch:  830 | step  100 | train loss: 1.5499\n",
      "======== Epoch 830 Toal loss 1.5443972785298417 =========\n",
      "Epoch:  831 | step  50 | train loss: 1.6356\n",
      "Epoch:  831 | step  100 | train loss: 1.5856\n",
      "======== Epoch 831 Toal loss 1.5390858107466039 =========\n",
      "Epoch:  832 | step  50 | train loss: 1.5187\n",
      "Epoch:  832 | step  100 | train loss: 1.6548\n",
      "======== Epoch 832 Toal loss 1.5385271950465877 =========\n",
      "Epoch:  833 | step  50 | train loss: 1.4400\n",
      "Epoch:  833 | step  100 | train loss: 1.5455\n",
      "======== Epoch 833 Toal loss 1.5363608143193934 =========\n",
      "Epoch:  834 | step  50 | train loss: 1.5253\n",
      "Epoch:  834 | step  100 | train loss: 1.4799\n",
      "======== Epoch 834 Toal loss 1.5297166603367502 =========\n",
      "Epoch:  835 | step  50 | train loss: 1.5305\n",
      "Epoch:  835 | step  100 | train loss: 1.5965\n",
      "======== Epoch 835 Toal loss 1.5337502132586347 =========\n",
      "Epoch:  836 | step  50 | train loss: 1.4058\n",
      "Epoch:  836 | step  100 | train loss: 1.6098\n",
      "======== Epoch 836 Toal loss 1.5326055947358046 =========\n",
      "Epoch:  837 | step  50 | train loss: 1.4237\n",
      "Epoch:  837 | step  100 | train loss: 1.6701\n",
      "======== Epoch 837 Toal loss 1.5372080715691172 =========\n",
      "Epoch:  838 | step  50 | train loss: 1.4595\n",
      "Epoch:  838 | step  100 | train loss: 1.5406\n",
      "======== Epoch 838 Toal loss 1.5254868356192983 =========\n",
      "Epoch:  839 | step  50 | train loss: 1.6241\n",
      "Epoch:  839 | step  100 | train loss: 1.4648\n",
      "======== Epoch 839 Toal loss 1.527175007796869 =========\n",
      "Epoch:  840 | step  50 | train loss: 1.6314\n",
      "Epoch:  840 | step  100 | train loss: 1.5183\n",
      "======== Epoch 840 Toal loss 1.5180348031889133 =========\n",
      "Epoch:  841 | step  50 | train loss: 1.5676\n",
      "Epoch:  841 | step  100 | train loss: 1.4846\n",
      "======== Epoch 841 Toal loss 1.515983488501572 =========\n",
      "Epoch:  842 | step  50 | train loss: 1.5689\n",
      "Epoch:  842 | step  100 | train loss: 1.5701\n",
      "======== Epoch 842 Toal loss 1.5268299744381169 =========\n",
      "Epoch:  843 | step  50 | train loss: 1.4812\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  843 | step  100 | train loss: 1.5598\n",
      "======== Epoch 843 Toal loss 1.5181932274888201 =========\n",
      "Epoch:  844 | step  50 | train loss: 1.5132\n",
      "Epoch:  844 | step  100 | train loss: 1.6240\n",
      "======== Epoch 844 Toal loss 1.5174089931860202 =========\n",
      "Epoch:  845 | step  50 | train loss: 1.4004\n",
      "Epoch:  845 | step  100 | train loss: 1.5460\n",
      "======== Epoch 845 Toal loss 1.5129059708215358 =========\n",
      "Epoch:  846 | step  50 | train loss: 1.3997\n",
      "Epoch:  846 | step  100 | train loss: 1.6251\n",
      "======== Epoch 846 Toal loss 1.5116202404828576 =========\n",
      "Epoch:  847 | step  50 | train loss: 1.4741\n",
      "Epoch:  847 | step  100 | train loss: 1.5660\n",
      "======== Epoch 847 Toal loss 1.5138672212275064 =========\n",
      "Epoch:  848 | step  50 | train loss: 1.5191\n",
      "Epoch:  848 | step  100 | train loss: 1.5052\n",
      "======== Epoch 848 Toal loss 1.512097940212343 =========\n",
      "Epoch:  849 | step  50 | train loss: 1.5557\n",
      "Epoch:  849 | step  100 | train loss: 1.6047\n",
      "======== Epoch 849 Toal loss 1.5110970764625362 =========\n",
      "Epoch:  850 | step  50 | train loss: 1.4882\n",
      "Epoch:  850 | step  100 | train loss: 1.5161\n",
      "======== Epoch 850 Toal loss 1.5044943695145894 =========\n",
      "Epoch:  851 | step  50 | train loss: 1.5573\n",
      "Epoch:  851 | step  100 | train loss: 1.4743\n",
      "======== Epoch 851 Toal loss 1.5025557715718338 =========\n",
      "Epoch:  852 | step  50 | train loss: 1.6189\n",
      "Epoch:  852 | step  100 | train loss: 1.4528\n",
      "======== Epoch 852 Toal loss 1.5022544986833402 =========\n",
      "Epoch:  853 | step  50 | train loss: 1.6132\n",
      "Epoch:  853 | step  100 | train loss: 1.5143\n",
      "======== Epoch 853 Toal loss 1.5010566013615305 =========\n",
      "Epoch:  854 | step  50 | train loss: 1.4651\n",
      "Epoch:  854 | step  100 | train loss: 1.5486\n",
      "======== Epoch 854 Toal loss 1.4993020305788614 =========\n",
      "Epoch:  855 | step  50 | train loss: 1.6481\n",
      "Epoch:  855 | step  100 | train loss: 1.4856\n",
      "======== Epoch 855 Toal loss 1.4994816828549393 =========\n",
      "Epoch:  856 | step  50 | train loss: 1.5323\n",
      "Epoch:  856 | step  100 | train loss: 1.5710\n",
      "======== Epoch 856 Toal loss 1.4930905510739583 =========\n",
      "Epoch:  857 | step  50 | train loss: 1.4893\n",
      "Epoch:  857 | step  100 | train loss: 1.5126\n",
      "======== Epoch 857 Toal loss 1.4948068537363193 =========\n",
      "Epoch:  858 | step  50 | train loss: 1.5079\n",
      "Epoch:  858 | step  100 | train loss: 1.4472\n",
      "======== Epoch 858 Toal loss 1.4939502981620105 =========\n",
      "Epoch:  859 | step  50 | train loss: 1.5037\n",
      "Epoch:  859 | step  100 | train loss: 1.5181\n",
      "======== Epoch 859 Toal loss 1.4888253909785574 =========\n",
      "Epoch:  860 | step  50 | train loss: 1.5207\n",
      "Epoch:  860 | step  100 | train loss: 1.5184\n",
      "======== Epoch 860 Toal loss 1.485423495129841 =========\n",
      "Epoch:  861 | step  50 | train loss: 1.4373\n",
      "Epoch:  861 | step  100 | train loss: 1.3869\n",
      "======== Epoch 861 Toal loss 1.4885431828537607 =========\n",
      "Epoch:  862 | step  50 | train loss: 1.5134\n",
      "Epoch:  862 | step  100 | train loss: 1.5409\n",
      "======== Epoch 862 Toal loss 1.4842679112907347 =========\n",
      "Epoch:  863 | step  50 | train loss: 1.5174\n",
      "Epoch:  863 | step  100 | train loss: 1.5441\n",
      "======== Epoch 863 Toal loss 1.4826410388558862 =========\n",
      "Epoch:  864 | step  50 | train loss: 1.4497\n",
      "Epoch:  864 | step  100 | train loss: 1.6101\n",
      "======== Epoch 864 Toal loss 1.4862554141176425 =========\n",
      "Epoch:  865 | step  50 | train loss: 1.4145\n",
      "Epoch:  865 | step  100 | train loss: 1.4784\n",
      "======== Epoch 865 Toal loss 1.481959083215977 =========\n",
      "Epoch:  866 | step  50 | train loss: 1.4644\n",
      "Epoch:  866 | step  100 | train loss: 1.4434\n",
      "======== Epoch 866 Toal loss 1.4772335252141564 =========\n",
      "Epoch:  867 | step  50 | train loss: 1.5506\n",
      "Epoch:  867 | step  100 | train loss: 1.3496\n",
      "======== Epoch 867 Toal loss 1.4826058682387437 =========\n",
      "Epoch:  868 | step  50 | train loss: 1.3856\n",
      "Epoch:  868 | step  100 | train loss: 1.4728\n",
      "======== Epoch 868 Toal loss 1.4759385818388404 =========\n",
      "Epoch:  869 | step  50 | train loss: 1.4935\n",
      "Epoch:  869 | step  100 | train loss: 1.5500\n",
      "======== Epoch 869 Toal loss 1.4705003122004068 =========\n",
      "Epoch:  870 | step  50 | train loss: 1.5129\n",
      "Epoch:  870 | step  100 | train loss: 1.5997\n",
      "======== Epoch 870 Toal loss 1.4726379508894634 =========\n",
      "Epoch:  871 | step  50 | train loss: 1.3387\n",
      "Epoch:  871 | step  100 | train loss: 1.4427\n",
      "======== Epoch 871 Toal loss 1.4742333259039777 =========\n",
      "Epoch:  872 | step  50 | train loss: 1.4521\n",
      "Epoch:  872 | step  100 | train loss: 1.4890\n",
      "======== Epoch 872 Toal loss 1.4659953660112086 =========\n",
      "Epoch:  873 | step  50 | train loss: 1.5038\n",
      "Epoch:  873 | step  100 | train loss: 1.3905\n",
      "======== Epoch 873 Toal loss 1.4622241529991957 =========\n",
      "Epoch:  874 | step  50 | train loss: 1.5002\n",
      "Epoch:  874 | step  100 | train loss: 1.4755\n",
      "======== Epoch 874 Toal loss 1.4645124741686069 =========\n",
      "Epoch:  875 | step  50 | train loss: 1.4274\n",
      "Epoch:  875 | step  100 | train loss: 1.4193\n",
      "======== Epoch 875 Toal loss 1.4611021503200377 =========\n",
      "Epoch:  876 | step  50 | train loss: 1.3280\n",
      "Epoch:  876 | step  100 | train loss: 1.4426\n",
      "======== Epoch 876 Toal loss 1.4636118072804396 =========\n",
      "Epoch:  877 | step  50 | train loss: 1.4460\n",
      "Epoch:  877 | step  100 | train loss: 1.4846\n",
      "======== Epoch 877 Toal loss 1.4576670716448528 =========\n",
      "Epoch:  878 | step  50 | train loss: 1.5371\n",
      "Epoch:  878 | step  100 | train loss: 1.4358\n",
      "======== Epoch 878 Toal loss 1.459352804393303 =========\n",
      "Epoch:  879 | step  50 | train loss: 1.4704\n",
      "Epoch:  879 | step  100 | train loss: 1.4892\n",
      "======== Epoch 879 Toal loss 1.458129025087124 =========\n",
      "Epoch:  880 | step  50 | train loss: 1.5241\n",
      "Epoch:  880 | step  100 | train loss: 1.4747\n",
      "======== Epoch 880 Toal loss 1.45152761974955 =========\n",
      "Epoch:  881 | step  50 | train loss: 1.4951\n",
      "Epoch:  881 | step  100 | train loss: 1.4507\n",
      "======== Epoch 881 Toal loss 1.4586272055540628 =========\n",
      "Epoch:  882 | step  50 | train loss: 1.4479\n",
      "Epoch:  882 | step  100 | train loss: 1.4929\n",
      "======== Epoch 882 Toal loss 1.4484328709966767 =========\n",
      "Epoch:  883 | step  50 | train loss: 1.5139\n",
      "Epoch:  883 | step  100 | train loss: 1.4698\n",
      "======== Epoch 883 Toal loss 1.44903422564995 =========\n",
      "Epoch:  884 | step  50 | train loss: 1.4585\n",
      "Epoch:  884 | step  100 | train loss: 1.4481\n",
      "======== Epoch 884 Toal loss 1.4490335423771927 =========\n",
      "Epoch:  885 | step  50 | train loss: 1.4467\n",
      "Epoch:  885 | step  100 | train loss: 1.3964\n",
      "======== Epoch 885 Toal loss 1.4483626383106882 =========\n",
      "Epoch:  886 | step  50 | train loss: 1.3212\n",
      "Epoch:  886 | step  100 | train loss: 1.4351\n",
      "======== Epoch 886 Toal loss 1.4454804736424267 =========\n",
      "Epoch:  887 | step  50 | train loss: 1.4657\n",
      "Epoch:  887 | step  100 | train loss: 1.4250\n",
      "======== Epoch 887 Toal loss 1.4423891635445076 =========\n",
      "Epoch:  888 | step  50 | train loss: 1.4919\n",
      "Epoch:  888 | step  100 | train loss: 1.4930\n",
      "======== Epoch 888 Toal loss 1.4411098036339614 =========\n",
      "Epoch:  889 | step  50 | train loss: 1.5411\n",
      "Epoch:  889 | step  100 | train loss: 1.5237\n",
      "======== Epoch 889 Toal loss 1.4384087769965814 =========\n",
      "Epoch:  890 | step  50 | train loss: 1.3715\n",
      "Epoch:  890 | step  100 | train loss: 1.4877\n",
      "======== Epoch 890 Toal loss 1.4368459926388129 =========\n",
      "Epoch:  891 | step  50 | train loss: 1.4867\n",
      "Epoch:  891 | step  100 | train loss: 1.5935\n",
      "======== Epoch 891 Toal loss 1.4311899693031622 =========\n",
      "Epoch:  892 | step  50 | train loss: 1.5890\n",
      "Epoch:  892 | step  100 | train loss: 1.4731\n",
      "======== Epoch 892 Toal loss 1.4403915492499746 =========\n",
      "Epoch:  893 | step  50 | train loss: 1.4141\n",
      "Epoch:  893 | step  100 | train loss: 1.5509\n",
      "======== Epoch 893 Toal loss 1.433078304538882 =========\n",
      "Epoch:  894 | step  50 | train loss: 1.4591\n",
      "Epoch:  894 | step  100 | train loss: 1.5060\n",
      "======== Epoch 894 Toal loss 1.434895813949709 =========\n",
      "Epoch:  895 | step  50 | train loss: 1.4051\n",
      "Epoch:  895 | step  100 | train loss: 1.5384\n",
      "======== Epoch 895 Toal loss 1.4250931516895449 =========\n",
      "Epoch:  896 | step  50 | train loss: 1.3742\n",
      "Epoch:  896 | step  100 | train loss: 1.4352\n",
      "======== Epoch 896 Toal loss 1.4269964452681503 =========\n",
      "Epoch:  897 | step  50 | train loss: 1.4051\n",
      "Epoch:  897 | step  100 | train loss: 1.4970\n",
      "======== Epoch 897 Toal loss 1.4252865421093577 =========\n",
      "Epoch:  898 | step  50 | train loss: 1.5413\n",
      "Epoch:  898 | step  100 | train loss: 1.3929\n",
      "======== Epoch 898 Toal loss 1.4231346041206423 =========\n",
      "Epoch:  899 | step  50 | train loss: 1.3067\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  899 | step  100 | train loss: 1.2629\n",
      "======== Epoch 899 Toal loss 1.427278577796812 =========\n",
      "Epoch:  900 | step  50 | train loss: 1.3545\n",
      "Epoch:  900 | step  100 | train loss: 1.3473\n",
      "======== Epoch 900 Toal loss 1.422812550048518 =========\n",
      "Epoch:  901 | step  50 | train loss: 1.3601\n",
      "Epoch:  901 | step  100 | train loss: 1.3327\n",
      "======== Epoch 901 Toal loss 1.4203404730897609 =========\n",
      "Epoch:  902 | step  50 | train loss: 1.2261\n",
      "Epoch:  902 | step  100 | train loss: 1.5760\n",
      "======== Epoch 902 Toal loss 1.4198409289848515 =========\n",
      "Epoch:  903 | step  50 | train loss: 1.4759\n",
      "Epoch:  903 | step  100 | train loss: 1.4469\n",
      "======== Epoch 903 Toal loss 1.4195898296387215 =========\n",
      "Epoch:  904 | step  50 | train loss: 1.4155\n",
      "Epoch:  904 | step  100 | train loss: 1.4343\n",
      "======== Epoch 904 Toal loss 1.4169295338111194 =========\n",
      "Epoch:  905 | step  50 | train loss: 1.4319\n",
      "Epoch:  905 | step  100 | train loss: 1.3960\n",
      "======== Epoch 905 Toal loss 1.4171117466639698 =========\n",
      "Epoch:  906 | step  50 | train loss: 1.3499\n",
      "Epoch:  906 | step  100 | train loss: 1.4216\n",
      "======== Epoch 906 Toal loss 1.4127525401309253 =========\n",
      "Epoch:  907 | step  50 | train loss: 1.4885\n",
      "Epoch:  907 | step  100 | train loss: 1.4281\n",
      "======== Epoch 907 Toal loss 1.4085750977198284 =========\n",
      "Epoch:  908 | step  50 | train loss: 1.3776\n",
      "Epoch:  908 | step  100 | train loss: 1.4846\n",
      "======== Epoch 908 Toal loss 1.4102519207853612 =========\n",
      "Epoch:  909 | step  50 | train loss: 1.3889\n",
      "Epoch:  909 | step  100 | train loss: 1.4585\n",
      "======== Epoch 909 Toal loss 1.4080185502525266 =========\n",
      "Epoch:  910 | step  50 | train loss: 1.3922\n",
      "Epoch:  910 | step  100 | train loss: 1.3769\n",
      "======== Epoch 910 Toal loss 1.4107226356258238 =========\n",
      "Epoch:  911 | step  50 | train loss: 1.4025\n",
      "Epoch:  911 | step  100 | train loss: 1.3432\n",
      "======== Epoch 911 Toal loss 1.4010955579881745 =========\n",
      "Epoch:  912 | step  50 | train loss: 1.4205\n",
      "Epoch:  912 | step  100 | train loss: 1.4022\n",
      "======== Epoch 912 Toal loss 1.4081847997215706 =========\n",
      "Epoch:  913 | step  50 | train loss: 1.4767\n",
      "Epoch:  913 | step  100 | train loss: 1.3766\n",
      "======== Epoch 913 Toal loss 1.399712171981005 =========\n",
      "Epoch:  914 | step  50 | train loss: 1.4910\n",
      "Epoch:  914 | step  100 | train loss: 1.4520\n",
      "======== Epoch 914 Toal loss 1.3982872691580919 =========\n",
      "Epoch:  915 | step  50 | train loss: 1.5191\n",
      "Epoch:  915 | step  100 | train loss: 1.4602\n",
      "======== Epoch 915 Toal loss 1.4001071239874614 =========\n",
      "Epoch:  916 | step  50 | train loss: 1.3089\n",
      "Epoch:  916 | step  100 | train loss: 1.4500\n",
      "======== Epoch 916 Toal loss 1.3956089465598749 =========\n",
      "Epoch:  917 | step  50 | train loss: 1.4171\n",
      "Epoch:  917 | step  100 | train loss: 1.4513\n",
      "======== Epoch 917 Toal loss 1.3942890700285997 =========\n",
      "Epoch:  918 | step  50 | train loss: 1.3506\n",
      "Epoch:  918 | step  100 | train loss: 1.3710\n",
      "======== Epoch 918 Toal loss 1.390889798722616 =========\n",
      "Epoch:  919 | step  50 | train loss: 1.3710\n",
      "Epoch:  919 | step  100 | train loss: 1.4118\n",
      "======== Epoch 919 Toal loss 1.388001148293658 =========\n",
      "Epoch:  920 | step  50 | train loss: 1.3837\n",
      "Epoch:  920 | step  100 | train loss: 1.4728\n",
      "======== Epoch 920 Toal loss 1.3870487891561616 =========\n",
      "Epoch:  921 | step  50 | train loss: 1.3476\n",
      "Epoch:  921 | step  100 | train loss: 1.2961\n",
      "======== Epoch 921 Toal loss 1.3897586178973438 =========\n",
      "Epoch:  922 | step  50 | train loss: 1.4610\n",
      "Epoch:  922 | step  100 | train loss: 1.3699\n",
      "======== Epoch 922 Toal loss 1.390856690523101 =========\n",
      "Epoch:  923 | step  50 | train loss: 1.3811\n",
      "Epoch:  923 | step  100 | train loss: 1.5026\n",
      "======== Epoch 923 Toal loss 1.390387877216184 =========\n",
      "Epoch:  924 | step  50 | train loss: 1.3549\n",
      "Epoch:  924 | step  100 | train loss: 1.4583\n",
      "======== Epoch 924 Toal loss 1.382226672598986 =========\n",
      "Epoch:  925 | step  50 | train loss: 1.4081\n",
      "Epoch:  925 | step  100 | train loss: 1.3088\n",
      "======== Epoch 925 Toal loss 1.3797485644255227 =========\n",
      "Epoch:  926 | step  50 | train loss: 1.3782\n",
      "Epoch:  926 | step  100 | train loss: 1.4487\n",
      "======== Epoch 926 Toal loss 1.3869952903530463 =========\n",
      "Epoch:  927 | step  50 | train loss: 1.3335\n",
      "Epoch:  927 | step  100 | train loss: 1.4495\n",
      "======== Epoch 927 Toal loss 1.3821937844036072 =========\n",
      "Epoch:  928 | step  50 | train loss: 1.3528\n",
      "Epoch:  928 | step  100 | train loss: 1.3119\n",
      "======== Epoch 928 Toal loss 1.3805806210370568 =========\n",
      "Epoch:  929 | step  50 | train loss: 1.3199\n",
      "Epoch:  929 | step  100 | train loss: 1.3272\n",
      "======== Epoch 929 Toal loss 1.3794024358919965 =========\n",
      "Epoch:  930 | step  50 | train loss: 1.2758\n",
      "Epoch:  930 | step  100 | train loss: 1.3179\n",
      "======== Epoch 930 Toal loss 1.3729262507058742 =========\n",
      "Epoch:  931 | step  50 | train loss: 1.4075\n",
      "Epoch:  931 | step  100 | train loss: 1.3655\n",
      "======== Epoch 931 Toal loss 1.3761627906706275 =========\n",
      "Epoch:  932 | step  50 | train loss: 1.3092\n",
      "Epoch:  932 | step  100 | train loss: 1.3800\n",
      "======== Epoch 932 Toal loss 1.3705921405699195 =========\n",
      "Epoch:  933 | step  50 | train loss: 1.2462\n",
      "Epoch:  933 | step  100 | train loss: 1.5140\n",
      "======== Epoch 933 Toal loss 1.3671689314570854 =========\n",
      "Epoch:  934 | step  50 | train loss: 1.4528\n",
      "Epoch:  934 | step  100 | train loss: 1.3835\n",
      "======== Epoch 934 Toal loss 1.3680604647814743 =========\n",
      "Epoch:  935 | step  50 | train loss: 1.2990\n",
      "Epoch:  935 | step  100 | train loss: 1.2905\n",
      "======== Epoch 935 Toal loss 1.3622714775364573 =========\n",
      "Epoch:  936 | step  50 | train loss: 1.3998\n",
      "Epoch:  936 | step  100 | train loss: 1.4676\n",
      "======== Epoch 936 Toal loss 1.36357151977415 =========\n",
      "Epoch:  937 | step  50 | train loss: 1.2948\n",
      "Epoch:  937 | step  100 | train loss: 1.3499\n",
      "======== Epoch 937 Toal loss 1.3681660716126605 =========\n",
      "Epoch:  938 | step  50 | train loss: 1.5141\n",
      "Epoch:  938 | step  100 | train loss: 1.3902\n",
      "======== Epoch 938 Toal loss 1.3625953042410253 =========\n",
      "Epoch:  939 | step  50 | train loss: 1.4250\n",
      "Epoch:  939 | step  100 | train loss: 1.2915\n",
      "======== Epoch 939 Toal loss 1.364587309883862 =========\n",
      "Epoch:  940 | step  50 | train loss: 1.3807\n",
      "Epoch:  940 | step  100 | train loss: 1.2856\n",
      "======== Epoch 940 Toal loss 1.3596924727525168 =========\n",
      "Epoch:  941 | step  50 | train loss: 1.2947\n",
      "Epoch:  941 | step  100 | train loss: 1.4246\n",
      "======== Epoch 941 Toal loss 1.3546771673652214 =========\n",
      "Epoch:  942 | step  50 | train loss: 1.2188\n",
      "Epoch:  942 | step  100 | train loss: 1.4040\n",
      "======== Epoch 942 Toal loss 1.360508950745187 =========\n",
      "Epoch:  943 | step  50 | train loss: 1.2649\n",
      "Epoch:  943 | step  100 | train loss: 1.3379\n",
      "======== Epoch 943 Toal loss 1.3633353380652946 =========\n",
      "Epoch:  944 | step  50 | train loss: 1.3354\n",
      "Epoch:  944 | step  100 | train loss: 1.3987\n",
      "======== Epoch 944 Toal loss 1.3524582919066515 =========\n",
      "Epoch:  945 | step  50 | train loss: 1.3040\n",
      "Epoch:  945 | step  100 | train loss: 1.3311\n",
      "======== Epoch 945 Toal loss 1.3557854299622822 =========\n",
      "Epoch:  946 | step  50 | train loss: 1.3230\n",
      "Epoch:  946 | step  100 | train loss: 1.3499\n",
      "======== Epoch 946 Toal loss 1.3480701650061258 =========\n",
      "Epoch:  947 | step  50 | train loss: 1.3723\n",
      "Epoch:  947 | step  100 | train loss: 1.4521\n",
      "======== Epoch 947 Toal loss 1.3496020092227594 =========\n",
      "Epoch:  948 | step  50 | train loss: 1.3540\n",
      "Epoch:  948 | step  100 | train loss: 1.4091\n",
      "======== Epoch 948 Toal loss 1.3516432075965694 =========\n",
      "Epoch:  949 | step  50 | train loss: 1.2556\n",
      "Epoch:  949 | step  100 | train loss: 1.5692\n",
      "======== Epoch 949 Toal loss 1.3478474781765202 =========\n",
      "Epoch:  950 | step  50 | train loss: 1.3135\n",
      "Epoch:  950 | step  100 | train loss: 1.3217\n",
      "======== Epoch 950 Toal loss 1.3421881111656748 =========\n",
      "Epoch:  951 | step  50 | train loss: 1.3350\n",
      "Epoch:  951 | step  100 | train loss: 1.3947\n",
      "======== Epoch 951 Toal loss 1.3384534207786 =========\n",
      "Epoch:  952 | step  50 | train loss: 1.3184\n",
      "Epoch:  952 | step  100 | train loss: 1.3673\n",
      "======== Epoch 952 Toal loss 1.3427601665016111 =========\n",
      "Epoch:  953 | step  50 | train loss: 1.2989\n",
      "Epoch:  953 | step  100 | train loss: 1.3529\n",
      "======== Epoch 953 Toal loss 1.3428944533433371 =========\n",
      "Epoch:  954 | step  50 | train loss: 1.3605\n",
      "Epoch:  954 | step  100 | train loss: 1.3333\n",
      "======== Epoch 954 Toal loss 1.341906710368831 =========\n",
      "Epoch:  955 | step  50 | train loss: 1.2229\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  955 | step  100 | train loss: 1.3995\n",
      "======== Epoch 955 Toal loss 1.3390438837733696 =========\n",
      "Epoch:  956 | step  50 | train loss: 1.2805\n",
      "Epoch:  956 | step  100 | train loss: 1.2327\n",
      "======== Epoch 956 Toal loss 1.3357992356385642 =========\n",
      "Epoch:  957 | step  50 | train loss: 1.3044\n",
      "Epoch:  957 | step  100 | train loss: 1.3558\n",
      "======== Epoch 957 Toal loss 1.3373184698383982 =========\n",
      "Epoch:  958 | step  50 | train loss: 1.2769\n",
      "Epoch:  958 | step  100 | train loss: 1.3629\n",
      "======== Epoch 958 Toal loss 1.3295281776567784 =========\n",
      "Epoch:  959 | step  50 | train loss: 1.3247\n",
      "Epoch:  959 | step  100 | train loss: 1.2793\n",
      "======== Epoch 959 Toal loss 1.329088573533345 =========\n",
      "Epoch:  960 | step  50 | train loss: 1.2546\n",
      "Epoch:  960 | step  100 | train loss: 1.3713\n",
      "======== Epoch 960 Toal loss 1.3304234423288486 =========\n",
      "Epoch:  961 | step  50 | train loss: 1.3175\n",
      "Epoch:  961 | step  100 | train loss: 1.4286\n",
      "======== Epoch 961 Toal loss 1.3245381485155927 =========\n",
      "Epoch:  962 | step  50 | train loss: 1.3359\n",
      "Epoch:  962 | step  100 | train loss: 1.2834\n",
      "======== Epoch 962 Toal loss 1.3249792364554676 =========\n",
      "Epoch:  963 | step  50 | train loss: 1.3661\n",
      "Epoch:  963 | step  100 | train loss: 1.3649\n",
      "======== Epoch 963 Toal loss 1.3261423576168898 =========\n",
      "Epoch:  964 | step  50 | train loss: 1.2934\n",
      "Epoch:  964 | step  100 | train loss: 1.3732\n",
      "======== Epoch 964 Toal loss 1.3250256699275196 =========\n",
      "Epoch:  965 | step  50 | train loss: 1.3076\n",
      "Epoch:  965 | step  100 | train loss: 1.2811\n",
      "======== Epoch 965 Toal loss 1.3161456856301161 =========\n",
      "Epoch:  966 | step  50 | train loss: 1.3828\n",
      "Epoch:  966 | step  100 | train loss: 1.3066\n",
      "======== Epoch 966 Toal loss 1.3250676994401265 =========\n",
      "Epoch:  967 | step  50 | train loss: 1.2002\n",
      "Epoch:  967 | step  100 | train loss: 1.2113\n",
      "======== Epoch 967 Toal loss 1.3197019856150558 =========\n",
      "Epoch:  968 | step  50 | train loss: 1.3552\n",
      "Epoch:  968 | step  100 | train loss: 1.2698\n",
      "======== Epoch 968 Toal loss 1.3194922325087757 =========\n",
      "Epoch:  969 | step  50 | train loss: 1.2320\n",
      "Epoch:  969 | step  100 | train loss: 1.2766\n",
      "======== Epoch 969 Toal loss 1.3179416007142726 =========\n",
      "Epoch:  970 | step  50 | train loss: 1.3704\n",
      "Epoch:  970 | step  100 | train loss: 1.4987\n",
      "======== Epoch 970 Toal loss 1.3207226652440016 =========\n",
      "Epoch:  971 | step  50 | train loss: 1.2987\n",
      "Epoch:  971 | step  100 | train loss: 1.3805\n",
      "======== Epoch 971 Toal loss 1.3136920163301917 =========\n",
      "Epoch:  972 | step  50 | train loss: 1.2950\n",
      "Epoch:  972 | step  100 | train loss: 1.3409\n",
      "======== Epoch 972 Toal loss 1.3171276736065625 =========\n",
      "Epoch:  973 | step  50 | train loss: 1.3364\n",
      "Epoch:  973 | step  100 | train loss: 1.3159\n",
      "======== Epoch 973 Toal loss 1.3104970523012363 =========\n",
      "Epoch:  974 | step  50 | train loss: 1.2996\n",
      "Epoch:  974 | step  100 | train loss: 1.4025\n",
      "======== Epoch 974 Toal loss 1.3150294650860919 =========\n",
      "Epoch:  975 | step  50 | train loss: 1.3007\n",
      "Epoch:  975 | step  100 | train loss: 1.1395\n",
      "======== Epoch 975 Toal loss 1.3075282447706393 =========\n",
      "Epoch:  976 | step  50 | train loss: 1.3515\n",
      "Epoch:  976 | step  100 | train loss: 1.2616\n",
      "======== Epoch 976 Toal loss 1.3090987050436376 =========\n",
      "Epoch:  977 | step  50 | train loss: 1.3055\n",
      "Epoch:  977 | step  100 | train loss: 1.3315\n",
      "======== Epoch 977 Toal loss 1.3054773003105227 =========\n",
      "Epoch:  978 | step  50 | train loss: 1.2658\n",
      "Epoch:  978 | step  100 | train loss: 1.3616\n",
      "======== Epoch 978 Toal loss 1.2978212348813933 =========\n",
      "Epoch:  979 | step  50 | train loss: 1.3343\n",
      "Epoch:  979 | step  100 | train loss: 1.4342\n",
      "======== Epoch 979 Toal loss 1.308388886412954 =========\n",
      "Epoch:  980 | step  50 | train loss: 1.4279\n",
      "Epoch:  980 | step  100 | train loss: 1.2996\n",
      "======== Epoch 980 Toal loss 1.303541692291818 =========\n",
      "Epoch:  981 | step  50 | train loss: 1.3188\n",
      "Epoch:  981 | step  100 | train loss: 1.3149\n",
      "======== Epoch 981 Toal loss 1.2980462370849237 =========\n",
      "Epoch:  982 | step  50 | train loss: 1.2702\n",
      "Epoch:  982 | step  100 | train loss: 1.2865\n",
      "======== Epoch 982 Toal loss 1.303055885361462 =========\n",
      "Epoch:  983 | step  50 | train loss: 1.4573\n",
      "Epoch:  983 | step  100 | train loss: 1.3116\n",
      "======== Epoch 983 Toal loss 1.2990593629154732 =========\n",
      "Epoch:  984 | step  50 | train loss: 1.2374\n",
      "Epoch:  984 | step  100 | train loss: 1.3063\n",
      "======== Epoch 984 Toal loss 1.2965516530401338 =========\n",
      "Epoch:  985 | step  50 | train loss: 1.4502\n",
      "Epoch:  985 | step  100 | train loss: 1.2912\n",
      "======== Epoch 985 Toal loss 1.2894078667570905 =========\n",
      "Epoch:  986 | step  50 | train loss: 1.3279\n",
      "Epoch:  986 | step  100 | train loss: 1.2875\n",
      "======== Epoch 986 Toal loss 1.2914280387444226 =========\n",
      "Epoch:  987 | step  50 | train loss: 1.2619\n",
      "Epoch:  987 | step  100 | train loss: 1.3160\n",
      "======== Epoch 987 Toal loss 1.2915015753691759 =========\n",
      "Epoch:  988 | step  50 | train loss: 1.2279\n",
      "Epoch:  988 | step  100 | train loss: 1.3043\n",
      "======== Epoch 988 Toal loss 1.2896698994365166 =========\n",
      "Epoch:  989 | step  50 | train loss: 1.2265\n",
      "Epoch:  989 | step  100 | train loss: 1.4286\n",
      "======== Epoch 989 Toal loss 1.2881950895960739 =========\n",
      "Epoch:  990 | step  50 | train loss: 1.3168\n",
      "Epoch:  990 | step  100 | train loss: 1.3034\n",
      "======== Epoch 990 Toal loss 1.288633979432951 =========\n",
      "Epoch:  991 | step  50 | train loss: 1.3292\n",
      "Epoch:  991 | step  100 | train loss: 1.3413\n",
      "======== Epoch 991 Toal loss 1.289411622334302 =========\n",
      "Epoch:  992 | step  50 | train loss: 1.2953\n",
      "Epoch:  992 | step  100 | train loss: 1.2217\n",
      "======== Epoch 992 Toal loss 1.2860385101985157 =========\n",
      "Epoch:  993 | step  50 | train loss: 1.3236\n",
      "Epoch:  993 | step  100 | train loss: 1.3123\n",
      "======== Epoch 993 Toal loss 1.2893942846515314 =========\n",
      "Epoch:  994 | step  50 | train loss: 1.2704\n",
      "Epoch:  994 | step  100 | train loss: 1.3598\n",
      "======== Epoch 994 Toal loss 1.2838548974293034 =========\n",
      "Epoch:  995 | step  50 | train loss: 1.1916\n",
      "Epoch:  995 | step  100 | train loss: 1.2833\n",
      "======== Epoch 995 Toal loss 1.282233503775868 =========\n",
      "Epoch:  996 | step  50 | train loss: 1.2508\n",
      "Epoch:  996 | step  100 | train loss: 1.2863\n",
      "======== Epoch 996 Toal loss 1.2812940308718177 =========\n",
      "Epoch:  997 | step  50 | train loss: 1.2310\n",
      "Epoch:  997 | step  100 | train loss: 1.3420\n",
      "======== Epoch 997 Toal loss 1.2719432706755351 =========\n",
      "Epoch:  998 | step  50 | train loss: 1.3458\n",
      "Epoch:  998 | step  100 | train loss: 1.2635\n",
      "======== Epoch 998 Toal loss 1.2798900924077847 =========\n",
      "Epoch:  999 | step  50 | train loss: 1.3422\n",
      "Epoch:  999 | step  100 | train loss: 1.2736\n",
      "======== Epoch 999 Toal loss 1.2747220091703462 =========\n"
     ]
    }
   ],
   "source": [
    "model.train()\n",
    "l_h = []\n",
    "epoch_loss = 0\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(),lr=config[\"LR\"])\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "\n",
    "for epoch in range(config['epoch']):\n",
    "    epoch_loss = 0\n",
    "    count = 0\n",
    "    \n",
    "    for step,d in enumerate(data_loader):\n",
    "        count+=1\n",
    "        optimizer.zero_grad()\n",
    "        d = d.long().transpose(1, 0).contiguous() # torch.Size([24, 32])\n",
    "\n",
    "\n",
    "        input_,target = d[:-1,:].to(device),d[1:,:].to(device)    # input  0   torch.Size([23, 32])\n",
    "                                            # output    torch.Size([23, 32])\n",
    "        hidden = model.init_hidden(input_.size(1)).to(device)\n",
    "        \n",
    "        \n",
    "        \n",
    "        out,hidden = model(input_,hidden)\n",
    "\n",
    "        loss = criterion(out,target.view(-1))\n",
    "        \n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm_(model.parameters(),1)\n",
    "        optimizer.step()\n",
    "        l_h.append(loss)\n",
    "        epoch_loss+=loss.item()\n",
    "        \n",
    "        if (step+1) % 50 == 0:\n",
    "                            \n",
    "            print('Epoch: ', epoch, '| step ',step+1,'| train loss: %.4f' % loss.item())\n",
    "    \n",
    "    print(\"======== Epoch {} Toal loss {} =========\".format(epoch,epoch_loss/count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXwAAAEKCAYAAAARnO4WAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvOIA7rQAAIABJREFUeJzt3Xl8FdX9//HXJwsJOwJhX8KiIIsiBAFRFFEQsH5dqmJbrdaKVWurtdooLq1KwaVWW/trpa1WW7W41xoVERHEUlbZlFUIEGRJQLZAyHZ+f9whDZg9d+76fj4e95G5c2fmfM6d8OHkzMw55pxDRERiX0K4AxARkdBQwhcRiRNK+CIicUIJX0QkTijhi4jECSV8EZE44WvCN7PbzexzM1tlZi+bWaqf5YmISOV8S/hm1hH4CZDhnOsHJAIT/CpPRESq5neXThLQ0MySgEbAVz6XJyIilUjy68DOuW1m9jiwBTgMfOCc++D47cxsIjARoHHjxoN69+5d67K2fn2IpilJtGjUoJ5Ri4hElyVLluQ559Jqsq35NbSCmZ0AvA5cCewFXgVec879o7J9MjIy3OLFi2td1s9eWcaw7q24PKNzXcMVEYlKZrbEOZdRk2397NI5D9jknMt1zhUBbwBn+FFQohmlGhNIRKRKfib8LcBQM2tkZgaMAlb7UVBiglGqfC8iUiXfEr5zbgHwGrAUWOmVNc2PssyMEmV8EZEq+XbRFsA59wDwgJ9lACQmoC4dEQGgqKiInJwcCgoKwh1KUKWmptKpUyeSk5PrfAxfE36oJJhRqha+iAA5OTk0bdqU9PR0Ar3J0c85x+7du8nJyaFbt251Pk5MDK2QYIbSvYgAFBQU0KpVq5hJ9hDotm7VqlW9/2qJiYQP6KKtiJSJpWR/VDDqFBMJ3yzwJ4+IiFQuNhI+sfe/uYhEryZNmoQ7hArFRMJPMFADX0SkajGR8M10W6aIRB7nHHfeeSf9+vWjf//+TJ8+HYDt27czYsQIBgwYQL9+/fjkk08oKSnh2muvLdv2t7/9bdDjiYnbMk136YhIBHrjjTdYtmwZy5cvJy8vj8GDBzNixAheeuklxowZw6RJkygpKeHQoUMsW7aMbdu2sWrVKgD27t0b9HhiI+GjLh0RqVh6ZlbQj5k9dXyNtps3bx5XXXUViYmJtG3blrPPPptFixYxePBgfvCDH1BUVMTFF1/MgAED6N69Oxs3buTWW29l/PjxjB49Ouhxx0TCx8CpjS8iFahpcvZDZXcPjhgxgrlz55KVlcXVV1/NnXfeyTXXXMPy5cuZMWMGf/jDH3jllVd49tlngxpPTPThJ5iphS8iEWfEiBFMnz6dkpIScnNzmTt3LqeffjqbN2+mTZs23HDDDVx//fUsXbqUvLw8SktLueyyy3jooYdYunRp0OOJiRZ+oEtHGV9EIssll1zC/PnzOfXUUzEzHn30Udq1a8fzzz/PY489RnJyMk2aNOGFF15g27ZtXHfddZSWlgIwZcqUoMfj2wQodVHXCVAem7GG1KREbh11og9RiUg0Wb16NSeffHK4w/BFRXWLlAlQQsbQXToiItWJiYSvB69ERKoXEwkfTXEoIuVEUld1sASjTjGR8A3UpSMiQGCikN27d8dU0j86Hn5qamq9jhMbd+kYOI2PLCJAp06dyMnJITc3N9yhBNXRGa/qw7eEb2a9gOnlVnUH7nfOPRn0sjA9eCUiACQnJ9drVqhY5lvCd86tBQYAmFkisA1404+ydNFWRKR6oerDHwV86Zzb7MfBNVqmiEj1QpXwJwAv+3VwjZYpIlI93xO+mTUALgJereTziWa22MwW1+ciixr4IiJVC0ULfyyw1Dm3s6IPnXPTnHMZzrmMtLS0OhVgGi1TRKRaoUj4V+Fjdw4ERstUvhcRqZqvCd/MGgHnA2/4Wg66aCsiUh1fH7xyzh0CWvlZBnhdOsr3IiJVipGhFXSXjohIdWIj4auFLyJSrRhJ+BotU0SkOrGR8MMdgIhIFIiNhG+xOf61iEgwxUbCR7fhi4hUJyYSfkKC6aKtiEg1YiLh68ErEZHqxUTCP1JcSs7Xh8MdhohIRIuJKQ4ffX8thSWl4Q5DRCSixUQLX8leRKR6MZHwjyosVuIXEalMTCX8klJduBURqUxMJfy8g0fCHYKISMSKqYS/c39BuEMQEYlYMZXwX1ywJdwhiIhErJhK+Mu37g13CCIiESsmEv79F/YBIFd9+CIilYqJhD+ke0sADhQUhzkSEZHI5fck5i3M7DUzW2Nmq81smB/ldGje0I/DiojEFL+HVngKeN85920zawA08qOQExo38OOwIiIxxbeEb2bNgBHAtQDOuUKg0K/yRESkan526XQHcoHnzOwzM/uLmTU+fiMzm2hmi81scW5uro/hiIjENz8TfhIwEPijc+40IB/IPH4j59w051yGcy4jLS3Nx3BEROKbnwk/B8hxzi3w3r9G4D8AEREJA98SvnNuB7DVzHp5q0YBX/hVnoiIVM3vu3RuBV707tDZCFznc3nsOlBAm6apfhcjIhJ1fL0P3zm3zOufP8U5d7Fz7ms/ywM465HZfhchIhKVYuJJ2/KOFJeSnpkV7jBERCJOzCV8ERGpWMwm/F0aG19E5Bgxm/BP//Us9h0uCncYIiIRI2YTPsDvZq0PdwgiIhEjphP+Mk2IIiJSJqYT/pLNX5OemUX+EY2TLyISMwn/lpE9Kv1s1wHNhCUiEjMJf1z/9pV+NvLxj0MXiIhIhIqZhN+8YXKVn89dl4tzLkTRiIhEnphJ+B1bVD3N4TXPLmTdzoMhikZEJPLETMI3M/q0b1blNmOenEt6Zhab8vJDFJWISOSImYQPMLJ3GmbVb3f5n+b7H4yISISJqYT/89G9+HLyuGq3yzuou3ZEJP7EVMI3MxISjFM7Na922/9syAtBRCIikSOmEv5Rz1ydUe02N/59CV/nF4YgGhGRyBCTCb9d81Rm3XF2ldscOFLMaQ/NZPaaXSGKSkQkvGIy4QP0SGtSo+3mb9ztcyQiIpHB14RvZtlmttLMlpnZYj/LqkiDpOqr9+/lX3HOY5oWUURiXyha+COdcwOcc9V3rAfZZ/edX+022/cVkL37UAiiEREJr5jt0gFonJLE8vtHM/mSftVum56ZxUGNqikiMczvhO+AD8xsiZlNrGgDM5toZovNbHFubm7QA2jeKJnvDulao237PTAj6OWLiEQKvxP+cOfcQGAscIuZjTh+A+fcNOdchnMuIy0tzedwqpeemcU9b64MdxgiIkHna8J3zn3l/dwFvAmc7md5VVn78AU13valBVtIz8yi+91ZLNm8h+KSUh8jExEJDd8Svpk1NrOmR5eB0cAqv8qrTkpSIqsfrHnSByh1cNkf5/Puqh0+RSUiEjp+tvDbAvPMbDmwEMhyzr3vY3nVatggkRPb1Oz+/PJ27S/wIRoRkdDyLeE75zY65071Xn2dc5P9Kqs2plzav9b7PJy1mlxNkygiUS6mb8usSEZ6Sz6qZtiFigye/CHf+v08tfZFJGrFXcIH6J7WhDUP1a4/H2Dltn2c/utZPkQkIuK/uEz4AKnJiayfPLZO+/7nSw2tLCLRJ24TPkByYt2q/50/LyA9M4vC4lJKSjUxuohEh7hO+AA90hrXed+T7n2PHve8G8RoRET8E/cJf9Yd5/DKjcPqdYz0zCze/CwnSBGJiPgj7hM+wOndWrL8gdH1Osbt05fz2pIc9mgWLRGJUEr4nuYNk5lx2zeG+qmVn7+6nIEPzWTf4aIgRSUiEjxK+OX0ateUT+4aWe/jnPqrD7j/X6twLnBB972V29mw60C9jysiUh9K+Mfp3LIRf76m/nO1vDB/Mx+vzWXG5zu46cWlPPjO6iBEJyJSd0nhDiASnd+nLf+cOJQJ0/5br+Nc97dFZctHW/siIuGiFn4lhnZvFdTjfbI+j015+RQWl2q4ZREJC4uklmdGRoZbvDjkc51Xauf+AopLHcOnfhTU457fpy2Txp1Ml5aNcEBiggX1+CISP8xsSU3nDK9RwjezHkCOc+6ImZ0DnAK84JzbW69IjxNpCf+o7Lx80ls3ZvjUj9i293DQj5+caKyfPC7oxxWR2FebhF/TLp3XgRIz6wn8FegGvFTH+KJOeuvA07hz7xrJgM4tgn78ohLHhl0Hg35cEZHyaprwS51zxcAlwJPOuduB9v6FFZkSE4y3bhlep0lUqnPeE3OCfkwRkfJqmvCLzOwq4PvAO966ZH9CinwzbhvBKZ2aB/2489bnceUz81m65WvOfCS41w1ERGqa8K8DhgGTnXObzKwb8A//wopsCQnG2z8+kzvOPymox/3eXxewYNMe3l72FTlfH+aqet4WKiJSXq3v0jGzE4DOzrkVNdw+EVgMbHPOXVjVtpF60bY66ZlZvh3781+NoXGKHpcQkYoF/aKtmX1sZs3MrCWwHHjOzJ6oYTw/BfSYaR31fWAGZz7yEZ9/tS/coYhIlKtpl05z59x+4FLgOefcIOC86nYys07AeOAvdQ8x8mVPHc+8X4zk2jPSfTl+zteHeeKDdfxr2TZO/dUHHCos9qUcEYltNb0PfyUwGngemOScW2RmK5xzp1Sz32vAFKAp8POKunTMbCIwEaBLly6DNm/eXPtaRJDC4lIWb97Ds/Oy+XD1Tt/KWXDPKNo2S2X7vsO0apxCgyQ9NC0Sj2rTpVPTzuEHgRnAp16y7w6sryaIC4Fdzrkl3sNaFXLOTQOmQaAPv4bxRKwGSQmc0aM1w7q3orCklN98sI5pczcGvZwhx02mnj11fNDLEJHYUqNmoXPuVefcKc65m7z3G51zl1Wz23DgIjPLBv4JnGtmcXNnj5mRkpTIPeNODkl5zjl27CsISVkiEp1qetG2k5m9aWa7zGynmb3u9c9Xyjl3t3Ouk3MuHZgAfOSc+14QYpYK9Lr3fYZOmcWMz3eEOxQRiVA17fh9Dngb6AB0BP7trZMaeOPmM5j3i5H8/qrTAHx5aKvQG4Hzxr8vIT0zi32HArNu7dhXwOHCkqCXJyLRp6YXbZc55wZUt66+ovU+/Lq46Ol5rMjx91bLu8f2Zsp7a/j2oE48fvmpvpYlIuHhx+BpeWb2PTNL9F7fA3bXPUR5+8dn+l7GlPfWAPDakhze/CwHgF0HCsjOy/e9bBGJPDVt4XcBniYwvIID/gP8xDm3JZjBxFMLv7y9hwoZ8OBM38vJnjqeMb+dy9qdB3hqwgCOFJdyRUZn38sVEf8EvYXvnNvinLvIOZfmnGvjnLuYwENYEgQtGjXgrVuG+17Oypx9rN0ZmEz9p/9cxl2v1Wh0DBGJEXWe8crMtjjnugQzmHht4R+Vf6SYEuc45ZcfhKzM64anc2bP1ow6uW3IyhSR4PGjD7/Ccuqxr1SgcUoSzVKTuWxglXe8BtVzn2Zz/fOLfR0ATkQiQ32GYYz6p2Ij1UMX9+WO0SfRoUVD3l25nZtfXBqScg8UFNE0NW6nORCJeVV26ZjZASpO7AY0dM4FddzeeO/SqczTH63n8Q/Whay8V380jCc+WMeR4hIuG9SJDs0bMrJ3m5CVLyI1F7SxdJxzTYMTktTHLSN7smHXQd5a9lVIyrv8T/PLlpduCcxT/8bNZ9C8YTJJCUbXVo1DEoeIBJeGWIwCZsaTE05jkjcuz0s3DAl5DJf+v/8w6jdzuPD38wAoLimltFS9eiLRRAk/ipR43W9n9GgNwAPf6hPyGA4UFOOco+ek9+h+z7s8+WHouppEpH6U8KNIarkx7+feOZLrhndj/eSxIY+j293vli0/+eF6Zn6xk3Memx3yOESkdjRZahT5zpCuDPNa911aNQIgOTGB7Knjyfn6EGc+Ep6k++ZnOWTvPsTWPYfYk1/Iipy9XD0sPSyxiEjl6vzglR90l0793D59GZcO7Mj1f1tcNnpmuJx1YmumXnYKHVs0DGscIrEuVA9eSYT57ZUDOOvENObeNZLvDQ3qQ9C19sn6PIZP/eiYdemZWSzK3hOmiERECT8GtWueyoMX9WPtwxeEOxQeeucL0jOzWLgpkOjX7zwY5ohE4pcSfoxKSAhMsTiseysA3v6x/4OzVeSv8zYBcMUzgXv7t+w5xO6DR8ISi0i8Ux9+jDtcWEJRaSnNUpNZsnkPB4+U8P1nF4Y7LH52/kn8ZNSJrN1xgAff+ZwXfzg03CGJRKXa9OEr4ceh7fsOk3+kmJ+9stz3WbeqkphglHgPb7158xmc1uWEsMUiEq0iIuGbWSowF0ghcPvna865B6raRwk/tEI18UpNXXhKe9btPMDjl59Kk5Qkuqc1CXdIIhEvUhK+AY2dcwfNLBmYB/zUOfffyvZRwg+9P8zeQEFRCVdkdOasRyPr4ansqePDHYJIxIuI2zJdwNFbMpK9V+T0HwkQGJjtjtG96NiiIY9c1h+AqZf2D3NUAemZWdzwQtUNgFXb9vGrf38OwI59BazdcSAUoYlEJV/v0vEmPF8G7AJmOucWVLDNRDNbbGaLc3Nz/QxHqpCQYFw5uAu3jOzBmL7tmHxJv3CHBMDML3aSnpnFW59tKxusbdvew1z7XODC86uLt/Lcp9kA/PCFRYx5cm64QhWJeL4OreCcKwEGmFkL4E0z6+ecW3XcNtOAaRDo0vEzHqnenWN6A/DdIV3Zsa+ArXsOsSH3IKu27Q9rXLdNX8Zt05cds+6NpTnMWRdoJBQWl7L7YCEAi7L3MDi9ZchjFIl0IbtLx8weAPKdc49Xto368CPXnHW5EXE7Z2VOatuEdd5DXWlNU1g06bwwRyQSGhHRh29maV7LHjNrCJwHrPGrPPHX2SellS3fPbZ3GCOp2LpyT/DmHTzCrNU7wxiNSGTy8y6dU4DngUQC/7G84px7sKp91MKPbCWljvdWbWd8//Y4B8WljkEPz+RAQXG4Q6vQVad35r4L+7Br/xHSW2uWLolNEXFbZl0o4UefQ4XFTJj237A+wFUTn2aeS/tmqSQkGAVFJew/XESbZqnhDkuk3pTwJaQOFRbT5/4ZjO3XjicnDKDXve+HO6Qa+dVFfXl9aQ5TLu1P3w7Nwx2OSJ1ERB++xI8GiYFfoz9+bxApSYllD0ylJifQvGFyOEOr0gNvf86KnH2M/908rv/bonCHI+I7JXyptyRv1q3yrhnWleuGd2P5A6PL1iUmWKhDq7FZa3aRnpnFlPdWk56Zxc79BeEOSSTo1KUjvsvOy6dlkwY0S01mx74Chk6ZFe6QamTTlHF8tnUvAzq14M3PtnHZoE5s3XOIzi0bhTs0kTK16dLRnLbiu/J3yLRrHj0XSstP1g5wWpcWnPubObxz65n066g+f4k+6tKRkPvhmd3CHUKdnPubOQA8+eE6VuTsLVt/8EjgttT3V+3Q5C4S0dTCl5A7qW3TsuXsqeP5/KvAhdP3fnoWY5/6JIyR1cyHq3fx4epdALxz65lc+Pt5ZZ/dOKI7Q3u0Yt76PO67sE+4QhSpkFr4EnKXZ3TiqQkDyt737dCc7KnjObl9s7ILu/+4fki4wquV8ske4Jm5G7nvrVX8dd4mrnxmPnnlWvw3/n0x2Xn5oQ5RpIwu2kpYFBSV8K9l27hycJdj1u/JLyTBoEWjBmXrZq/dxXXPRe9tk0vvOx8DTnsoMNnM1Ev7c8nAjqQkJYY3MIkJumgrES81OfEbyR6gZeMG31h3Vs/WAKx56AJ+O3Md76zYzra9h32PMVgGPjSTVuXqlfnGSrq0bMQZXr1EQkVdOhLxkhITWPfwWFKTE7l73Mn06dAs3CHV2u78wmPef+cvC0jPzOLDLwKDvO3JL6SgqCQcoUkcUZeORJ38I8XkFxbTqnEKq7bt475/rWJFzj6+M6QLLy3YEu7w6mXNQxeQkpSAmfHEzHVckdGJohLHqm37+NapHcIdnkQgjaUjccU5R1GJo0FSAumZWQD89+5RXPvcQtZE+ZSHHVs0LOu+WjhpFGlNUghMFx0YvTTBKHsv8UkJX+LWtLlfkmDGD8/qDsCBgiL6//KDMEcVPKnJCRQUlfLKjcO44pn53DmmF7eM7BnusCSMNHiaxK2JI3qUJXuApqnJrHnoAgBev2kYF0V5t0hBUSkAVzwzH4CsFdtxzvF1fiHLtu6talcRtfAlPl309Dz6dmjGywu3cnp6SxZm7wl3SHX21i3D+eHzi8jz5vRdcu95tGqSEuaoJFTUpSNSQx98voMh3Vsx9b3VbNlziE837A53SEHx2LdPoX+n5vzx4y95asJp4Q5HfKSEL1JHRy/6xpofj+zJlYM707llIw4UFNE09dh5CvYeKmTAgzO/Mcy1RL6IePDKzDoDLwDtgFJgmnPuKb/KEwmG07u1ZHH2Hkojpx0UFE/P3sDTszeUvZ8+cSiHi0rokdaEFo2S2boneh5kk7rzcxLz9kB759xSM2sKLAEuds59Udk+auFLpNiTX0iLhsnMWZ/Ldc8t4tUfDePyP80Pd1i+S0lKYMRJaXRs0ZDmDZO5/fyTAJizLpe+HZrRWtcGIk5EtPCdc9uB7d7yATNbDXQEKk34IpHi6BAPjZID490MTm/J364bTLfWjSkpdZz7mznH3CN/y8ge/GH2l2GLN1iOFJcy03v6F+CcXmkszv6aye+uZmj3ltx+3kkM6d4qjBFKfYSkD9/M0oG5QD/n3P7jPpsITATo0qXLoM2bN/sej0hNOef4Mjefnm2aHLN+x74C2jRNYfDkD9mdX0j21PGszNnHt56eV8mRYs+TVw7g4tM6hjuMuBdRF23NrAkwB5jsnHujqm3VpSPRZtvew5SWumOmPczOy+ecxz/m4Yv7ce9bq8IYnb96pDWmf8fmXHxaR659bhHLHxhN7oEjdG3ViOTEBHbsK6BtsxQ9CeyziEn4ZpYMvAPMcM49Ud32SvgSa85+bDaXDezE1UO7lg2PHKu+O6QLL3pjGZ3cvhmrt+/niStO5dKBnfi/p+dxy8iePDVrPVk/OSvMkcaWiEj4Fvhv/Xlgj3Putprso4QvseyZOV9SXOoY2r0Vz8z5kg/K9ZXHk3UPjyU50dTyD5JISfhnAp8AKwnclglwj3Pu3cr2UcKXeDLlvdU8M2djzHf9VObiAR3o17E53x3SlYYNjp0M5m+fbuKX//6C9ZPHkpx47AgwW/cc4khx6Teuq8SriEj4daGEL/Hq6ANfbZqm8KuL+nLTi0vDHFF4vHTDEDo0b8g5j398zPpNU8aV/UUw5NcfsnP/ET0k5tHgaSJR5pO7RgLQq11TxvZvz6Yp49g0ZRzZU8dz1on/mxnr/hifGP07f17wjWQP0O3ud9l3qAiAnfsD8wSnZ2axdc+hUIYX9dTCF4kQW3YfolnDpGPm8wXYdaCA0yfPYsPksSQlJlBQVELv+94HIHvq+JgdDqKm7rqgF1cP7UrT1GTGPfUJZ57YmnvGnRzusEImIh68EpHa6dKqUYXr2zRNPab7ItV7GGxkr7RjtvvFBb1ZuGk3s9fm+hdkBHr0/bU8+v7asvdfbN/PnLW5vHbTsLIxgzIensm0azIY2OWEcIUZEdTCF4lCZ0yZxTVnpPOjs3uwcNMe0ls3ok3T1GO26XHPu5R4gwJ1a92YTXn54Qg1onx0x9ls3nOI5/+TTXqrxtx/YR8SEiq/W6i4pJRlW/eSkd4yhFHWji7aiggAK3P28cX2fYzs3YbTJ8+ib4dmnHlia56ZszHcoUWMpqlJDOp6Ag9f3I9OJzTicGEJJ9//PqN6t+HyjE786B9LI/oCsS7aiggA/Ts158rBXcpa/+2bN+SuMb3LPn/9pmHf6BqKNwcKivl4bS5nPjKb9MwsTr4/cH1k1ppd/Ogf/7tb6umP1vN1fiG7Dx7hyQ/XHXOMLbsPse9wUUjjrgu18EXixK79BTROSaJxShLpmVl0bNGQTzPPLfu8tNTx6pKt7D1UxJT31hyz70d3nM25v5kT6pAj3oJ7RpGSlMCAB//3FPUnd42kdZMUDhUWY2ZlA/H5RV06IlKlO19dTtdWjfjxuSd+47PSUsfnX+3nW0/P4793j+IPszdw34V9aJCUwH1vreLv/9UAh7Vx6cCOPHHFAFZt20f3tMY0ahDce2WU8EWk3pZs3sOgrpVfrMw9cISzH5vNyl+O4XBRCZ9uyOPGvy8JYYTR46ZzevDHjwPDZ//5mgzOO7kN//lyN80bJtOzTZOyO6/qQglfRMLi7eVf8ZOXP+P/fXcgN8fp08J1VdcLw7poKyJhcdGpHcieOp5x/duz/IHRbPz1ON6/7SxuPbdn2TaPXNYfgG8P6sTCSaPCFWpcUsIXEV80b5hMQoLRu10z7hjdi6cmDADg24M6A/CD4d1o0zSV9ZPH0rtdU/5yTUbZNgALJ41iTN+2YYk9VqlLR0Qiyv6CIgoKS2jTLHAr6aHCYvrcPwOAds1S2bG/gDvOP4nfzFxX1WGijrp0RCTuNEtNLkv2AI0aJJUlwzN6tiJ76nhuHXUiK345+hv7Zo7tzTXDujL75+eEKtyoorF0RCQq/OuW4XQpN5Vks9RkWjdJIe9gYPTM41vI2VPH83V+YdlMY89em8G89bvp2aYJ0xdvZfnWvaELPkKoS0dEotrO/QXsPlhInw7N6rT/tLlf8ut311S/oc/UpSMiUo22zVLrnOwBbjirO6sfvICBXVqQPXU82VPHc+/4Y4dXfvGHQ8joGv0jbapLR0TimpnRsEEib9w8vGzdOb3SeDhrNYvvPY/WTVIAaJKSxM0vLuXSgR0Z07cde/IL+e2H63jlxmG8s+Irbp++PFxVqDE/57R9FrgQ2OWc61eTfdSlIyLRyDlH7sEjZOcdomebJhhwQuMG7NxfwFOz1jNhcGdaN0nhjKkfHbNf55YN2brnMA0SE1g3eWydyo6IJ23NbARwEHhBCV9EJGDn/gJSkxIpKC6hbbNU0jOz+PUl/fnOkC51Ol5EzHjlnJtrZul+HV9EJBq19W45bU5gNq5QjrWvi7YiInEi7AnfzCaa2WIzW5ybG19zcYqIhFLYE75zbppzLsM5l5GWFt8z74iI+CnsCV9ERELDt4RvZi8D84FeZpZjZtf7VZaIiFTPz7t0rvLr2CIiUnvq0hERiRNK+CIicUIanuFBAAAIC0lEQVQJX0QkTijhi4jECSV8EZE4oYQvIhInlPBFROKEEr6ISJxQwhcRiRNK+CIicUIJX0QkTijhi4jECSV8EZE4oYQvIhInlPBFROKEEr6ISJxQwhcRiRNK+CIicUIJX0QkTvia8M3sAjNba2YbzCzTz7JERKRqviV8M0sE/gCMBfoAV5lZH7/KExGRqvnZwj8d2OCc2+icKwT+Cfyfj+WJiEgVknw8dkdga7n3OcCQ4zcys4nARO/tQTNbW8fyWgN5ddw3UsRCHSA26hELdYDYqEcs1AH8q0fXmm7oZ8K3Cta5b6xwbhowrd6FmS12zmXU9zjhFAt1gNioRyzUAWKjHrFQB4iMevjZpZMDdC73vhPwlY/liYhIFfxM+IuAE82sm5k1ACYAb/tYnoiIVMG3Lh3nXLGZ/RiYASQCzzrnPverPILQLRQBYqEOEBv1iIU6QGzUIxbqABFQD3PuG93qIiISg/SkrYhInFDCFxGJE1Gf8CNt+AYz62xms81stZl9bmY/9da3NLOZZrbe+3mCt97M7Hde/CvMbGC5Y33f2369mX2/3PpBZrbS2+d3ZlbRLbDBqk+imX1mZu9477uZ2QIvpuneBXnMLMV7v8H7PL3cMe721q81szHl1vt+7syshZm9ZmZrvHMyLBrPhZnd7v0+rTKzl80sNRrOhZk9a2a7zGxVuXW+f/+VlRHEOjzm/U6tMLM3zaxFuc9q9R3X5TzWmXMual8ELgZ/CXQHGgDLgT5hjqk9MNBbbgqsIzC0xKNAprc+E3jEWx4HvEfguYWhwAJvfUtgo/fzBG/5BO+zhcAwb5/3gLE+1udnwEvAO977V4AJ3vKfgJu85ZuBP3nLE4Dp3nIf77ykAN2885UYqnMHPA/80FtuALSItnNB4CHGTUDDcufg2mg4F8AIYCCwqtw637//ysoIYh1GA0ne8iPl6lDr77i257Fe5yPYv5yhfHkneka593cDd4c7ruNi/BdwPrAWaO+taw+s9ZafAa4qt/1a7/OrgGfKrX/GW9ceWFNu/THbBTn2TsAs4FzgHe8fVV65X/Sy75/A3VjDvOUkbzs7/pwc3S4U5w5oRiBR2nHro+pc8L+n1lt63+07wJhoORdAOscmS9+//8rKCFYdjvvsEuDFir676r7juvybqk89or1Lp6LhGzqGKZZv8P4EOw1YALR1zm0H8H628TarrA5Vrc+pYL0fngTuAkq9962Avc654grKLovX+3yft31t6xdM3YFc4DkLdEv9xcwaE2Xnwjm3DXgc2AJsJ/DdLiG6zkV5ofj+KyvDDz8g8NcF1cRa0fq6/Juqs2hP+DUaviEczKwJ8Dpwm3Nuf1WbVrDO1WF9UJnZhcAu59yS8qurKDsS65FE4E/xPzrnTgPyCfx5X5lIrANe//P/Eegi6AA0JjAKbWVlR2Q9aiDq4jazSUAx8OLRVRVsVtc6BL1+0Z7wI3L4BjNLJpDsX3TOveGt3mlm7b3P2wO7vPWV1aGq9Z0qWB9sw4GLzCybwEin5xJo8bcws6MP7JUvuyxe7/PmwJ5q6uH3ucsBcpxzC7z3rxH4DyDazsV5wCbnXK5zrgh4AziD6DoX5YXi+6+sjKDxLh5fCHzXef0udahDHrU/j3UX7P7GUL4ItOA2Emj5HL0Q0jfMMRnwAvDkcesf49iLSI96y+M59kLVQm99SwL9zyd4r01AS++zRd62Ry9UjfO5Tufwv4u2r3LsBaabveVbOPYC0yvecl+OvYi1kcAFrJCcO+AToJe3/EvvPETVuSAwyuznQCOvnOeBW6PlXPDNPnzfv//KyghiHS4AvgDSjtuu1t9xbc9jveoR7F/OUL8IXNlfR+AK+KQIiOdMAn92rQCWea9xBPreZgHrvZ9Hf2GNwEQxXwIrgYxyx/oBsMF7XVdufQawytvnaep5IacGdTqH/yX87gTujNjg/aKmeOtTvfcbvM+7l9t/khfrWsrdxRKKcwcMABZ75+MtL2FE3bkAfgWs8cr6u5dQIv5cAC8TuO5QRKDFen0ovv/KyghiHTYQ6F8/+m/8T3X9jutyHuv60tAKIiJxItr78EVEpIaU8EVE4oQSvohInFDCFxGJE0r4IiJxQglf4pKZTfJGn1xhZsvMbIiZ3WZmjcIdm4hfdFumxB0zGwY8AZzjnDtiZq0JPAzzHwL3fueFNUARn6iFL/GoPZDnnDsC4CX4bxMYp2a2mc0GMLPRZjbfzJaa2ave+EiYWbaZPWJmC71XT2/95d549cvNbG54qiZSObXwJe54iXsegaEKPiQwzvgcb9ygDOdcntfqf4PAk5L5ZvYLAk9APuht92fn3GQzuwa4wjl3oZmtBC5wzm0zsxbOub1hqaBIJdTCl7jjnDsIDAImEhg+ebqZXXvcZkMJTGbxqZktA74PdC33+cvlfg7zlj8F/mZmNxAYP0UkoiRVv4lI7HHOlQAfAx97LfPvH7eJATOdc1dVdojjl51zPzKzIQQGAVtmZgOcc7uDG7lI3amFL3HHzHqZ2YnlVg0ANgMHCExLCfBfYHi5/vlGZnZSuX2uLPdzvrdND+fcAufc/QSGvS0/HK5I2KmFL/GoCfB7b+LpYgKjEU4kMEXee2a23Tk30uvmednMUrz97iUw2iFAipktINBoOvpXwGPefyRGYITG5SGpjUgN6aKtSC2Vv7gb7lhEakNdOiIicUItfBGROKEWvohInFDCFxGJE0r4IiJxQglfRCROKOGLiMSJ/w9guKgM29fa0wAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "def draw_loss(loss_history):\n",
    "    \n",
    "    plt.plot(loss_history,label=\"loss\",linewidth=1)\n",
    "    plt.legend(loc='best')\n",
    "    plt.xlabel('Steps')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.ylim((0, int(max(loss_history))))\n",
    "    plt.show()\n",
    "\n",
    "    \n",
    "draw_loss(l_h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "<EOS>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open(\"./data/char2id_dict.pickle\",'rb') as f:\n",
    "    char2id_dict = pickle.load(f)\n",
    "\n",
    "\n",
    "def generate(model, start_words, ix2char, char2ix, prefix_words=None):\n",
    "    model.eval()\n",
    "    \n",
    "    results = list(start_words)\n",
    "    start_word_len = len(start_words)\n",
    "\n",
    "    input_ = torch.Tensor([char2ix['<start>']]).view(1, 1).long().to(device)\n",
    "    \n",
    "   \n",
    "    \n",
    "    hidden = model.init_hidden(input_.size(1)).to(device)\n",
    "    \n",
    "\n",
    "    if prefix_words:\n",
    "        for word in prefix_words:\n",
    "            output, hidden = model(input_, hidden)\n",
    "#             input_ = input_.detach().new([char2ix[word]]).view(1, 1)\n",
    "\n",
    "    for i in range(200):\n",
    "        output, hidden = model(input_, hidden)\n",
    "#         print(output.detach())\n",
    "        if i < start_word_len:\n",
    "            w = results[i]\n",
    "            \n",
    "            input_ = input_.data.new([char2ix[w]]).view(1, 1)\n",
    "        else:\n",
    "            \n",
    "            top_index = output.data[0].topk(1)[1][0].item()\n",
    "            w = ix2char[top_index]\n",
    "            print(w)\n",
    "            results.append(w)\n",
    "            input_ = input_.detach().new([top_index]).view(1, 1)\n",
    "        if w == '<EOS>':\n",
    "            del results[-1]\n",
    "            break\n",
    "    return results\n",
    "\n",
    "def gen_acrostic(model,start_words,ix2word,word2ix, prefix_words = None):\n",
    "    model.eval()\n",
    "\n",
    "    results = []\n",
    "    start_word_len = len(start_words)\n",
    "    input_ = torch.Tensor([word2ix['<start>']]).view(1,1).long().to(device)\n",
    "    \n",
    "    hidden = model.init_hidden(input_.size(1)).to(device)\n",
    "    \n",
    "    index=0 # \n",
    "    # \n",
    "    pre_word='<start>'\n",
    " \n",
    "    if prefix_words:\n",
    "        for word in prefix_words:\n",
    "            output,hidden = model(input_ ,hidden)\n",
    "            input_  = torch.tensor(input_.detach().new([word2ix[word]])).view(1,1)\n",
    " \n",
    "    for i in range(800):\n",
    "        output,hidden = model(input_ ,hidden)\n",
    "       \n",
    "        top_index  = output.detach()[0].topk(1)[1][0].item()\n",
    "        \n",
    "        w = ix2word[top_index]\n",
    "        print(w)\n",
    "       \n",
    "        if pre_word  in ('','<start>') :\n",
    "            # \n",
    " \n",
    "            if index==start_word_len:\n",
    "                # \n",
    "                break\n",
    "            else:  \n",
    "                # \n",
    "                w = start_words[index]\n",
    "                index+=1\n",
    "                input_  = torch.tensor(input_.detach().new([word2ix[w]])).view(1,1)    \n",
    "        else:\n",
    "            # \n",
    "            input_t = torch.tensor(input_.detach().new([word2ix[w]])).view(1,1)\n",
    "        results.append(w)\n",
    "        pre_word = w\n",
    "\n",
    "\n",
    "\n",
    "start_word = ''\n",
    "prefix_word = None\n",
    "\n",
    "\n",
    "result = generate(model,start_word,id2char_dict,char2id_dict,prefix_word)\n",
    "# result = gen_acrostic(model,start_word,id2char_dict,char2id_dict,prefix_word)\n",
    "print(''.join(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sin/anaconda3/envs/pytorch_learn/lib/python3.6/site-packages/torch/serialization.py:241: UserWarning: Couldn't retrieve source code for container of type Model. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    }
   ],
   "source": [
    "torch.save(model, 'poetry_model.pickle')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
